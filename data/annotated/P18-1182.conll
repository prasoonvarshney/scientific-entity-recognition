This	O
work	O
has	O
been	O
supported	O
by	O
the	O
National	O
Council	O
of	O
Scientific	O
and	O
Technological	O
Development	O
from	O
Brazil	O
(	O
CNPq	O
)	O
under	O
the	O
grants	O
203065/2014	O
-	O
0	O
and	O
206971/2014	O
-	O
1	O
.	O

Acknowledgments	O
.	O

Using	O
a	O
new	O
delexicalized	O
version	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
(	O
made	O
publicly	O
available	O
)	O
,	O
we	O
showed	O
that	O
the	O
neural	O
model	O
substantially	O
improves	O
over	O
two	O
strong	O
baselines	O
in	O
terms	O
of	O
accuracy	B-MetricName
of	O
the	O
referring	O
expressions	O
and	O
fluency	B-MetricName
of	O
the	O
lexicalized	O
texts	O
.	O

NeuralREG	B-MethodName
decides	O
both	O
on	O
referential	O
form	O
and	O
on	O
referential	O
content	O
in	O
an	O
integrated	O
,	O
end	O
-	O
to	O
-	O
end	O
approach	O
,	O
without	O
using	O
explicit	O
features	O
.	O

We	O
introduced	O
a	O
deep	O
learning	O
model	O
for	O
the	O
generation	B-TaskName
of	I-TaskName
referring	I-TaskName
expressions	I-TaskName
in	O
discourse	O
texts	O
.	O

Conclusion	O
.	O

This	O
shows	O
the	O
importance	O
of	O
the	O
attention	O
mechanism	O
in	O
the	O
decoding	O
step	O
of	O
NeuralREG	B-MethodName
in	O
order	O
to	O
generate	O
fine	O
-	O
grained	O
referring	O
expressions	O
in	O
discourse	O
.	O

Finally	O
,	O
our	O
NeuralREG	B-MethodName
variant	O
with	O
the	O
lowest	O
results	O
were	O
our	O
'	O
vanilla	O
'	O
sequence	B-MethodName
-	I-MethodName
to	I-MethodName
-	I-MethodName
sequence	I-MethodName
(	O
Seq2Seq	B-MethodName
)	O
,	O
whose	O
the	O
lexicalized	O
texts	O
were	O
significantly	O
less	O
fluent	O
and	O
clear	O
than	O
the	O
original	O
ones	O
.	O

This	O
result	O
appears	O
to	O
be	O
not	O
consistent	O
with	O
the	O
findings	O
of	O
Libovický	O
and	O
Helcl	O
(	O
2017	O
)	O
,	O
who	O
reported	O
better	O
results	O
on	O
multi	O
-	O
modal	O
machine	O
translation	O
with	O
hierarchical	B-MethodName
-	I-MethodName
attention	I-MethodName
as	O
opposed	O
to	O
the	O
flat	O
variants	O
(	O
Specia	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Surprisingly	O
,	O
the	O
most	O
complex	O
variant	O
(	O
HierAtt	B-MethodName
)	O
with	O
a	O
hierarchical	B-MethodName
-	I-MethodName
attention	I-MethodName
mechanism	I-MethodName
gave	O
lower	O
results	O
than	O
CAtt	B-MethodName
,	O
producing	O
lexicalized	O
texts	O
which	O
were	O
rated	O
as	O
less	O
fluent	O
than	O
the	O
original	O
ones	O
and	O
not	O
significantly	O
more	O
fluent	O
from	O
the	O
ones	O
generated	O
by	O
the	O
baselines	O
.	O

The	O
texts	O
lexicalized	O
by	O
this	O
variant	O
were	O
also	O
considered	O
statistically	O
more	O
fluent	O
than	O
the	O
ones	O
generated	O
by	O
the	O
two	O
proposed	O
baselines	O
in	O
the	O
human	O
evaluation	O
.	O

Although	O
all	O
the	O
versions	O
performed	O
relatively	O
similar	O
,	O
the	O
concatenativeattention	B-MethodName
(	O
CAtt	B-MethodName
)	O
version	O
generated	O
the	O
closest	O
referring	O
expressions	O
from	O
the	O
gold	O
-	O
standard	O
ones	O
and	O
presented	O
the	O
highest	O
textual	O
accuracy	O
in	O
the	O
automatic	O
evaluation	O
.	O

NeuralREG	B-MethodName
was	O
implemented	O
with	O
3	O
different	O
decoding	O
architectures	O
:	O
Seq2Seq	B-MethodName
,	O
CAtt	B-MethodName
and	O
HierAtt	B-MethodName
.	O

(	O
2016	O
)	O
also	O
did	O
not	O
perform	O
well	O
in	O
the	O
generation	O
of	O
pronouns	O
,	O
revealing	O
a	O
poor	O
capacity	O
to	O
detect	O
highly	O
salient	O
entities	O
in	O
a	O
text	O
.	O

However	O
,	O
the	O
approach	O
of	O
Castro	O
Ferreira	O
et	O
al	O
.	O

OnlyNames	B-MethodName
,	O
as	O
the	O
name	O
already	O
reveals	O
,	O
does	O
not	O
manage	O
to	O
generate	O
any	O
pronouns	O
.	O

However	O
,	O
they	O
performed	O
poorly	O
when	O
it	O
came	O
to	O
pronominalization	O
,	O
which	O
is	O
an	O
important	O
ingredient	O
for	O
fluent	O
,	O
coherent	O
text	O
.	O

These	O
baselines	O
performed	O
relatively	O
well	O
because	O
they	O
frequently	O
generated	O
full	O
names	O
,	O
which	O
occur	O
often	O
for	O
our	O
wikified	O
references	O
.	O

Baselines	O
We	O
introduced	O
two	O
strong	O
baselines	O
which	O
generated	O
roughly	O
half	O
of	O
the	O
referring	O
expressions	O
identical	O
to	O
the	O
gold	O
standard	O
in	O
an	O
automatic	O
evaluation	O
.	O

Besides	O
the	O
REG	B-TaskName
task	O
,	O
these	O
data	O
can	O
be	O
useful	O
for	O
many	O
other	O
tasks	O
related	O
to	O
,	O
for	O
instance	O
,	O
the	O
NLG	O
process	O
(	O
Reiter	O
and	O
Dale	O
,	O
2000;Gatt	O
and	O
Krahmer	O
,	O
2018	O
)	O
and	O
Wikification	O
(	O
Moussallem	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Data	O
The	O
collection	O
of	O
referring	O
expressions	O
used	O
in	O
our	O
experiments	O
was	O
extracted	O
from	O
a	O
novel	O
,	O
delexicalized	O
and	O
publicly	O
available	O
version	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017a	O
,	O
b	O
)	O
,	O
where	O
the	O
discourse	O
entities	O
were	O
replaced	O
with	O
general	O
tags	O
for	O
decreasing	O
the	O
data	O
sparsity	O
.	O

Later	O
in	O
a	O
complementary	O
human	O
evaluation	O
,	O
the	O
texts	O
with	O
referring	O
expressions	O
generated	O
by	O
a	O
variant	O
of	O
our	O
novel	O
model	O
were	O
considered	O
statistically	O
more	O
fluent	O
than	O
the	O
texts	O
lexicalized	O
by	O
the	O
two	O
baselines	O
.	O

In	O
an	O
automatic	O
evaluation	O
on	O
a	O
collection	O
of	O
78,901	O
referring	O
expressions	O
to	O
1,501	O
Wikipedia	O
entities	O
,	O
the	O
different	O
versions	O
of	O
the	O
model	O
all	O
yielded	O
better	O
results	O
than	O
the	O
two	O
(	O
competitive	O
)	O
baselines	O
.	O

The	O
model	O
was	O
implemented	O
using	O
an	O
encoder	O
-	O
decoder	O
approach	O
where	O
a	O
target	O
referent	O
and	O
its	O
surrounding	O
linguistic	O
contexts	O
were	O
first	O
encoded	O
and	O
combined	O
into	O
a	O
single	O
vector	O
representation	O
which	O
subsequently	O
was	O
decoded	O
into	O
a	O
referring	O
expression	O
to	O
the	O
target	O
,	O
suitable	O
for	O
the	O
specific	O
discourse	O
context	O
.	O

It	O
generates	O
referring	O
expressions	O
for	O
discourse	O
entities	O
by	O
simultaneously	O
selecting	O
form	O
and	O
content	O
without	O
any	O
need	O
of	O
feature	O
extraction	O
techniques	O
.	O

This	O
study	O
introduced	O
NeuralREG	B-MethodName
,	O
an	O
end	O
-	O
to	O
-	O
end	O
approach	O
based	O
on	O
neural	O
networks	O
which	O
tackles	O
the	O
full	O
Referring	B-TaskName
Expression	I-TaskName
Generation	I-TaskName
process	O
.	O

Discussion	O
.	O

Finally	O
,	O
the	O
original	O
texts	O
were	O
rated	O
significantly	O
higher	O
than	O
both	O
baselines	O
in	O
terms	O
of	O
the	O
three	O
metrics	O
,	O
also	O
than	O
NeuralREG+Seq2Seq	B-MethodName
and	O
Neu	B-MethodName
-	I-MethodName
ralREG+HierAtt	I-MethodName
in	O
terms	O
of	O
fluency	B-MetricName
,	O
and	O
than	O
NeuralREG+Seq2Seq	B-MethodName
in	O
terms	O
of	O
clarity	B-MetricName
.	O

The	O
results	O
for	O
the	O
3	O
different	O
decoding	O
methods	O
of	O
NeuralREG	B-MethodName
also	O
did	O
not	O
reveal	O
a	O
significant	O
difference	O
.	O

In	O
comparison	O
with	O
the	O
neural	O
models	O
,	O
NeuralREG+CAtt	B-MethodName
significantly	O
outperformed	O
the	O
baselines	O
in	O
terms	O
of	O
fluency	B-MetricName
,	O
whereas	O
the	O
other	O
comparisons	O
between	O
baselines	O
and	O
neural	O
models	O
were	O
not	O
statistically	O
significant	O
.	O

Different	O
from	O
the	O
automatic	O
evaluation	O
,	O
the	O
results	O
of	O
both	O
baselines	O
were	O
not	O
statistically	O
significant	O
for	O
the	O
three	O
metrics	O
.	O

To	O
test	O
the	O
statistical	O
significance	O
of	O
the	O
pairwise	O
comparisons	O
,	O
we	O
used	O
the	O
Wilcoxon	O
signedrank	O
test	O
corrected	O
for	O
multiple	O
comparisons	O
using	O
the	O
Bonferroni	O
method	O
.	O

Concerning	O
the	O
size	O
of	O
the	O
triple	O
sets	O
,	O
we	O
did	O
not	O
find	O
any	O
clear	O
pattern	O
.	O

Inspection	O
of	O
the	O
Table	O
reveals	O
a	O
clear	O
pattern	O
:	O
all	O
three	O
neural	O
models	O
scored	O
higher	O
than	O
the	O
baselines	O
on	O
all	O
metrics	O
,	O
with	O
especially	O
NeuralREG+CAtt	B-MethodName
approaching	O
the	O
ratings	O
for	O
the	O
original	O
sentences	O
,	O
although	O
-again	O
-differences	O
between	O
the	O
neural	O
models	O
were	O
small	O
.	O

Results	O
Table	O
3	O
summarizes	O
the	O
results	O
.	O

English	O
(	O
44	O
)	O
,	O
while	O
14	O
and	O
2	O
self	O
-	O
reported	O
as	O
fluent	O
or	O
having	O
a	O
basic	O
proficiency	O
,	O
respectively	O
.	O

before	O
his	O
death	O
in	O
california	O
he	O
had	O
been	O
awarded	O
the	O
distinguished	O
service	O
medal	O
by	O
the	O
us	O
navy	O
an	O
award	O
higher	O
than	O
the	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

Original	O
alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
18	O
november	O
1923	O
.	O

before	O
his	O
death	O
in	O
california	O
he	O
had	O
been	O
awarded	O
the	O
distinguished	O
service	O
medal	O
an	O
award	O
higher	O
than	O
the	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

alan	O
shephard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

HierAtt	O
.	O

before	O
his	O
death	O
in	O
california	O
he	O
had	O
been	O
awarded	O
the	O
distinguished	O
service	O
medal	O
by	O
the	O
us	O
navy	O
an	O
award	O
higher	O
than	O
the	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

CAtt	O
.	O

before	O
his	O
death	O
in	O
california	O
him	O
had	O
been	O
awarded	O
the	O
distinguished	O
service	O
medal	O
by	O
the	O
united	O
states	O
navy	O
an	O
award	O
higher	O
than	O
the	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

Seq2Seq	O
.	O

before	O
alan	O
shepard	O
death	O
in	O
california	O
him	O
had	O
been	O
awarded	O
distinguished	O
service	O
medal	O
an	O
award	O
higher	O
than	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

Ferreira	O
.	O

before	O
alan	O
shepard	O
death	O
in	O
california	O
alan	O
shepard	O
had	O
been	O
awarded	O
distinguished	O
service	O
medal	O
(	O
united	O
states	O
navy	O
)	O
an	O
award	O
higher	O
than	O
department	O
of	O
commerce	O
gold	O
medal	O
.	O

OnlyNames	B-MethodName
alan	O
shepard	O
was	O
born	O
in	O
new	O
hampshire	O
on	O
1923	O
-	O
11	O
-	O
18	O
.	O

Model	O
Text	O
.	O

The	O
majority	O
declared	O
themselves	O
native	O
speakers	O
of	O
.	O

Their	O
average	O
age	O
was	O
36	O
years	O
and	O
27	O
of	O
them	O
were	O
females	O
.	O

Participants	O
We	O
recruited	O
60	O
participants	O
,	O
10	O
per	O
list	O
,	O
via	O
Mechanical	O
Turk	O
.	O

The	O
experiment	O
is	O
available	O
on	O
the	O
website	O
of	O
the	O
author	O
3	O
.	O

Once	O
introduced	O
to	O
a	O
trial	O
,	O
the	O
participants	O
were	O
asked	O
to	O
rate	O
the	O
fluency	B-MetricName
(	O
"	O
does	O
the	O
text	O
flow	O
in	O
a	O
natural	O
,	O
easy	O
to	O
read	O
manner	O
?	O
"	O
)	O
,	O
grammaticality	B-MetricName
(	O
"	O
is	O
the	O
text	O
grammatical	O
(	O
no	O
spelling	O
or	O
grammatical	O
errors	O
)	O
?	O
"	O
)	O
and	O
clarity	B-MetricName
(	O
"	O
does	O
the	O
text	O
clearly	O
express	O
the	O
data	O
?	O
"	O
)	O
of	O
each	O
target	O
text	O
on	O
a	O
7	B-MetricName
-	I-MetricName
Likert	I-MetricName
scale	I-MetricName
,	O
focussing	O
on	O
the	O
highlighted	O
referring	O
expressions	O
.	O

The	O
experiment	O
had	O
a	O
latin	O
-	O
square	O
design	O
,	O
distributing	O
the	O
144	O
trials	O
over	O
6	O
different	O
lists	O
such	O
that	O
each	O
participant	O
rated	O
24	O
trials	O
,	O
one	O
for	O
each	O
of	O
the	O
24	O
corpus	O
instances	O
,	O
making	O
sure	O
that	O
participants	O
saw	O
equal	O
numbers	O
of	O
triple	O
set	O
sizes	O
and	O
generated	O
versions	O
.	O

Method	O
.	O

For	O
each	O
size	O
group	O
,	O
we	O
randomly	O
selected	O
4	O
instances	O
(	O
of	O
varying	O
degrees	O
of	O
variation	O
between	O
the	O
generated	O
texts	O
)	O
giving	O
rise	O
to	O
144	O
trials	O
(=	O
6	O
triple	O
set	O
sizes	O
*	O
4	O
instances	O
*	O
6	O
text	O
versions	O
)	O
,	O
each	O
consisting	O
of	O
a	O
set	O
of	O
triples	O
and	O
a	O
target	O
text	O
describing	O
it	O
with	O
the	O
lexicalized	O
referring	O
expressions	O
highlighted	O
in	O
yellow	O
.	O

Instances	O
were	O
chosen	O
following	O
2	O
criteria	O
:	O
the	O
number	O
of	O
triples	O
in	O
the	O
source	O
set	O
(	O
ranging	O
from	O
2	O
to	O
7	O
)	O
and	O
the	O
differences	O
between	O
the	O
target	O
texts	O
.	O

For	O
each	O
of	O
the	O
selected	O
instances	O
,	O
we	O
took	O
into	O
account	O
its	O
source	O
triple	O
set	O
and	O
its	O
6	O
target	O
texts	O
:	O
one	O
original	O
(	O
randomly	O
chosen	O
)	O
and	O
its	O
versions	O
with	O
the	O
referring	O
expressions	O
generated	O
by	O
each	O
of	O
the	O
5	O
models	O
introduced	O
in	O
this	O
study	O
(	O
two	O
baselines	O
,	O
three	O
neural	O
models	O
)	O
.	O

Material	O
We	O
quasi	O
-	O
randomly	O
selected	O
24	O
instances	O
from	O
the	O
delexicalized	O
version	O
of	O
the	O
WebNLG	O
corpus	O
related	O
to	O
the	O
test	O
part	O
of	O
the	O
re	O
-	O
ferring	O
expression	O
collection	O
.	O

Complementary	O
to	O
the	O
automatic	O
evaluation	O
,	O
we	O
performed	O
an	O
evaluation	O
with	O
human	O
judges	O
,	O
comparing	O
the	O
quality	O
judgments	O
of	O
the	O
original	O
texts	O
to	O
the	O
versions	O
generated	O
by	O
our	O
various	O
models	O
.	O

Human	O
Evaluation	O
.	O

The	O
more	O
complex	O
Neural	B-MethodName
-	I-MethodName
REG+HierAtt	I-MethodName
yielded	O
the	O
lowest	O
results	O
,	O
even	O
though	O
the	O
differences	O
with	O
the	O
other	O
two	O
models	O
were	O
small	O
and	O
not	O
even	O
statistically	O
significant	O
in	O
many	O
of	O
the	O
cases	O
.	O

The	O
results	O
for	O
the	O
different	O
decoding	O
methods	O
for	O
NeuralREG	B-MethodName
were	O
similar	O
,	O
with	O
the	O
Neu	B-MethodName
-	I-MethodName
ralREG+CAtt	I-MethodName
performing	O
slightly	O
better	O
in	O
terms	O
of	O
the	O
BLEU	B-TaskName
score	O
,	O
text	O
accuracy	B-TaskName
and	O
String	B-TaskName
Edit	I-TaskName
Distance	I-TaskName
.	O

Especially	O
noteworthy	O
was	O
the	O
score	O
on	O
pronoun	O
accuracy	O
,	O
indicating	O
that	O
the	O
model	O
was	O
well	O
capable	O
of	O
predicting	O
when	O
to	O
generate	O
a	O
pronominal	O
reference	O
in	O
our	O
dataset	O
.	O

When	O
considering	O
the	O
texts	O
lexicalized	O
with	O
the	O
referring	O
expressions	O
produced	O
by	O
NeuralREG	B-MethodName
,	O
at	O
least	O
28	O
%	O
of	O
them	O
are	O
similar	O
to	O
the	O
original	O
texts	O
.	O

This	O
means	O
that	O
NeuralREG	B-MethodName
predicted	O
3	O
out	O
of	O
4	O
references	O
completely	O
correct	O
,	O
whereas	O
the	O
incorrect	O
ones	O
needed	O
an	O
average	O
of	O
2	O
post	O
-	O
edition	O
operations	O
in	O
character	O
level	O
to	O
be	O
equal	O
to	O
the	O
gold	O
-	O
standard	O
.	O

They	O
achieved	O
BLEU	B-MetricName
scores	O
,	O
text	O
and	O
referential	O
accuracies	O
as	O
well	O
as	O
string	B-MetricName
edit	I-MetricName
distances	I-MetricName
in	O
the	O
range	O
of	O
79.01	B-MetricValue
-	O
79.39	B-MetricValue
,	O
28%-30	B-MetricValue
%	I-MetricValue
,	O
73%-74	B-MetricValue
%	I-MetricValue
and	O
2.25	B-MetricValue
-	I-MetricValue
2.36	I-MetricValue
,	O
respectively	O
.	O

Importantly	O
,	O
the	O
three	O
NeuralREG	B-MethodName
variant	O
models	O
statistically	O
outperformed	O
the	O
two	O
baseline	O
systems	O
.	O

dropout	B-HyperparameterName
probability	I-HyperparameterName
0.3	B-HyperparameterValue
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
5	B-HyperparameterValue
,	O
and	O
Neu	B-HyperparameterName
-	I-HyperparameterName
ralREG+HierAtt	I-HyperparameterName
with	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
of	O
0.3	B-HyperparameterValue
and	O
beam	B-HyperparameterName
size	I-HyperparameterName
of	O
1	B-HyperparameterValue
selected	O
based	O
on	O
the	O
highest	O
accuracy	O
on	O
the	O
development	O
set	O
.	O

Rankings	O
were	O
determined	O
by	O
statistical	O
significance	O
.	O

We	O
reported	O
results	O
on	O
the	O
test	O
set	O
for	O
Neu	B-MethodName
-	I-MethodName
ralREG+Seq2Seq	I-MethodName
and	O
NeuralREG+CAtt	B-MethodName
using	O
(	O
2	O
)	O
Accuracy	B-MetricName
(	O
Acc	B-MetricName
.	O
)	O
,	O
Precision	B-MetricName
(	O
Prec	B-MetricName
.	O
)	O
,	O
Recall	B-MetricName
(	O
Rec	B-MetricName
.	O
)	O
and	O
F	B-MetricName
-	I-MetricName
Score	I-MetricName
results	O
in	O
the	O
prediction	O
of	O
pronominal	O
forms	O
;	O
and	O
(	O
3	O
)	O
Accuracy	B-MetricName
(	O
Acc	B-MetricName
.	O
)	O
and	O
BLEU	B-MetricName
score	O
results	O
of	O
the	O
texts	O
with	O
the	O
generated	O
referring	O
expressions	O
.	O

(	O
2016	O
)	O
performed	O
statistically	O
better	O
than	O
On	B-MethodName
-	I-MethodName
lyNames	I-MethodName
on	O
all	O
metrics	O
due	O
to	O
its	O
capability	O
,	O
albeit	O
to	O
a	O
limited	O
extent	O
,	O
to	O
predict	O
pronominal	O
references	O
(	O
which	O
OnlyNames	B-MethodName
obviously	O
can	O
not	O
)	O
.	O

The	O
method	O
based	O
on	O
Castro	O
Ferreira	O
et	O
al	O
.	O

The	O
first	O
thing	O
to	O
note	O
in	O
the	O
results	O
of	O
the	O
first	O
table	O
is	O
that	O
the	O
baselines	O
in	O
the	O
top	O
two	O
rows	O
performed	O
quite	O
strong	O
on	O
this	O
task	O
,	O
generating	O
more	O
than	O
half	O
of	O
the	O
referring	O
expressions	O
exactly	O
as	O
in	O
the	O
goldstandard	O
.	O

Results	O
Table	O
1	O
summarizes	O
the	O
results	O
for	O
all	O
models	O
on	O
all	O
metrics	O
on	O
the	O
test	O
set	O
and	O
Table	O
2	O
depicts	O
a	O
text	O
example	O
lexicalized	O
by	O
each	O
model	O
.	O

The	O
results	O
described	O
in	O
the	O
next	O
section	O
were	O
obtained	O
on	O
the	O
test	O
set	O
by	O
the	O
NeuralREG	B-MethodName
version	O
with	O
the	O
highest	O
accuracy	O
on	O
the	O
development	O
set	O
over	O
the	O
epochs	O
.	O

For	O
each	O
decoding	O
version	O
(	O
Seq2Seq	O
,	O
CAtt	O
and	O
HierAtt	O
)	O
,	O
we	O
searched	O
for	O
the	O
best	O
combination	O
of	O
drop	B-HyperparameterName
-	I-HyperparameterName
out	I-HyperparameterName
probability	I-HyperparameterName
of	O
0.2	B-HyperparameterValue
or	O
0.3	B-HyperparameterValue
in	O
both	O
the	O
encoding	O
and	O
decoding	O
layers	O
,	O
using	O
beam	B-HyperparameterName
search	I-HyperparameterName
with	I-HyperparameterName
a	I-HyperparameterName
size	I-HyperparameterName
of	O
1	B-HyperparameterValue
or	O
5	B-HyperparameterValue
with	O
predictions	O
up	O
to	O
30	O
tokens	O
or	O
until	O
2	O
ending	O
tokens	O
were	O
predicted	O
(	O
EOS	O
)	O
.	O

We	O
ran	O
each	O
model	O
for	O
60	B-HyperparameterValue
epochs	B-HyperparameterName
,	O
applying	O
early	O
stopping	O
for	O
model	O
selection	O
based	O
on	O
accuracy	O
on	O
the	O
development	O
set	O
with	O
patience	O
of	O
20	B-HyperparameterValue
epochs	B-HyperparameterName
.	O

Models	O
were	O
trained	O
using	O
stochastic	O
gradient	O
descent	O
with	O
Adadelta	O
(	O
Zeiler	O
,	O
2012	O
)	O
and	O
mini	O
-	O
batches	B-HyperparameterName
of	I-HyperparameterName
size	I-HyperparameterName
40	B-HyperparameterValue
.	O

All	O
non	O
-	O
recurrent	O
matrices	O
were	O
initialized	O
following	O
the	O
method	O
of	O
Glorot	O
and	O
Bengio	O
(	O
2010	O
)	O
.	O

Source	O
and	O
target	O
word	O
embeddings	O
were	O
300D	B-HyperparameterValue
each	O
and	O
trained	O
jointly	O
with	O
the	O
model	O
,	O
whereas	O
hidden	B-HyperparameterName
units	I-HyperparameterName
were	O
512D	B-HyperparameterValue
for	O
each	O
direction	O
,	O
totaling	O
1024D	B-HyperparameterValue
in	O
the	O
bidirection	O
layers	O
.	O

Settings	O
NeuralREG	B-MethodName
was	O
implemented	O
using	O
Dynet	O
(	O
Neubig	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

To	O
test	O
the	O
statistical	O
significance	O
of	O
the	O
BLEU	B-TaskName
scores	O
of	O
the	O
models	O
,	O
we	O
used	O
a	O
bootstrap	O
resampling	O
together	O
with	O
an	O
approximate	O
randomization	O
method	O
(	O
Clark	O
et	O
al	O
.	O
,	O
2011	O
)	O
2	O
.	O

Post	O
-	O
hoc	O
McNemar	O
's	O
and	O
Wilcoxon	O
signed	O
ranked	O
tests	O
adjusted	O
by	O
the	O
Bonferroni	O
method	O
were	O
used	O
to	O
test	O
the	O
statistical	O
significance	O
of	O
the	O
models	O
in	O
terms	O
of	O
accuracy	B-MetricName
and	O
string	B-MetricName
edit	I-MetricName
distance	I-MetricName
,	O
respectively	O
.	O

Since	O
our	O
model	O
does	O
not	O
handle	O
referring	O
expressions	O
for	O
constants	O
(	O
dates	O
and	O
numbers	O
)	O
,	O
we	O
just	O
copied	O
their	O
source	O
version	O
into	O
the	O
template	O
.	O

Finally	O
,	O
we	O
lexicalized	O
the	O
original	O
templates	O
with	O
the	O
referring	O
expressions	O
produced	O
by	O
the	O
models	O
and	O
compared	O
them	O
with	O
the	O
original	O
texts	O
in	O
the	O
corpus	O
using	O
accuracy	B-MetricName
and	O
BLEU	B-MetricName
score	O
(	O
Papineni	O
et	O
al	O
.	O
,	O
2002	O
)	O
as	O
a	O
measure	O
of	O
fluency	O
.	O

Since	O
pronouns	O
are	O
highlighted	O
as	O
the	O
most	O
likely	O
referential	O
form	O
to	O
be	O
used	O
when	O
a	O
referent	O
is	O
salient	O
in	O
the	O
discourse	O
,	O
as	O
argued	O
in	O
the	O
introduction	O
,	O
we	O
also	O
computed	O
pronoun	O
accuracy	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
-	O
score	O
in	O
order	O
to	O
evaluate	O
the	O
performance	O
of	O
the	O
models	O
for	O
capturing	O
discourse	O
salience	O
.	O

We	O
compared	O
the	O
referring	O
expressions	O
produced	O
by	O
the	O
evaluated	O
models	O
with	O
the	O
goldstandards	O
ones	O
using	O
accuracy	B-MetricName
and	O
String	B-MetricName
Edit	I-MetricName
Distance	I-MetricName
(	O
Levenshtein	B-MetricName
,	O
1966	O
)	O
.	O

Data	O
We	O
evaluated	O
our	O
models	O
on	O
the	O
training	O
,	O
development	O
and	O
test	O
referring	O
expression	O
sets	O
described	O
in	O
Section	O
3.3	O
.	O
Metrics	O
.	O

Automatic	O
evaluation	O
.	O

All	O
features	O
were	O
extracted	O
automatically	O
from	O
the	O
texts	O
using	O
the	O
sentence	O
tokenizer	O
and	O
dependency	O
parser	O
of	O
Stanford	O
CoreNLP	O
(	O
Manning	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Text	O
and	O
sentence	O
information	O
statuses	O
mark	O
whether	O
a	O
reference	O
is	O
a	O
initial	O
or	O
a	O
subsequent	O
mention	O
to	O
an	O
entity	O
in	O
the	O
text	O
and	O
the	O
sentence	O
,	O
respectively	O
.	O

Regarding	O
the	O
features	O
,	O
syntactic	O
position	O
distinguishes	O
whether	O
a	O
reference	O
is	O
the	O
subject	O
,	O
object	O
or	O
subject	O
determiner	O
(	O
genitive	O
)	O
in	O
a	O
sentence	O
.	O

Finally	O
,	O
if	O
a	O
referring	O
expression	O
is	O
not	O
found	O
in	O
the	O
training	O
set	O
for	O
a	O
given	O
entity	O
,	O
the	O
same	O
method	O
as	O
OnlyNames	B-MethodName
is	O
used	O
.	O

In	O
case	O
a	O
referring	O
expression	O
for	O
a	O
wiki	O
target	O
is	O
not	O
found	O
in	O
this	O
way	O
,	O
a	O
backoff	O
method	O
is	O
applied	O
by	O
removing	O
one	O
factor	O
at	O
a	O
time	O
in	O
the	O
following	O
order	O
:	O
sentence	O
information	O
status	O
,	O
text	O
information	O
status	O
and	O
grammatical	O
position	O
.	O

Once	O
the	O
choice	O
of	O
referential	O
form	O
is	O
made	O
,	O
the	O
most	O
frequent	O
variant	O
is	O
chosen	O
in	O
the	O
training	O
corpus	O
given	O
the	O
referent	O
,	O
syntactic	O
position	O
and	O
information	O
status	O
.	O

P	O
(	O
f	O
|	O
X	O
)	O
∝	O
P	O
(	O
f	O
)	O
x∈X	O
P	O
(	O
x	O
|	O
f	O
)	O
f	O
∈F	O
P	O
(	O
f	O
)	O
x∈X	O
P	O
(	O
x	O
|	O
f	O
)	O
(	O
14	O
)	O
The	O
method	O
calculates	O
the	O
likelihood	O
of	O
each	O
referential	O
form	O
f	O
given	O
a	O
set	O
of	O
features	O
X	O
,	O
consisting	O
of	O
grammatical	O
position	O
and	O
information	O
status	O
(	O
new	O
or	O
given	O
in	O
the	O
text	O
and	O
sentence	O
)	O
.	O

The	O
choice	O
is	O
made	O
by	O
a	O
Naive	O
Bayes	O
method	O
as	O
Equation	O
14	O
depicts	O
.	O

Ferreira	O
works	O
by	O
first	O
choosing	O
whether	O
a	O
reference	O
should	O
be	O
a	O
proper	O
name	O
,	O
pronoun	O
,	O
description	O
or	O
demonstrative	O
.	O

This	O
method	O
refers	O
to	O
each	O
entity	O
by	O
their	O
Wikipedia	O
ID	O
,	O
replacing	O
each	O
underscore	O
in	O
the	O
ID	O
for	O
whitespaces	O
(	O
e.g.	O
,	O
Appleton	O
International	O
Airport	O
to	O
"	O
Appleton	O
International	O
Airport	O
"	O
)	O
.	O

OnlyNames	B-MethodName
is	O
motivated	O
by	O
the	O
similarity	O
among	O
the	O
Wikipedia	O
ID	O
of	O
an	O
element	O
and	O
a	O
proper	O
name	O
reference	O
to	O
it	O
.	O

(	O
2016	O
)	O
,	O
dubbed	O
Ferreira	O
.	O

We	O
compared	O
the	O
performance	O
of	O
NeuralREG	B-MethodName
against	O
two	O
baselines	O
:	O
OnlyNames	B-MethodName
and	O
a	O
model	O
based	O
on	O
the	O
choice	O
of	O
referential	O
form	O
method	O
of	O
Castro	O
Ferreira	O
et	O
al	O
.	O

Models	O
for	O
Comparison	O
.	O

In	O
order	O
to	O
find	O
the	O
referring	O
expression	O
y	O
that	O
maximizes	O
the	O
likelihood	O
in	O
Equation	O
11	O
,	O
we	O
apply	O
a	O
beam	O
search	O
with	O
length	O
normalization	O
with	O
α	B-HyperparameterName
=	O
0.6	B-HyperparameterValue
(	O
Wu	O
et	O
al	O
.	O
,	O
2016	O
):	O
lp(y	O
)	O
=	O
(	O
5	O
+	O
|y|	O
)	O
α	O
(	O
5	O
+	O
1	O
)	O
α	O
(	O
12	O
)	O
The	O
decoder	O
is	O
trained	O
to	O
minimize	O
the	O
negative	O
log	O
likelihood	O
of	O
the	O
next	O
token	O
in	O
the	O
target	O
referring	O
expression	O
:	O
J(θ	O
)	O
=	O
−	O
i	O
log	O
p(yi|y	O
<	O
i	O
,	O
X	O
(	O
pre	O
)	O
,	O
x	O
(	O
wiki	O
)	O
,	O
X	O
(	O
pos	O
)	O
)	O
(	O
13	O
)	O
.	O

si	O
=	O
Φ	O
dec	O
(	O
si−1	O
,	O
[	O
ci	O
,	O
Vy	O
i−1	O
,	O
V	O
wiki	O
]	O
)	O
(	O
10	O
)	O
p(y	O
i	O
|y	O
<	O
i	O
,	O
X	O
(	O
pre	O
)	O
,	O
x	O
(	O
wiki	O
)	O
,	O
X	O
(	O
pos	O
)	O
)	O
=	O
softmax(W	O
c	O
s	O
i	O
+	O
b)(11	O
)	O
In	O
Equation	O
10	O
,	O
s	O
0	O
and	O
c	O
0	O
are	O
zero	O
-	O
initialized	O
vectors	O
.	O

In	O
each	O
decoding	O
step	O
i	O
,	O
a	O
final	O
summary	O
-	O
vector	O
for	O
each	O
context	O
c	O
(	O
k	O
)	O
i	O
is	O
computed	O
by	O
summing	O
the	O
encoder	O
states	O
h	O
(	O
k	O
)	O
j	O
weighted	O
by	O
the	O
attention	O
probabilities	O
α	O
(	O
k	O
)	O
i	O
:	O
c	O
(	O
k	O
)	O
i	O
=	O
N	O
j=1	O
α	O
(	O
k	O
)	O
ij	O
h	O
(	O
k	O
)	O
j	O
(	O
6	O
)	O
To	O
combine	O
c	O
HierAtt	O
implements	O
a	O
second	O
attention	O
mechanism	O
inspired	O
by	O
Libovický	O
and	O
Helcl	O
(	O
2017	O
)	O
in	O
order	O
to	O
generate	O
attention	O
weights	O
for	O
the	O
pre	O
-	O
and	O
pos	O
-	O
context	O
summary	O
-	O
vectors	O
c	O
(	O
k	O
)	O
i	O
=	O
v	O
(	O
k)T	O
b	O
tanh(W	O
(	O
k	O
)	O
b	O
si−1	O
+	O
U	O
(	O
k	O
)	O
b	O
c	O
(	O
k	O
)	O
i	O
)	O
(	O
7	O
)	O
β	O
(	O
k	O
)	O
i	O
=	O
exp(e	O
(	O
k	O
)	O
i	O
)	O
n	O
exp(e	O
(	O
n	O
)	O
i	O
)	O
(	O
8)	O
ci	O
=	O
k	O
β	O
(	O
k	O
)	O
i	O
U	O
(	O
k	O
)	O
b	O
c	O
(	O
k	O
)	O
i	O
(	O
9	O
)	O
Decoding	O
Given	O
the	O
summary	O
-	O
vector	O
c	O
i	O
,	O
the	O
embedding	O
of	O
the	O
previous	O
referring	O
expression	O
token	O
V	O
y	O
i−1	O
,	O
the	O
previous	O
decoder	O
state	O
s	O
i−1	O
and	O
the	O
entity	O
-	O
embedding	O
V	O
wiki	O
,	O
the	O
decoders	O
predict	O
their	O
next	O
state	O
which	O
later	O
is	O
used	O
to	O
compute	O
a	O
probability	O
distribution	O
over	O
the	O
tokens	O
in	O
the	O
output	O
vocabulary	O
for	O
the	O
next	O
timestep	O
as	O
Equations	O
10	O
and	O
11	O
show	O
.	O

We	O
compute	O
energies	O
e	O
(	O
k	O
)	O
ij	O
=	O
v	O
(	O
k)T	O
a	O
tanh(W	O
(	O
k	O
)	O
a	O
si−1	O
+	O
U	O
(	O
k	O
)	O
a	O
h	O
(	O
k	O
)	O
j	O
)	O
(	O
4	O
)	O
α	O
(	O
k	O
)	O
ij	O
=	O
exp(e	O
(	O
k	O
)	O
ij	O
)	O
N	O
n=1	O
exp(e	O
(	O
k	O
)	O
in	O
)	O
(	O
5	O
)	O
In	O
general	O
,	O
the	O
attention	O
probability	O
α	O
(	O
k	O
)	O
ij	O
determines	O
the	O
amount	O
of	O
contribution	O
of	O
the	O
jth	O
token	O
of	O
k	O
-	O
context	O
in	O
the	O
generation	O
of	O
the	O
ith	O
token	O
of	O
the	O
referring	O
expression	O
.	O

Seq2Seq	O
models	O
the	O
context	O
vector	O
c	O
i	O
at	O
each	O
timestep	O
i	O
concatenating	O
the	O
pre	O
-	O
and	O
pos	O
-	O
context	O
annotation	O
vectors	O
averaged	O
over	O
time	O
:	O
ĥ(pre	O
)	O
=	O
1	O
N	O
N	O
i	O
h	O
(	O
pre	O
)	O
i	O
(	O
1	O
)	O
ĥ(pos	O
)	O
=	O
1	O
N	O
N	O
i	O
h	O
(	O
pos	O
)	O
i	O
(	O
2	O
)	O
ci	O
=	O
[	O
ĥ(pre	O
)	O
,	O
ĥ(pos	O
)	O
]	O
(	O
3	O
)	O
CAtt	O
is	O
an	O
LSTM	O
decoder	O
augmented	O
with	O
an	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2015	O
)	O
over	O
the	O
pre	O
-	O
and	O
pos	O
-	O
context	O
encodings	O
,	O
which	O
is	O
used	O
to	O
compute	O
c	O
i	O
at	O
each	O
timestep	O
.	O

The	O
difference	O
between	O
the	O
decoder	O
variations	O
is	O
the	O
method	O
to	O
compute	O
c	O
i	O
.	O

All	O
decoders	O
at	O
each	O
timestep	O
i	O
of	O
the	O
generation	O
process	O
take	O
as	O
input	O
features	O
their	O
previous	O
state	O
s	O
i−1	O
,	O
the	O
target	O
entity	O
-	O
embedding	O
V	O
wiki	O
,	O
the	O
embedding	O
of	O
the	O
previous	O
word	O
of	O
the	O
referring	O
expression	O
V	O
y	O
i−1	O
and	O
finally	O
the	O
summary	O
vector	O
of	O
the	O
pre	O
-	O
and	O
poscontexts	O
c	O
i	O
.	O

The	O
referring	B-TaskName
expression	I-TaskName
generation	I-TaskName
module	O
is	O
an	O
LSTM	O
decoder	O
implemented	O
in	O
3	O
different	O
versions	O
:	O
Seq2Seq	O
,	O
CAtt	O
and	O
HierAtt	O
.	O

Decoder	O
.	O

Finally	O
,	O
the	O
encoding	O
of	O
target	O
entity	O
x	O
(	O
wiki	O
)	O
is	O
simply	O
its	O
entry	O
in	O
the	O
shared	O
input	O
word	O
-	O
embedding	O
matrix	O
V	O
wiki	O
.	O

The	O
same	O
process	O
is	O
repeated	O
for	O
the	O
pos	O
-	O
context	O
resulting	O
in	O
representations	O
(	O
−	O
→	O
h	O
(	O
pos	O
)	O
1	O
,	O
•	O
•	O
•	O
,	O
−	O
→	O
h	O
(	O
pos	O
)	O
l	O
)	O
and	O
(	O
←	O
−	O
h	O
(	O
pos	O
)	O
1	O
,	O
•	O
•	O
•	O
,	O
←	O
−	O
h	O
(	O
pos	O
)	O
l	O
)	O
and	O
annotation	O
vectors	O
h	O
(	O
pos	O
)	O
t	O
=	O
[	O
−	O
→	O
h	O
(	O
pos	O
)	O
t	O
,	O
←	O
−	O
h	O
(	O
pos	O
)	O
t	O
]	O
.	O

The	O
final	O
annotation	O
vector	O
for	O
each	O
encoding	O
timestep	O
t	O
is	O
obtained	O
by	O
the	O
concatenation	O
of	O
the	O
forward	O
and	O
backward	O
representations	O
h	O
(	O
pre	O
)	O
t	O
=	O
[	O
−	O
→	O
h	O
(	O
pre	O
)	O
t	O
,	O
←	O
−	O
h	O
(	O
pre	O
)	O
t	O
]	O
.	O

The	O
pre	O
-	O
context	O
X	O
(	O
pre	O
)	O
=	O
{	O
x	O
(	O
pre	O
)	O
1	O
,	O
x	O
(	O
pre	O
)	O
2	O
,	O
...	O
,	O
x(pre	O
)	O
m	O
}	O
is	O
represented	O
by	O
forward	O
and	O
backward	O
hidden	O
-	O
state	O
vectors	O
(	O
−	O
→	O
h	O
(	O
pre	O
)	O
1	O
,	O
•	O
•	O
•	O
,	O
−	O
→	O
h	O
(	O
pre	O
)	O
m	O
)	O
and	O
(	O
←	O
−	O
h	O
(	O
pre	O
)	O
1	O
,	O
•	O
•	O
•	O
,	O
←	O
−	O
h	O
(	O
pre	O
)	O
m	O
)	O
.	O

These	O
modules	O
learn	O
feature	O
representations	O
of	O
the	O
text	O
surrounding	O
the	O
target	O
entity	O
x	O
(	O
wiki	O
)	O
,	O
which	O
are	O
used	O
for	O
the	O
referring	B-TaskName
expression	I-TaskName
generation	I-TaskName
.	O

Our	O
model	O
starts	O
by	O
encoding	O
the	O
pre	O
-	O
and	O
poscontexts	O
with	O
two	O
separate	O
bidirectional	O
LSTM	O
encoders	O
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997;Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
.	O

Context	O
encoders	O
.	O

The	O
model	O
is	O
implemented	O
as	O
a	O
multi	O
-	O
encoder	O
,	O
attentiondecoder	O
network	O
with	O
bidirectional	O
(	O
Schuster	O
and	O
Paliwal	O
,	O
1997	O
)	O
Long	O
-	O
Short	O
Term	O
Memory	O
Layers	O
(	O
LSTM	O
)	O
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
sharing	O
the	O
same	O
input	O
word	O
-	O
embedding	O
matrix	O
V	O
,	O
as	O
explained	O
further	O
.	O

NeuralREG	B-MethodName
aims	O
to	O
generate	O
a	O
referring	O
expression	O
y	O
=	O
{	O
y	O
1	O
,	O
y	O
2	O
,	O
...	O
,	O
y	O
T	O
}	O
with	O
T	O
tokens	O
to	O
refer	O
to	O
a	O
target	O
entity	O
token	O
x	O
(	O
wiki	O
)	O
given	O
a	O
discourse	O
precontext	O
X	O
(	O
pre	O
)	O
=	O
{	O
x	O
}	O
with	O
m	O
and	O
l	O
tokens	O
,	O
respectively	O
.	O

NeuralREG	B-MethodName
.	O

In	O
the	O
next	O
section	O
,	O
we	O
show	O
in	O
detail	O
how	O
NeuralREG	B-MethodName
models	O
the	O
problem	O
of	O
generating	B-TaskName
a	I-TaskName
referring	I-TaskName
expression	I-TaskName
to	O
a	O
discourse	O
entity	O
.	O

In	O
this	O
context	O
,	O
it	O
is	O
important	O
to	O
observe	O
that	O
the	O
conversion	O
of	O
the	O
general	O
tags	O
to	O
the	O
Wikipedia	O
IDs	O
can	O
be	O
done	O
in	O
constant	O
time	O
during	O
the	O
generation	O
process	O
,	O
since	O
their	O
mapping	O
,	O
like	O
the	O
first	O
representation	O
in	O
Figure	O
2	O
,	O
is	O
the	O
first	O
step	O
of	O
the	O
process	O
.	O

Although	O
the	O
references	O
to	O
discourse	O
entities	O
are	O
represented	O
by	O
general	O
tags	O
in	O
a	O
delexicalized	O
template	O
produced	O
in	O
the	O
generation	O
process	O
(	O
AGENT-1	O
,	O
BRIDGE-1	O
,	O
etc	O
.	O
)	O
,	O
for	O
the	O
purpose	O
of	O
disambiguation	O
,	O
NeuralREG	B-MethodName
's	O
inputs	O
have	O
the	O
references	O
represented	O
by	O
the	O
Wikipedia	O
ID	O
of	O
their	O
entities	O
.	O

References	O
to	O
other	O
discourse	O
entities	O
in	O
the	O
pre	O
-	O
and	O
pos	O
-	O
contexts	O
are	O
represented	O
by	O
their	O
Wikipedia	O
ID	O
,	O
whereas	O
constants	O
(	O
numbers	O
,	O
dates	O
)	O
are	O
represented	O
by	O
a	O
one	O
-	O
word	O
ID	O
removing	O
quotes	O
and	O
replacing	O
white	O
spaces	O
with	O
underscores	O
(	O
e.g.	O
,	O
120	O
million	O
(	O
Australian	O
dollars	O
)	O
for	O
"	O
120	O
million	O
(	O
Australian	O
dollars	O
)	O
"	O
in	O
Figure	O
2	O
)	O
.	O

pieces	O
of	O
text	O
before	O
and	O
after	O
the	O
target	O
reference	O
.	O

AGENT-1	O
has	O
a	O
total	O
of	O
PATIENT-4	O
floors	O
and	O
cost	O
PATIENT-3	O
.	O

Pre	O
-	O
and	O
pos	O
-	O
contexts	O
are	O
the	O
lowercased	O
,	O
tokenized	O
and	O
delexicalized	O
Tag	O
Entity	O
AGENT-1	O
108	O
St	O
Georges	O
Terrace	O
BRIDGE-1	O
Perth	O
PATIENT-1	O
Australia	O
PATIENT-2	O
1988@year	O
PATIENT-3	O
"	O
120	O
million	O
(	O
Australian	O
dollars)"@USD	O
PATIENT-4	O
50@Integer	O
AGENT-1	O
was	O
completed	O
in	O
PATIENT-2	O
in	O
BRIDGE-1	O
,	O
PATIENT-1	O
.	O

Each	O
instance	O
of	O
the	O
final	O
dataset	O
consists	O
of	O
a	O
truecased	O
tokenized	O
referring	O
expression	O
,	O
the	O
target	O
entity	O
(	O
distinguished	O
by	O
its	O
Wikipedia	O
ID	O
)	O
,	O
and	O
the	O
discourse	O
context	O
preceding	O
and	O
following	O
the	O
relevant	O
reference	O
(	O
we	O
refer	O
to	O
these	O
as	O
the	O
pre	O
-	O
and	O
pos	O
-	O
context	O
)	O
.	O

We	O
split	O
this	O
collection	O
in	O
training	O
,	O
developing	O
and	O
test	O
sets	O
,	O
totaling	O
63,061	O
,	O
7,097	O
and	O
8,743	O
referring	O
expressions	O
in	O
each	O
one	O
of	O
them	O
.	O

In	O
total	O
,	O
the	O
final	O
version	O
of	O
our	O
dataset	O
contains	O
78,901	O
referring	O
expressions	O
to	O
1,501	O
Wikipedia	O
entities	O
,	O
in	O
which	O
71.4	O
%	O
(	O
56,321	O
)	O
are	O
proper	O
names	O
,	O
5.6	O
%	O
(	O
4,467	O
)	O
pronouns	O
,	O
22.6	O
%	O
(	O
17,795	O
)	O
descriptions	O
and	O
0.4	O
%	O
(	O
318	O
)	O
demonstrative	O
referring	O
expressions	O
.	O

Using	O
the	O
delexicalized	B-DatasetName
version	I-DatasetName
of	I-DatasetName
the	I-DatasetName
WebNLG	I-DatasetName
corpus	O
,	O
we	O
automatically	O
extracted	O
all	O
referring	O
expressions	O
by	O
tokenizing	O
the	O
original	O
and	O
delexicalized	O
versions	O
of	O
the	O
texts	O
and	O
then	O
finding	O
the	O
non	O
overlapping	O
items	O
.	O

Once	O
all	O
texts	O
were	O
processed	O
and	O
the	O
referring	O
expressions	O
extracted	O
,	O
we	O
filtered	O
only	O
the	O
ones	O
referring	O
to	O
Wikipedia	O
entities	O
,	O
removing	O
references	O
to	O
constants	O
like	O
dates	O
and	O
numbers	O
,	O
for	O
which	O
no	O
references	O
are	O
generated	O
by	O
the	O
model	O
.	O

For	O
instance	O
,	O
by	O
processing	O
the	O
text	O
in	O
Figure	O
1	O
and	O
its	O
delexicalized	O
template	O
in	O
Figure	O
2	O
,	O
we	O
would	O
extract	O
referring	O
expressions	O
like	O
"	O
108	O
St	O
Georges	O
Terrace	O
"	O
and	O
"	O
It	O
"	O
to	O
AGENT-1	O
,	O
108	O
St	O
Georges	O
Terrace	O
,	O
"	O
Perth	O
"	O
to	O
BRIDGE-1	O
,	O
Perth	O
,	O
"	O
Australia	O
"	O
to	O
PATIENT-1	O
,	O
Australia	O
and	O
so	O
on	O
.	O

Referring	O
expression	O
collection	O
.	O

While	O
this	O
dataset	O
(	O
which	O
we	O
make	O
available	O
)	O
has	O
various	O
uses	O
,	O
we	O
used	O
it	O
to	O
extract	O
a	O
collection	O
of	O
referring	O
expressions	O
to	O
Wikipedia	O
entities	O
in	O
order	O
to	O
evaluate	O
how	O
well	O
our	O
REG	B-TaskName
model	O
can	O
produce	O
references	O
to	O
entities	O
throughout	O
a	O
(	O
small	O
)	O
text	O
.	O

Figure	O
2	O
shows	O
the	O
entity	O
mapping	O
and	O
the	O
delexicalized	O
template	O
for	O
the	O
example	O
in	O
Figure	O
1	O
in	O
its	O
versions	O
representing	O
the	O
references	O
with	O
general	O
tags	O
and	O
Wikipedia	O
IDs	O
.	O
We	O
delexicalized	O
20,198	O
distinct	O
texts	O
describing	O
7,812	O
distinct	O
sets	O
of	O
RDF	O
triples	O
,	O
resulting	O
in	O
16,628	O
distinct	O
templates	O
.	O

Once	O
all	O
entities	O
in	O
the	O
text	O
were	O
mapped	O
to	O
different	O
roles	O
,	O
the	O
first	O
two	O
authors	O
of	O
this	O
study	O
manually	O
replaced	O
the	O
referring	O
expressions	O
in	O
the	O
original	O
target	O
texts	O
by	O
their	O
respective	O
tags	O
.	O

Entities	O
which	O
appear	O
on	O
both	O
sides	O
in	O
the	O
relations	O
of	O
a	O
set	O
were	O
represented	O
as	O
BRIDGEs	O
.	O
To	O
distinguish	O
different	O
AGENTs	O
,	O
PATIENTs	O
and	O
BRIDGEs	O
in	O
a	O
set	O
,	O
an	O
ID	O
was	O
given	O
to	O
each	O
entity	O
of	O
each	O
kind	O
(	O
PATIENT-1	O
,	O
PATIENT-2	O
,	O
etc	O
.	O
)	O
.	O

All	O
entities	O
that	O
appear	O
on	O
the	O
left	O
and	O
right	O
side	O
of	O
the	O
triples	O
were	O
mapped	O
to	O
AGENTs	O
and	O
PATIENTs	O
,	O
respectively	O
.	O

We	O
delexicalized	O
the	O
training	O
and	O
development	O
parts	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
by	O
first	O
automatically	O
mapping	O
each	O
entity	O
in	O
the	O
source	O
representation	O
to	O
a	O
general	O
tag	O
.	O

Delexicalized	B-DatasetName
WebNLG	I-DatasetName
.	O

In	O
order	O
to	O
be	O
able	O
to	O
train	O
and	O
evaluate	O
our	O
models	O
for	O
referring	B-TaskName
expression	I-TaskName
generation	I-TaskName
(	O
the	O
topic	O
of	O
this	O
study	O
)	O
,	O
we	O
produced	O
a	O
delexicalized	O
version	O
of	O
the	O
original	O
corpus	O
.	O

The	O
corpus	O
consists	O
of	O
25,298	O
texts	O
describing	O
9,674	O
sets	O
of	O
up	O
to	O
7	O
RDF	O
triples	O
(	O
an	O
average	O
of	O
2.62	O
texts	O
per	O
set	O
)	O
in	O
15	O
domains	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

Figure	O
1	O
depicts	O
an	O
example	O
of	O
a	O
set	O
of	O
5	O
RDF	O
triples	O
and	O
the	O
corresponding	O
text	O
.	O

The	O
target	O
side	O
contains	O
English	O
texts	O
,	O
obtained	O
by	O
crowdsourcing	O
,	O
which	O
describe	O
the	O
source	O
triples	O
.	O

Each	O
RDF	O
triple	O
is	O
formed	O
by	O
a	O
Subject	O
,	O
Predicate	O
and	O
Object	O
,	O
where	O
the	O
Subject	O
and	O
Object	O
are	O
constants	O
or	O
Wikipedia	O
entities	O
,	O
and	O
predicates	O
represent	O
a	O
relation	O
between	O
these	O
two	O
elements	O
in	O
the	O
triple	O
.	O

The	O
source	O
side	O
of	O
the	O
corpus	O
are	O
sets	O
of	O
Resource	O
Description	O
Framework	O
(	O
RDF	O
)	O
triples	O
.	O

In	O
this	O
challenge	O
,	O
participants	O
had	O
to	O
automatically	O
convert	O
non	O
-	O
linguistic	O
data	O
from	O
the	O
Semantic	O
Web	O
into	O
a	O
textual	O
format	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

Our	O
data	O
is	O
based	O
on	O
the	O
WebNLG	B-DatasetName
corpus	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017a	O
)	O
,	O
which	O
is	O
a	O
parallel	O
resource	O
ini-	O
tially	O
released	O
for	O
the	O
eponymous	O
NLG	O
challenge	O
.	O

WebNLG	B-DatasetName
corpus	O
.	O

3	O
Data	O
and	O
processing	O
.	O

Below	O
we	O
describe	O
our	O
model	O
in	O
more	O
detail	O
,	O
as	O
well	O
as	O
the	O
data	O
on	O
which	O
we	O
develop	O
and	O
evaluate	O
it	O
.	O

In	O
contrast	O
,	O
we	O
introduce	O
NeuralREG	B-MethodName
,	O
an	O
end	O
-	O
to	O
-	O
end	O
approach	O
based	O
on	O
neural	O
networks	O
which	O
generates	O
referring	O
expressions	O
to	O
discourse	O
entities	O
directly	O
from	O
a	O
delexicalized	O
/	O
wikified	O
text	O
fragment	O
,	O
without	O
the	O
use	O
of	O
any	O
feature	O
extraction	O
technique	O
.	O

Moreover	O
,	O
many	O
of	O
these	O
models	O
only	O
address	O
part	O
of	O
the	O
problem	O
,	O
either	O
concentrating	O
on	O
the	O
choice	O
of	O
referential	O
form	O
or	O
on	O
deciding	O
on	O
the	O
contents	O
of	O
,	O
for	O
example	O
,	O
proper	O
names	O
or	O
definite	O
descriptions	O
.	O

Typically	O
,	O
these	O
features	O
are	O
extracted	O
automatically	O
from	O
the	O
context	O
,	O
and	O
engineering	O
relevant	O
ones	O
can	O
be	O
complex	O
.	O

In	O
sum	O
,	O
existing	O
REG	B-TaskName
models	O
for	O
text	O
generation	O
strongly	O
rely	O
on	O
abstract	O
features	O
such	O
as	O
the	O
salience	O
of	O
a	O
referent	O
for	O
deciding	O
on	O
the	O
form	O
or	O
content	O
of	O
a	O
referent	O
.	O

(	O
2010	O
)	O
.	O

More	O
details	O
about	O
the	O
models	O
can	O
be	O
seen	O
on	O
Belz	O
et	O
al	O
.	O

Some	O
participating	O
systems	O
approached	O
this	O
with	O
traditional	O
pipelines	O
for	O
selecting	O
referential	O
form	O
,	O
followed	O
by	O
referential	O
content	O
,	O
while	O
others	O
proposed	O
more	O
integrated	O
methods	O
.	O

The	O
input	O
for	O
the	O
models	O
were	O
texts	O
in	O
which	O
the	O
referring	O
expressions	O
to	O
the	O
topic	O
of	O
the	O
relevant	O
Wikipedia	O
entry	O
were	O
removed	O
and	O
appropriate	O
references	O
throughout	O
the	O
text	O
needed	O
to	O
be	O
generated	O
(	O
by	O
selecting	O
,	O
for	O
each	O
gap	O
,	O
from	O
a	O
list	O
of	O
candidate	O
referring	O
expressions	O
of	O
different	O
forms	O
and	O
with	O
different	O
contents	O
)	O
.	O

This	O
was	O
the	O
case	O
,	O
for	O
instance	O
,	O
in	O
the	O
GREC	B-DatasetName
shared	O
task	O
(	O
Belz	O
et	O
al	O
.	O
,	O
2010	O
)	O
,	O
which	O
aimed	O
to	O
evaluate	O
models	O
for	O
automatically	O
generated	O
referring	O
expressions	O
grounded	O
in	O
discourse	O
.	O

Of	O
course	O
,	O
when	O
texts	O
are	O
generated	O
in	O
practical	O
settings	O
,	O
both	O
form	O
and	O
content	O
need	O
to	O
be	O
chosen	O
.	O

(	O
2011);van	O
Deemter	O
(	O
2016	O
)	O
for	O
proper	O
names	O
.	O

To	O
this	O
end	O
,	O
separate	O
models	O
are	O
typically	O
used	O
,	O
including	O
,	O
for	O
example	O
,	O
Dale	O
and	O
Reiter	O
(	O
1995	O
)	O
for	O
generating	O
descriptions	O
,	O
and	O
Siddharthan	O
et	O
al	O
.	O

Importantly	O
,	O
these	O
models	O
do	O
not	O
specify	O
which	O
contents	O
a	O
particular	O
reference	O
,	O
be	O
it	O
a	O
proper	O
name	O
or	O
description	O
,	O
should	O
have	O
.	O

(	O
2016	O
)	O
proposed	O
a	O
data	O
-	O
driven	O
,	O
non	O
-	O
deterministic	O
model	O
for	O
generating	O
referential	O
forms	O
,	O
taking	O
into	O
account	O
salience	O
features	O
extracted	O
from	O
the	O
discourse	O
such	O
as	O
grammatical	O
position	O
,	O
givenness	O
and	O
recency	O
of	O
the	O
reference	O
.	O

More	O
recently	O
,	O
Castro	O
Ferreira	O
et	O
al	O
.	O

Reiter	O
and	O
Dale	O
(	O
2000	O
)	O
for	O
instance	O
,	O
discussed	O
a	O
straightforward	O
rule	O
-	O
based	O
method	O
based	O
on	O
this	O
notion	O
,	O
stating	O
that	O
full	O
proper	O
names	O
can	O
be	O
used	O
for	O
initial	O
references	O
,	O
typically	O
less	O
salient	O
than	O
subsequent	O
references	O
,	O
which	O
,	O
according	O
to	O
the	O
study	O
,	O
can	O
be	O
realized	O
by	O
a	O
pronoun	O
in	O
case	O
there	O
is	O
no	O
mention	O
to	O
any	O
other	O
entity	O
of	O
same	O
person	O
,	O
gender	O
and	O
number	O
between	O
the	O
reference	O
and	O
its	O
antecedents	O
.	O

Building	O
on	O
these	O
ideas	O
,	O
many	O
REG	B-TaskName
models	O
for	O
generating	O
references	O
in	O
texts	O
also	O
strongly	O
rely	O
on	O
the	O
concept	O
of	O
salience	O
and	O
factors	O
contributing	O
to	O
it	O
.	O

In	O
models	O
such	O
as	O
these	O
,	O
notions	O
like	O
salience	O
play	O
a	O
central	O
role	O
,	O
where	O
it	O
is	O
assumed	O
that	O
entities	O
which	O
are	O
salient	O
in	O
the	O
discourse	O
are	O
more	O
likely	O
to	O
be	O
referred	O
to	O
using	O
shorter	O
referring	O
expressions	O
(	O
like	O
a	O
pronoun	O
)	O
than	O
less	O
salient	O
entities	O
,	O
which	O
are	O
typically	O
referred	O
to	O
using	O
longer	O
expressions	O
(	O
like	O
full	O
proper	O
names	O
)	O
.	O

In	O
psycholinguistic	O
models	O
of	O
reference	O
,	O
various	O
linguistic	O
factors	O
have	O
been	O
proposed	O
as	O
influencing	O
the	O
form	O
of	O
referential	O
expressions	O
,	O
including	O
cognitive	O
status	O
(	O
Gundel	O
et	O
al	O
.	O
,	O
1993	O
)	O
,	O
centering	O
(	O
Grosz	O
et	O
al	O
.	O
,	O
1995	O
)	O
and	O
information	O
density	O
(	O
Jaeger	O
,	O
2010	O
)	O
.	O

There	O
is	O
,	O
however	O
,	O
a	O
lot	O
of	O
earlier	O
work	O
on	O
selecting	O
the	O
form	O
and	O
content	O
of	O
referring	O
expressions	O
,	O
both	O
in	O
psycholinguistics	O
and	O
in	O
computational	O
linguistics	O
.	O

However	O
,	O
the	O
usage	O
of	O
deep	O
neural	O
networks	O
for	O
REG	B-TaskName
has	O
remained	O
limited	O
and	O
we	O
are	O
not	O
aware	O
of	O
any	O
other	O
integrated	O
,	O
end	O
-	O
to	O
-	O
end	O
model	O
for	O
generating	B-TaskName
referring	I-TaskName
expressions	I-TaskName
in	O
discourse	O
.	O

In	O
recent	O
years	O
,	O
we	O
have	O
seen	O
a	O
surge	O
of	O
interest	O
in	O
using	O
(	O
deep	O
)	O
neural	O
networks	O
for	O
a	O
wide	O
range	O
of	O
NLG	O
-	O
related	O
tasks	O
,	O
as	O
the	O
generation	O
of	O
(	O
first	O
sentences	O
of	O
)	O
Wikipedia	O
entries	O
(	O
Lebret	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
poetry	O
(	O
Zhang	O
and	O
Lapata	O
,	O
2014	O
)	O
,	O
and	O
texts	O
from	O
abstract	O
meaning	O
representations	O
(	O
e.g.	O
,	O
Konstas	O
et	O
al	O
.	O
,	O
2017;Castro	O
Ferreira	O
et	O
al	O
.	O
,	O
2017a	O
)	O
.	O

Related	O
work	O
.	O

We	O
compare	O
NeuralREG	B-MethodName
against	O
two	O
baselines	O
in	O
an	O
automatic	O
and	O
human	O
evaluation	O
,	O
showing	O
that	O
the	O
integrated	O
neural	O
model	O
is	O
a	O
marked	O
improvement	O
.	O

Both	O
this	O
data	O
set	O
and	O
the	O
model	O
will	O
be	O
made	O
publicly	O
available	O
.	O

While	O
our	O
approach	O
,	O
dubbed	O
as	O
NeuralREG	B-MethodName
,	O
is	O
compatible	O
with	O
different	O
applications	O
of	O
REG	B-TaskName
models	O
,	O
in	O
this	O
paper	O
,	O
we	O
concentrate	O
on	O
the	O
last	O
one	O
,	O
relying	O
on	O
a	O
specifically	O
constructed	O
set	O
of	O
78,901	O
referring	O
expressions	O
to	O
1,501	O
entities	O
in	O
the	O
context	O
of	O
the	O
semantic	O
web	O
,	O
derived	O
from	O
a	O
(	O
delexicalized	O
)	O
version	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
(	O
Gardent	O
et	O
al	O
.	O
,	O
2017a	O
,	O
b	O
)	O
.	O

Based	O
on	O
the	O
delexicalized	O
input	O
,	O
the	O
model	O
generates	O
outputs	O
which	O
may	O
be	O
likened	O
to	O
templates	O
in	O
which	O
references	O
to	O
the	O
discourse	O
entities	O
are	O
not	O
realized	O
(	O
as	O
in	O
"	O
The	O
ground	O
of	O
ENTITY-1	O
is	O
located	O
in	O
ENTITY-2	O
.	O
"	O
)	O
.	O

Some	O
of	O
these	O
approaches	O
have	O
recently	O
focused	O
on	O
inputs	O
which	O
references	O
to	O
entities	O
are	O
delexicalized	O
to	O
general	O
tags	O
(	O
e.g.	O
,	O
ENTITY-1	O
,	O
ENTITY-2	O
)	O
in	O
order	O
to	O
decrease	O
data	O
sparsity	O
.	O

Besides	O
its	O
use	O
in	O
traditional	O
pipeline	O
NLG	O
systems	O
(	O
Reiter	O
and	O
Dale	O
,	O
2000	O
)	O
,	O
REG	B-TaskName
has	O
also	O
become	O
relevant	O
in	O
modern	O
"	O
end	O
-	O
to	O
-	O
end	O
"	O
NLG	O
approaches	O
,	O
which	O
perform	O
the	O
task	O
in	O
a	O
more	O
integrated	O
manner	O
(	O
see	O
e.g.	O
Konstas	O
et	O
al	O
.	O
,	O
2017;Gardent	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

Our	O
approach	O
is	O
based	O
on	O
neural	O
networks	O
which	O
generate	O
referring	O
expressions	O
to	O
discourse	O
entities	O
relying	O
on	O
the	O
surrounding	O
linguistic	O
context	O
,	O
without	O
the	O
use	O
of	O
any	O
feature	O
extraction	O
technique	O
.	O

Instead	O
,	O
in	O
this	O
paper	O
,	O
we	O
propose	O
NeuralREG	B-MethodName
:	O
an	O
end	O
-	O
to	O
-	O
end	O
approach	O
addressing	O
the	O
full	O
REG	B-TaskName
task	O
,	O
which	O
given	O
a	O
number	O
of	O
entities	O
in	O
a	O
text	O
,	O
produces	O
corresponding	O
referring	O
expressions	O
,	O
simultaneously	O
selecting	O
both	O
form	O
and	O
content	O
.	O

Most	O
of	O
the	O
earlier	O
REG	B-TaskName
approaches	O
focus	O
either	O
on	O
selecting	O
referential	O
form	O
(	O
Orita	O
et	O
al	O
.	O
,	O
2015;Castro	O
Ferreira	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
or	O
on	O
selecting	O
referential	O
content	O
,	O
typically	O
zooming	O
in	O
on	O
one	O
specific	O
kind	O
of	O
reference	O
such	O
as	O
a	O
pronoun	O
(	O
e.g.	O
,	O
Henschel	O
et	O
al	O
.	O
,	O
2000;Callaway	O
and	O
Lester	O
,	O
2002	O
)	O
,	O
definite	O
description	O
(	O
e.g.	O
,	O
Dale	O
and	O
Haddock	O
,	O
1991;Dale	O
and	O
Reiter	O
,	O
1995	O
)	O
or	O
proper	O
name	O
generation	O
(	O
e.g.	O
,	O
Siddharthan	O
et	O
al	O
.	O
,	O
2011;van	O
Deemter	O
,	O
2016;Castro	O
Ferreira	O
et	O
al	O
.	O
,	O
2017b	O
)	O
.	O

For	O
example	O
,	O
both	O
"	O
Frida	O
"	O
and	O
1	O
https://github.com/ThiagoCF05/	O
NeuralREG	O
"	O
Kahlo	O
"	O
are	O
name	O
-	O
variants	O
that	O
may	O
occur	O
in	O
a	O
text	O
,	O
and	O
she	O
can	O
alternatively	O
also	O
be	O
described	O
as	O
,	O
say	O
,	O
"	O
the	O
famous	O
female	O
painter	O
"	O
.	O

In	O
addition	O
,	O
the	O
REG	B-TaskName
model	O
must	O
account	O
for	O
the	O
different	O
ways	O
in	O
which	O
a	O
particular	O
referential	O
form	O
can	O
be	O
realized	O
.	O

First	O
,	O
the	O
referential	O
form	O
needs	O
to	O
be	O
decided	O
,	O
asking	O
whether	O
a	O
reference	O
at	O
a	O
given	O
point	O
in	O
the	O
text	O
should	O
assume	O
the	O
form	O
of	O
,	O
for	O
example	O
,	O
a	O
proper	O
name	O
(	O
"	O
Frida	O
Kahlo	O
"	O
)	O
,	O
a	O
pronoun	O
(	O
"	O
she	O
"	O
)	O
or	O
description	O
(	O
"	O
the	O
Mexican	O
painter	O
"	O
)	O
.	O

Referring	B-TaskName
Expression	I-TaskName
Generation	I-TaskName
(	O
REG	B-TaskName
)	O
,	O
the	O
task	O
responsible	O
for	O
generating	O
these	O
references	O
,	O
is	O
typically	O
presented	O
as	O
a	O
twostep	O
procedure	O
.	O

Since	O
the	O
input	O
data	O
will	O
often	O
consist	O
of	O
entities	O
and	O
the	O
relations	O
between	O
them	O
,	O
generating	O
references	O
for	O
these	O
entities	O
is	O
a	O
core	O
task	O
in	O
many	O
NLG	O
systems	O
(	O
Dale	O
and	O
Reiter	O
,	O
1995;Krahmer	O
and	O
van	O
Deemter	O
,	O
2012	O
)	O
.	O

Natural	O
Language	O
Generation	O
(	O
NLG	O
)	O
is	O
the	O
task	O
of	O
automatically	O
converting	O
non	O
-	O
linguistic	O
data	O
into	O
coherent	O
natural	O
language	O
text	O
(	O
Reiter	O
and	O
Dale	O
,	O
2000;Gatt	O
and	O
Krahmer	O
,	O
2018	O
)	O
.	O

Introduction	O
.	O

Data	O
and	O
models	O
are	O
publicly	O
available	O
1	O
.	O

Using	O
a	O
delexicalized	O
version	O
of	O
the	O
WebNLG	B-DatasetName
corpus	O
,	O
we	O
show	O
that	O
the	O
neural	O
model	O
substantially	O
improves	O
over	O
two	O
strong	O
baselines	O
.	O

In	O
this	O
paper	O
,	O
we	O
present	O
a	O
new	O
approach	O
(	O
NeuralREG	B-MethodName
)	O
,	O
relying	O
on	O
deep	O
neural	O
networks	O
,	O
which	O
makes	O
decisions	O
about	O
form	O
and	O
content	O
in	O
one	O
go	O
without	O
explicit	O
feature	O
extraction	O
.	O

Traditionally	O
,	O
Referring	B-TaskName
Expression	I-TaskName
Generation	I-TaskName
(	O
REG	B-TaskName
)	O
models	O
first	O
decide	O
on	O
the	O
form	O
and	O
then	O
on	O
the	O
content	O
of	O
references	O
to	O
discourse	O
entities	O
in	O
text	O
,	O
typically	O
relying	O
on	O
features	O
such	O
as	O
salience	O
and	O
grammatical	O
function	O
.	O

NeuralREG	B-MethodName
:	O
An	O
end	O
-	O
to	O
-	O
end	O
approach	O
to	O
referring	B-TaskName
expression	I-TaskName
generation	I-TaskName
.	O

