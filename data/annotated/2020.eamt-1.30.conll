We	O
have	O
also	O
experimented	O
with	O
different	O
forms	O
of	O
training	O
data	O
generated	O
from	O
the	O
BabelDr	B-MethodName
SCFG	O
.	O

Figure	O
1	O
provides	O
an	O
overview	O
of	O
the	O
ellipsis	B-TaskName
translation	I-TaskName
task	O
as	O
it	O
would	O
be	O
performed	O
in	O
BabelDr	B-MethodName
.	O
Starting	O
with	O
a	O
source	O
sentence	O
,	O
we	O
perform	O
ellipsis	B-TaskName
detection	I-TaskName
using	O
a	O
binary	O
classifier	O
(	O
support	B-MethodName
-	I-MethodName
vector	I-MethodName
machine	I-MethodName
)	O
trained	O
on	O
handcrafted	O
features	O
.	O

For	O
this	O
study	O
,	O
we	O
have	O
therefore	O
used	O
a	O
test	O
suite	O
based	O
on	O
the	O
Ba	B-MethodName
-	I-MethodName
belDr	I-MethodName
coverage	O
and	O
described	O
in	O
(	O
Rayner	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Since	O
the	O
currently	O
deployed	O
version	O
of	O
babelDr	B-MethodName
only	O
handles	O
ellipsis	O
in	O
a	O
limited	O
manner	O
,	O
doctors	O
were	O
instructed	O
to	O
use	O
only	O
complete	O
sentences	O
.	O

Ellipsis	O
in	O
BabelDr	B-MethodName
.	O
In	O
the	O
BabelDr	B-MethodName
context	O
,	O
instead	O
of	O
producing	O
a	O
literal	O
translation	O
of	O
the	O
ellipsis	O
,	O
we	O
aim	O
at	O
mapping	O
elliptical	O
utterances	O
to	O
the	O
closest	O
non	O
-	O
elliptical	O
core	O
sentence	O
,	O
for	O
which	O
translations	O
are	O
available	O
in	O
the	O
system	O
.	O

To	O
handle	O
sentences	O
that	O
are	O
out	O
of	O
grammar	O
coverage	O
,	O
BabelDr	B-MethodName
also	O
includes	O
a	O
large	O
vocabulary	O
recogniser	O
.	O

BabelDr	B-MethodName
is	O
a	O
speech	O
-	O
enabled	O
fixed	O
-	O
phrase	O
translator	O
designed	O
to	O
allow	O
French	O
speaking	O
doctors	O
to	O
carry	O
out	O
diagnostic	O
interviews	O
with	O
patients	O
with	O
whom	O
they	O
do	O
n't	O
have	O
any	O
common	O
language	O
in	O
emergency	O
settings	O
where	O
no	O
interpreters	O
are	O
available	O
.	O

2	O
The	O
context	O
:	O
BabelDr	B-MethodName
.	O
The	O
BabelDr	B-MethodName
system	O
.	O

The	O
aim	O
of	O
this	O
paper	O
is	O
to	O
compare	O
different	O
approaches	O
to	O
translate	O
ellipsis	O
in	O
the	O
context	O
of	O
Ba	B-MethodName
-	I-MethodName
belDr	I-MethodName
.	O
Section	O
2	O
describes	O
the	O
BabelDr	B-MethodName
system	O
.	O

In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
automatic	B-TaskName
translation	I-TaskName
of	I-TaskName
ellipsis	I-TaskName
in	O
medical	O
dialogues	O
,	O
in	O
the	O
particular	O
context	O
of	O
BabelDr	B-MethodName
,	O
a	O
speech	O
to	O
speech	O
translation	O
system	O
for	O
the	O
medical	O
domain	O
(	O
Spechbach	O
et	O
al	O
.	O
,	O
2019	O
)	O
2	O
.	O

In	O
this	O
work	O
,	O
we	O
evaluate	O
four	O
different	O
approaches	O
to	O
translate	B-TaskName
ellipsis	I-TaskName
in	O
medical	O
dialogues	O
in	O
the	O
context	O
of	O
the	O
speech	O
to	O
speech	O
translation	O
system	O
BabelDr	B-MethodName
.	O
We	O
also	O
investigate	O
the	O
impact	O
of	O
training	O
data	O
,	O
using	O
an	O
undersampling	O
method	O
and	O
data	O
with	O
elliptical	O
utterances	O
in	O
context	O
.	O

A	O
further	O
aspect	O
worth	O
investigating	O
is	O
exploring	O
novel	O
architectures	O
to	O
add	O
the	O
context	O
in	O
different	O
ways	O
:	O
train	O
a	O
context	O
aware	O
decoder	O
to	O
correct	O
translations	O
(	O
Voita	O
et	O
al	O
.	O
,	O
2019	O
,	O
for	O
neural	O
machine	O
translation	O
,	O
)	O
or	O
train	O
a	O
dual	B-MethodName
-	I-MethodName
source	I-MethodName
BERT	I-MethodName
(	O
Correia	O
and	O
Martins	O
,	O
2019	O
)	O
adding	O
context	O
on	O
the	O
tuning	O
step	O
for	O
sequence	B-TaskName
classification	I-TaskName
.	O

Each	O
source	O
variation	O
has	O
been	O
annotated	O
with	O
a	O
single	O
correct	O
core	O
sentence	O
,	O
but	O
this	O
does	O
not	O
reflect	O
the	O
real	O
use	O
case	O
:	O
the	O
purpose	O
of	O
BabelDr	B-MethodName
is	O
to	O
allow	O
doctors	O
to	O
collect	O
information	O
from	O
the	O
patient	O
,	O
not	O
to	O
translate	O
their	O
exact	O
utterance	O
.	O

Of	O
all	O
the	O
tested	O
systems	O
,	O
the	O
hybrid	O
approach	O
,	O
combining	O
neural	O
machine	O
translation	O
and	O
classification	O
models	O
is	O
the	O
most	O
successful	O
both	O
in	O
terms	O
of	O
our	O
task	O
specific	O
metric	O
(	O
SER	B-MetricName
)	O
and	O
in	O
terms	O
of	O
precision	B-MetricName
/	O
recall	B-MetricName
/	O
F1	B-MetricName
.	O

In	O
this	O
study	O
we	O
have	O
applied	O
different	O
approaches	O
to	O
an	O
ellipsis	B-TaskName
translation	I-TaskName
task	O
,	O
in	O
the	O
context	O
of	O
a	O
medical	O
speech	O
translator	O
.	O

With	O
these	O
results	O
,	O
we	O
confirmed	O
that	O
in	O
this	O
context	O
,	O
training	O
models	O
with	O
ellipsis	O
improves	O
performance	O
in	O
terms	O
of	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

11	O
%	O
of	O
the	O
sentences	O
were	O
classified	O
correctly	O
by	O
CamemBERT	B-MethodName
and	O
badly	O
by	O
LSTM	B-MethodName
,	O
and	O
4	O
%	O
the	O
other	O
way	O
around	O
.	O

with	O
the	O
additional	O
ellipsis	O
corpus	O
outperforms	O
the	O
one	O
trained	O
with	O
only	O
the	O
sampled	O
data	O
by	O
0.29	B-MetricValue
,	O
0.34	B-MetricValue
,	O
0.34	B-MetricValue
and	O
0.34	B-MetricValue
for	O
each	O
metric	O
respectively	O
.	O

For	O
those	O
sentences	O
that	O
the	O
hybrid	O
classifies	O
/	O
translates	O
adequately	O
,	O
52	O
%	O
are	O
well	O
translated	O
/	O
classified	O
by	O
both	O
models	O
,	O
20	O
%	O
by	O
LSTM	B-MethodName
only	O
and	O
the	O
rest	O
by	O
CamemBERT	B-MethodName
only	O
.	O

This	O
hybrid	O
achieved	O
0.23	B-MetricValue
and	O
0.50	B-MetricValue
on	O
elliptical	O
sentences	O
for	O
SER	B-MetricName
and	O
F1	B-MetricName
,	O
outperforming	O
the	O
best	O
model	O
by	O
0.21	B-MetricValue
and	O
0.11	B-MetricValue
for	O
those	O
metrics	O
respectively	O
.	O

Based	O
on	O
the	O
observation	O
that	O
sentences	O
that	O
were	O
not	O
well	O
classified	O
by	O
CamemBERT	B-MethodName
were	O
classified	O
correctly	O
by	O
LSTM	B-MethodName
,	O
we	O
decided	O
to	O
combine	O
LSTM	B-MethodName
and	O
camemBERT	B-MethodName
to	O
build	O
a	O
hybrid	O
system	O
.	O

However	O
,	O
LSTM	B-MethodName
outperforms	O
tf	O
-	O
idf	O
for	O
all	O
sentences	O
,	O
showing	O
that	O
LSTM	B-MethodName
is	O
better	O
suited	O
for	O
non	O
-	O
elliptical	O
sentences	O
.	O

For	O
elliptical	O
sentences	O
only	O
,	O
tf	B-MethodName
-	I-MethodName
idf	I-MethodName
is	O
the	O
second	O
best	O
approach	O
with	O
0.53	B-MetricValue
,	O
0.34	B-MetricValue
,	O
0.32	B-MetricValue
,	O
0.32	B-MetricValue
for	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

Classification	B-TaskName
,	O
with	O
CamemBERT	B-MethodName
,	O
achieves	O
the	O
best	O
scores	O
across	O
all	O
approaches	O
for	O
both	O
ellip	O
-	O
tical	O
and	O
all	O
sentences	O
.	O

Table	O
5	O
presents	O
the	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
for	O
elliptical	O
and	O
all	O
sentences	O
.	O

section	O
4	O
)	O
,	O
except	O
for	O
machine	O
translation	O
where	O
we	O
already	O
chose	O
LSTM	B-MethodName
(	O
cf	O
.	O

Accordingly	O
,	O
we	O
carried	O
out	O
the	O
subsequent	O
experiments	O
using	O
the	O
LSTM	B-MethodName
model	O
for	O
the	O
machine	O
translation	O
approach	O
.	O

Because	O
of	O
training	O
data	O
size	O
and	O
number	O
of	O
parameters	O
,	O
training	O
time	O
was	O
considerably	O
lower	O
for	O
the	O
LSTM	B-MethodName
architecture	O
with	O
sampled	O
data	O
.	O

Regarding	O
the	O
machine	B-TaskName
translation	I-TaskName
approaches	O
,	O
while	O
results	O
suggest	O
that	O
both	O
architectures	O
are	O
suitable	O
for	O
the	O
task	O
,	O
we	O
observe	O
that	O
LSTMsampled	B-MethodName
and	O
LSTM	B-MethodName
slightly	O
outperform	O
Transformer	B-MethodName
and	O
Transformer	B-MethodName
-	I-MethodName
sampled	I-MethodName
on	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

We	O
observe	O
that	O
the	O
proposed	O
under	O
-	O
sampling	O
method	O
(	O
fastText	B-MethodName
-	I-MethodName
sampled	I-MethodName
,	O
LSTM	B-MethodName
-	I-MethodName
sampled	I-MethodName
and	O
Transformer	B-MethodName
-	I-MethodName
sampled	I-MethodName
)	O
produces	O
better	O
results	O
in	O
this	O
particular	O
context	O
indicating	O
that	O
a	O
more	O
balanced	O
data	O
set	O
improves	O
performance	O
in	O
terms	O
of	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

We	O
then	O
compared	O
performance	O
by	O
calculating	O
SER	B-MetricName
,	O
precision	B-MetricName
,	O
recall	B-MetricName
and	O
F1	B-MetricName
.	O

To	O
evaluate	O
the	O
under	O
-	O
sampling	O
method	O
,	O
we	O
ran	O
the	O
experiment	O
with	O
two	O
approaches	O
,	O
machine	B-TaskName
translation	I-TaskName
(	O
LSTM	B-MethodName
,	O
Transformer	B-MethodName
)	O
and	O
classification	B-TaskName
(	O
fast	B-MethodName
-	I-MethodName
Text	I-MethodName
)	O
,	O
trained	O
with	O
two	O
different	O
data	O
sets	O
:	O
undersampled	O
data	O
(	O
hereafter	O
sampled	O
)	O
and	O
all	O
data	O
.	O

Finally	O
,	O
including	O
only	O
the	O
best	O
model	O
for	O
each	O
approach	O
in	O
terms	O
of	O
F1	B-MetricName
,	O
we	O
evaluate	O
the	O
impact	O
of	O
training	O
on	O
Ellipsis	O
data	O
(	O
subsection	O
6.3	O
)	O
.	O

To	O
select	O
the	O
best	O
result	O
in	O
this	O
list	O
,	O
we	O
used	O
the	O
log	B-MetricName
probability	I-MetricName
of	O
the	O
generated	O
core	O
sentence	O
from	O
the	O
neural	O
machine	O
translation	O
:	O
if	O
it	O
was	O
below	O
a	O
threshold	B-HyperparameterName
(	O
<	B-HyperparameterValue
−0.25	I-HyperparameterValue
)	O
,	O
we	O
kept	O
the	O
core	O
sentence	O
generated	O
by	O
the	O
classifier	O
,	O
else	O
we	O
kept	O
the	O
NMT	O
result	O
.	O

We	O
used	O
OpenNMT	B-MethodName
(	O
Klein	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
train	O
the	O
models	O
.	O

The	O
model	O
was	O
trained	O
with	O
a	O
dropout	B-HyperparameterName
rate	I-HyperparameterName
of	O
0.3	B-HyperparameterValue
and	O
a	O
batch	B-HyperparameterName
size	I-HyperparameterName
of	O
64	B-HyperparameterValue
examples	O
.	O

Encoder	O
and	O
decoder	O
were	O
each	O
composed	O
of	O
two	O
LSTM	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997	O
)	O
with	O
an	O
attention	O
mechanism	O
on	O
the	O
decoder	O
side	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014;Luong	O
et	O
al	O
.	O
,	O
2015	O
)	O
.	O

We	O
have	O
trained	O
two	O
different	O
NMT	O
models	O
:	O
LSTM	B-MethodName
We	O
trained	O
a	O
neural	O
machine	O
translation	O
model	O
with	O
an	O
embedding	B-HyperparameterName
size	I-HyperparameterName
of	O
512	B-HyperparameterValue
in	O
the	O
encoder	O
and	O
decoder	O
.	O

Machine	B-TaskName
Translation	I-TaskName
.	O

fastText	B-MethodName
The	O
second	O
approach	O
uses	O
a	O
sequence	O
classification	O
baseline	O
based	O
on	O
bag	O
of	O
tricks	O
(	O
Joulin	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

To	O
do	O
so	O
,	O
we	O
set	O
-	O
up	O
10	B-HyperparameterValue
epochs	B-HyperparameterName
using	O
the	O
Transformer	O
framework	O
for	O
python	O
(	O
Wolf	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

We	O
used	O
the	O
CamemBERT	B-MethodName
pre	O
-	O
trained	O
model	O
(	O
Martin	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
added	O
a	O
classification	O
layer	O
on	O
top	O
of	O
the	O
model	O
to	O
fine	O
-	O
tune	O
it	O
with	O
our	O
data	O
(	O
Sun	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

We	O
trained	O
two	O
different	O
neural	O
classifiers	O
:	O
CamemBERT	B-MethodName
This	O
classifier	O
uses	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
French	O
based	O
on	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
which	O
is	O
used	O
for	O
many	O
NLP	O
tasks	O
.	O

Sequence	B-TaskName
Classification	I-TaskName
.	O

To	O
encode	O
each	O
source	O
sentence	O
,	O
we	O
used	O
an	O
already	O
trained	O
Universal	B-MethodName
Sentence	I-MethodName
Encoder	I-MethodName
4	O
(	O
hereafter	O
uencoder	O
)	O
.	O

Universal	B-MethodName
Sentence	I-MethodName
Encoder	I-MethodName
The	O
second	O
approach	O
uses	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
multilingual	O
encoding	O
(	O
Chidambaram	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Common	O
pre	O
-	O
processing	O
methods	O
for	O
tf	B-MethodName
-	I-MethodName
idf	I-MethodName
are	O
lemmatizing	O
and	O
removing	O
stop	O
words	O
;	O
however	O
,	O
since	O
accurate	O
preservation	O
of	O
meaning	O
is	O
imperative	O
in	O
a	O
medical	O
dialog	O
context	O
,	O
e.g.	O
in	O
terms	O
of	O
verb	O
tenses	O
,	O
in	O
our	O
experiments	O
words	O
were	O
left	O
as	O
word	O
forms	O
.	O

We	O
employed	O
two	O
approaches	O
to	O
embed	O
each	O
sentence	O
:	O
tf	B-MethodName
-	I-MethodName
idf	I-MethodName
The	O
first	O
approach	O
uses	O
a	O
customised	B-MethodName
tf	I-MethodName
-	I-MethodName
idf	I-MethodName
(	O
Salton	O
and	O
Buckley	O
,	O
1988	O
)	O
,	O
where	O
tf	B-MethodName
-	I-MethodName
idf	I-MethodName
was	O
applied	O
to	O
subword	O
occurrences	O
(	O
two	O
to	O
four	O
characters	O
)	O
in	O
variations	O
for	O
a	O
given	O
core	O
sentence	O
.	O

This	O
method	O
achieves	O
98	B-MetricValue
%	I-MetricValue
of	O
accuracy	B-MetricName
on	O
ellipsis	B-TaskName
detection	I-TaskName
.	O

We	O
could	O
have	O
applied	O
the	O
standard	O
BLEU	B-MetricName
score	O
for	O
the	O
evaluation	O
of	O
the	O
MT	B-TaskName
approaches	O
,	O
but	O
since	O
it	O
is	O
not	O
applicable	O
to	O
the	O
other	O
approaches	O
,	O
it	O
is	O
not	O
appropriate	O
for	O
our	O
comparison	O
.	O

Since	O
the	O
target	O
is	O
a	O
finite	O
set	O
of	O
sentences	O
,	O
we	O
also	O
measured	O
system	O
performance	O
on	O
the	O
test	O
data	O
using	O
three	O
standard	O
metrics	O
for	O
classification	B-TaskName
:	O
recall	B-MetricName
,	O
precision	B-MetricName
and	O
F1	B-MetricName
.	O

We	O
therefore	O
measured	O
the	O
sentence	B-MetricName
error	I-MetricName
rate	I-MetricName
(	O
SER	B-MetricName
)	O
,	O
defined	O
as	O
the	O
percentage	O
of	O
utterances	O
for	O
which	O
the	O
resulting	O
core	O
sentence	O
is	O
not	O
identical	O
to	O
the	O
annotated	O
correct	O
core	O
sentence	O
.	O

The	O
aim	O
of	O
this	O
study	O
is	O
to	O
evaluate	O
the	O
performance	O
of	O
four	O
different	O
approaches	O
for	O
the	O
ellipsis	B-TaskName
translation	I-TaskName
task	O
:	O
indexing	O
,	O
classification	O
,	O
neural	O
machine	O
translation	O
and	O
hybrid	O
.	O

Previous	O
studies	O
have	O
focused	O
on	O
automatic	O
ellipsis	B-TaskName
detection	I-TaskName
and	O
resolution	O
,	O
but	O
only	O
few	O
specifically	O
address	O
the	O
problem	O
of	O
automatic	O
translation	O
of	O
ellipsis	O
.	O

Ellipsis	B-TaskName
Translation	I-TaskName
for	O
a	O
Medical	O
Speech	O
to	O
Speech	O
Translation	O
System	O
In	O
diagnostic	O
interviews	O
,	O
elliptical	O
utterances	O
allow	O
doctors	O
to	O
question	O
patients	O
in	O
a	O
more	O
efficient	O
and	O
economical	O
way	O
.	O

However	O
,	O
only	O
few	O
studies	O
specifically	O
address	O
this	O
problem	O
in	O
machine	B-TaskName
translation	I-TaskName
(	O
MT	B-TaskName
)	O
,	O
despite	O
the	O
recent	O
interest	O
for	O
context	O
modelling	O
in	O
neural	B-TaskName
machine	I-TaskName
translation	I-TaskName
(	O
see	O
for	O
example	O
,	O
Bawden	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

In	O
NLP	O
,	O
different	O
studies	O
have	O
focused	O
on	O
automatic	B-TaskName
ellipsis	I-TaskName
detection	I-TaskName
and	O
resolution	O
either	O
with	O
rules	O
(	O
patterns	O
or	O
grammars	O
)	O
(	O
for	O
example	O
,	O
the	O
pioneer	O
work	O
from	O
Hardt	O
,	O
1992	O
)	O
or	O
classification	O
techniques	O
(	O
for	O
example	O
,	O
Hardt	O
and	O
Rambow	O
,	O
2001;Bos	O
and	O
Spenader	O
,	O
2011;Liu	O
et	O
al	O
.	O
,	O
2016;Kenyon	O
-	O
Dean	O
et	O
al	O
.	O
,	O
2016;McShane	O
and	O
Babkin	O
,	O
2016;Rønning	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Like	O
anaphora	O
,	O
ellipsis	O
require	O
context	O
to	O
be	O
understood	O
,	O
but	O
contrary	O
to	O
anaphora	O
,	O
there	O
is	O
no	O
indicator	O
that	O
there	O
is	O
a	O
missing	O
part	O
in	O
the	O
sentence	O
1	O
.	O

This	O
project	O
is	O
funded	O
by	O
the	O
"	O
Fondation	O
Privée	O
des	O
Hôpitaux	O
Universitaires	O
de	O
Genève	O
"	O
.	O

Acknowledgements	O
.	O

Finally	O
,	O
future	O
work	O
will	O
also	O
include	O
the	O
replication	O
of	O
these	O
experiments	O
with	O
data	O
from	O
real	O
diagnostic	O
interviews	O
and	O
with	O
data	O
from	O
other	O
diagnostic	O
domains	O
.	O

In	O
future	O
work	O
,	O
a	O
more	O
task	O
-	O
oriented	O
annotation	O
approach	O
would	O
be	O
interesting	O
.	O

Often	O
,	O
even	O
if	O
the	O
core	O
sentence	O
is	O
not	O
an	O
exact	O
match	O
(	O
e.g.	O
"	O
"	O
in	O
the	O
lower	O
part	O
"	O
vs	O
"	O
in	O
the	O
lower	O
part	O
of	O
the	O
abdomen	O
"	O
)	O
,	O
in	O
context	O
it	O
still	O
allows	O
the	O
doctor	O
to	O
obtain	O
the	O
required	O
information	O
.	O

One	O
limitation	O
of	O
this	O
study	O
is	O
the	O
annotation	O
of	O
the	O
test	O
data	O
.	O

We	O
also	O
observe	O
that	O
the	O
inclusion	O
of	O
ellipsis	O
training	O
data	O
further	O
improves	O
results	O
.	O

Results	O
show	O
that	O
under	O
-	O
sampling	O
the	O
training	O
data	O
improves	O
results	O
for	O
all	O
tested	O
approaches	O
.	O

Conclusion	O
.	O

We	O
also	O
observe	O
many	O
cases	O
where	O
the	O
core	O
sentence	O
was	O
very	O
close	O
to	O
the	O
correct	O
one	O
,	O
but	O
more	O
or	O
less	O
generic	O
.	O

Some	O
of	O
the	O
classification	O
errors	O
were	O
due	O
to	O
ambiguous	O
cases	O
where	O
more	O
than	O
one	O
core	O
sentence	O
would	O
be	O
appropriate	O
for	O
a	O
given	O
elliptical	O
utterance	O
.	O

Closer	O
investigation	O
of	O
the	O
15	O
%	O
of	O
elliptical	O
sentences	O
which	O
were	O
badly	O
classified	O
revealed	O
several	O
cases	O
.	O

We	O
observed	O
that	O
85	O
%	O
of	O
the	O
elliptical	O
sentences	O
were	O
well	O
classified	O
by	O
both	O
models	O
.	O

Table	O
5	O
)	O
.	O

With	O
the	O
additional	O
ellipsis	O
training	O
data	O
,	O
Hybrid	O
also	O
outperforms	O
the	O
other	O
approaches	O
(	O
88	O
%	O
of	O
elliptical	O
utterances	O
are	O
translated	O
correctly	O
)	O
,	O
yet	O
the	O
difference	O
is	O
not	O
as	O
large	O
as	O
with	O
plain	O
training	O
data	O
only	O
(	O
cf	O
.	O

To	O
determine	O
if	O
the	O
inclusion	O
of	O
ellipsis	O
data	O
in	O
the	O
training	O
data	O
affects	O
performance	O
,	O
we	O
selected	O
the	O
three	O
best	O
models	O
based	O
on	O
the	O
results	O
described	O
in	O
the	O
previous	O
section	O
and	O
trained	O
them	O
with	O
the	O
ellipsis	O
corpus	O
described	O
in	O
section	O
5.3	O
in	O
addition	O
to	O
the	O
sampled	O
training	O
data	O
.	O

Ellipsis	O
Training	O
Data	O
.	O

subsection	O
6.1	O
)	O
.	O

In	O
order	O
to	O
select	O
the	O
best	O
approach	O
and	O
model	O
to	O
handle	O
ellipsis	O
in	O
this	O
context	O
,	O
we	O
measured	O
the	O
performance	O
of	O
two	O
different	O
models	O
for	O
each	O
approach	O
(	O
cf	O
.	O

Approaches	O
.	O

Under	O
-	O
sampling	O
.	O

We	O
then	O
give	O
results	O
for	O
different	O
models	O
trained	O
with	O
under	O
-	O
sampled	O
data	O
(	O
subsection	O
6.2	O
)	O
.	O

In	O
this	O
section	O
we	O
first	O
describe	O
the	O
evaluation	O
of	O
the	O
under	O
-	O
sampling	O
method	O
(	O
subsection	O
6.1	O
)	O
.	O

Results	O
.	O

The	O
same	O
concatenation	O
was	O
performed	O
on	O
the	O
test	O
data	O
.	O

To	O
train	O
the	O
models	O
,	O
we	O
transformed	O
the	O
elliptical	O
source	O
variations	O
by	O
concatenating	O
them	O
with	O
the	O
context	O
source	O
variation	O
.	O

Each	O
of	O
these	O
elliptical	O
variation	O
-	O
core	O
pairs	O
follows	O
a	O
matching	O
complete	O
variation	O
-	O
core	O
pair	O
which	O
serves	O
as	O
context	O
,	O
as	O
shown	O
in	O
Table	O
3	O
.	O

To	O
produce	O
elliptical	O
utterances	O
,	O
we	O
have	O
kept	O
only	O
the	O
value	O
of	O
the	O
variable	O
as	O
source	O
variation	O
,	O
associated	O
with	O
a	O
corresponding	O
complete	O
core	O
sentence	O
.	O

]	O
recently	O
?	O
"	O
)	O
.	O

These	O
variables	O
are	O
placeholders	O
that	O
are	O
replaced	O
by	O
different	O
values	O
at	O
system	O
-	O
compile	O
time	O
,	O
e.g.	O
"	O
avez	O
-	O
vous	O
pris	O
[	O
des	O
anti	O
-	O
douleurs|des	O
medicaments	O
contre	O
l'acidité|	O
...	O
]	O
récemment	O
?	O
"	O
(	O
"	O
Did	O
you	O
take	O
[	O
painkillers|antacids|	O
.	O

To	O
generate	O
training	O
data	O
for	O
ellipsis	O
in	O
context	O
,	O
we	O
exploit	O
grammar	O
rules	O
that	O
contain	O
variables	O
.	O

Ellipsis	O
Corpus	O
.	O

For	O
example	O
,	O
"	O
avez	O
-	O
vous	O
mal	O
au	O
ventre	O
en	O
position	O
de	O
chien	O
de	O
fusil	O
?	O
"	O
(	O
do	O
you	O
have	O
abdominal	O
pain	O
in	O
a	O
fetal	O
position	O
?	O
)	O
still	O
had	O
731	O
variations	O
whereas	O
"	O
combien	O
de	O
kilos	O
avezvous	O
pris	O
?	O
"	O
(	O
how	O
much	O
weight	O
did	O
you	O
gain	O
?	O
)	O
had	O
only	O
1	O
.	O

Even	O
though	O
we	O
managed	O
to	O
reduce	O
most	O
of	O
the	O
categories	O
,	O
minority	O
classes	O
were	O
still	O
under	O
-	O
represented	O
compared	O
to	O
the	O
majority	O
classes	O
.	O

Furthermore	O
,	O
75	O
%	O
of	O
the	O
core	O
sentences	O
were	O
mapped	O
to	O
less	O
than	O
32	O
variations	O
.	O

After	O
under	O
-	O
sampling	O
,	O
the	O
resulting	O
corpus	O
contained	O
159'902	O
variations	O
and	O
87	O
ambiguous	O
samples	O
.	O

Build	O
a	O
new	O
list	O
of	O
variations	O
by	O
iteratively	O
extracting	O
variations	O
from	O
a	O
list	O
in	O
randomised	O
order	O
until	O
all	O
bigrams	O
are	O
covered	O
.	O

2	O
.	O

For	O
each	O
core	O
sentence	O
,	O
extract	O
all	O
bigrams	O
present	O
in	O
the	O
associated	O
variations	O
.	O

To	O
reduce	O
the	O
number	O
of	O
variations	O
by	O
core	O
sentence	O
while	O
keeping	O
data	O
as	O
representative	O
as	O
possible	O
,	O
we	O
propose	O
a	O
new	O
algorithm	O
for	O
undersampling	O
based	O
on	O
bigrams	O
consisting	O
in	O
the	O
following	O
steps	O
:	O
1	O
.	O

We	O
applied	O
under	O
-	O
sampling	O
,	O
which	O
is	O
suggested	O
as	O
the	O
best	O
alternative	O
when	O
the	O
training	O
sample	O
size	O
is	O
too	O
large	O
(	O
Mazurowski	O
et	O
al	O
.	O
,	O
2008;Haixiang	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Therefore	O
,	O
we	O
used	O
resampling	O
techniques	O
to	O
rebalance	O
the	O
sample	O
space	O
in	O
order	O
to	O
alleviate	O
the	O
effect	O
of	O
the	O
skewed	O
class	O
distribution	O
on	O
the	O
learning	O
process	O
.	O

In	O
this	O
context	O
,	O
where	O
all	O
core	O
sentences	O
are	O
relevant	O
for	O
the	O
task	O
,	O
the	O
exclusion	O
or	O
misclassification	O
/	O
translation	O
of	O
minority	O
categories	O
(	O
in	O
our	O
case	O
,	O
core	O
sentences	O
)	O
on	O
the	O
dataset	O
could	O
lead	O
to	O
a	O
heavy	O
cost	O
(	O
Haixiang	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

As	O
mentioned	O
in	O
the	O
previous	O
section	O
,	O
our	O
main	O
corpus	O
is	O
highly	O
imbalanced	O
.	O

Sampled	O
Data	O
.	O

Since	O
we	O
are	O
interested	O
in	O
evaluating	O
the	O
complete	O
set	O
of	O
core	O
sentences	O
,	O
we	O
have	O
maintained	O
the	O
same	O
distribution	O
when	O
splitting	O
the	O
data	O
into	O
development	O
and	O
training	O
.	O

For	O
example	O
,	O
the	O
core	O
sentence	O
"	O
avez	O
-	O
vous	O
pris	O
des	O
médicaments	O
contre	O
la	O
douleur	O
?	O
"	O
(	O
have	O
you	O
taken	O
any	O
painkillers	O
?	O
)	O
is	O
mapped	O
to	O
3'496'503	O
source	O
variations	O
(	O
14	O
%	O
of	O
the	O
entire	O
dataset	O
)	O
whereas	O
"	O
avez	O
-	O
vous	O
de	O
l'oxygène	O
à	O
la	O
maison	O
?	O
"	O
(	O
do	O
you	O
have	O
oxygen	O
at	O
home	O
?	O
)	O
is	O
only	O
mapped	O
to	O
5	O
source	O
variations	O
.	O

These	O
core	O
sentences	O
are	O
not	O
represented	O
equally	O
in	O
the	O
corpus	O
:	O
50	O
%	O
of	O
the	O
4'132	O
core	O
sentences	O
occur	O
less	O
than	O
52	O
times	O
in	O
the	O
data	O
.	O

The	O
variations	O
are	O
mapped	O
to	O
the	O
4'132	O
different	O
core	O
sentences	O
available	O
for	O
the	O
abdominal	O
domain	O
.	O

Table	O
2	O
shows	O
two	O
examples	O
of	O
such	O
sentences	O
.	O

Most	O
of	O
these	O
ambiguous	O
sentences	O
are	O
elliptical	O
.	O

The	O
main	O
data	O
set	O
includes	O
23	O
M	O
variations	O
,	O
of	O
which	O
321'698	O
are	O
ambiguous	O
(	O
i.e.	O
sentences	O
that	O
can	O
be	O
mapped	O
to	O
more	O
than	O
one	O
core	O
sentence	O
)	O
.	O

All	O
Data	O
.	O

Table	O
1	O
summarises	O
the	O
number	O
of	O
sentences	O
,	O
words	O
and	O
vocabulary	O
for	O
each	O
set	O
.	O

All	O
data	O
were	O
generated	O
6	O
https://opennmt.net/OpenNMT-tf/model.html	O
from	O
a	O
recent	O
version	O
of	O
the	O
BabelDr	O
SCFG	O
for	O
the	O
abdominal	O
diagnostic	O
domain	O
and	O
consist	O
of	O
variation	O
-	O
core	O
pairs	O
.	O

In	O
this	O
section	O
,	O
we	O
describe	O
the	O
training	O
data	O
sets	O
used	O
for	O
this	O
study	O
.	O

Training	O
Data	O
.	O

The	O
threshold	O
was	O
set	O
based	O
on	O
the	O
observation	O
that	O
93	O
%	O
of	O
the	O
sentences	O
above	O
that	O
threshold	O
were	O
mistranslated	O
.	O

The	O
hybrid	O
approach	O
combines	O
the	O
best	O
neural	O
machine	O
translation	O
model	O
with	O
the	O
best	O
classification	O
model	O
to	O
build	O
an	O
N	O
-	O
best	O
list	O
of	O
sentences	O
,	O
in	O
this	O
experiment	O
a	O
2	O
-	O
best	O
list	O
which	O
includes	O
the	O
core	O
sentence	O
generated	O
by	O
machine	O
translation	O
and	O
one	O
sentence	O
from	O
the	O
classification	O
results	O
.	O

Hybrid	O
.	O

For	O
both	O
architectures	O
,	O
early	O
stopping	O
was	O
used	O
to	O
reduce	O
the	O
number	O
of	O
training	O
steps	O
by	O
monitoring	O
the	O
performance	O
on	O
the	O
development	O
set	O
.	O

Transformer	O
The	O
second	O
model	O
relies	O
on	O
a	O
transformer	O
based	O
architecture	O
for	O
machine	O
translation	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
with	O
default	O
parameters	O
and	O
size	O
6	O
.	O

This	O
system	O
is	O
described	O
in	O
detail	O
in	O
(	O
Mutal	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

With	O
these	O
approaches	O
,	O
the	O
task	O
is	O
to	O
translate	O
the	O
source	O
utterance	O
into	O
a	O
core	O
sentence	O
.	O

The	O
other	O
hyper	O
parameters	O
were	O
set	O
by	O
default	O
5	O
.	O

We	O
used	O
fastText	O
on	O
bigrams	O
with	O
100	O
epochs	O
and	O
a	O
learning	O
rate	O
of	O
0,2	O
.	O

In	O
this	O
approach	O
,	O
the	O
task	O
is	O
to	O
classify	O
each	O
variation	O
into	O
a	O
core	O
sentence	O
using	O
a	O
distance	O
based	O
classification	O
method	O
(	O
Xing	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

We	O
then	O
used	O
the	O
approximate	O
nearest	O
neighbor	O
search	O
(	O
Andoni	O
and	O
Indyk	O
,	O
2006	O
)	O
to	O
extract	O
the	O
closest	O
variation	O
sentence	O
with	O
cosine	O
similarity	O
,	O
and	O
return	O
the	O
corresponding	O
core	O
sentence	O
.	O

To	O
do	O
so	O
,	O
each	O
sentence	O
was	O
represented	O
by	O
a	O
vector	O
and	O
a	O
similarity	O
metric	O
was	O
used	O
to	O
compare	O
them	O
.	O

In	O
this	O
approach	O
,	O
the	O
task	O
is	O
to	O
find	O
the	O
source	O
variations	O
that	O
are	O
the	O
closest	O
matches	O
for	O
a	O
new	O
utterance	O
.	O

Indexing	O
.	O

Each	O
approach	O
has	O
its	O
own	O
built	O
-	O
in	O
tokenization	O
method	O
to	O
reach	O
optimal	O
results	O
,	O
except	O
for	O
machine	O
translation	O
where	O
we	O
applied	O
BPE	O
.	O

The	O
source	O
sentences	O
were	O
preprocessed	O
using	O
the	O
same	O
method	O
for	O
all	O
the	O
models	O
:	O
they	O
have	O
been	O
lower	O
cased	O
and	O
tokenized	O
.	O

The	O
same	O
training	O
data	O
(	O
described	O
in	O
Section	O
5	O
)	O
was	O
used	O
for	O
all	O
approaches	O
.	O

In	O
the	O
following	O
sections	O
,	O
we	O
describe	O
the	O
four	O
approaches	O
applied	O
after	O
concatenation	O
.	O

This	O
concatenated	O
sentence	O
is	O
then	O
processed	O
like	O
other	O
utterances	O
.	O

If	O
the	O
utterance	O
is	O
identified	O
as	O
an	O
ellipsis	O
,	O
it	O
is	O
concatenated	O
with	O
the	O
previous	O
utterance	O
from	O
the	O
dialog	O
(	O
Tiedemann	O
and	O
Scherrer	O
,	O
2017	O
)	O
.	O

Therefore	O
,	O
the	O
sentence	O
length	O
,	O
the	O
first	O
word	O
of	O
the	O
sentence	O
and	O
its	O
part	O
-	O
of	O
-	O
speech	O
are	O
used	O
as	O
features	O
to	O
train	O
the	O
classifier	O
.	O

In	O
this	O
context	O
,	O
elliptical	O
sentences	O
can	O
easily	O
be	O
detected	O
by	O
sentence	O
length	O
and	O
syntactic	O
structure	O
.	O

As	O
mentioned	O
earlier	O
,	O
our	O
objective	O
is	O
to	O
use	O
the	O
context	O
(	O
previous	O
utterance	O
)	O
to	O
map	O
elliptical	O
utterances	O
to	O
the	O
closest	O
core	O
sentence	O
.	O

Approaches	O
.	O

The	O
metrics	O
were	O
calculated	O
using	O
a	O
module	O
in	O
Sklearn	O
3	O
.	O

The	O
macroaverage	O
better	O
reflects	O
the	O
statistics	O
of	O
the	O
smaller	O
classes	O
and	O
therefore	O
is	O
more	O
appropriate	O
when	O
all	O
classes	O
are	O
equally	O
important	O
(	O
Jurafsky	O
and	O
Martin	O
,	O
2014	O
)	O
.	O

As	O
the	O
test	O
data	O
is	O
not	O
perfectly	O
balanced	O
,	O
we	O
computed	O
the	O
performance	O
for	O
each	O
class	O
,	O
and	O
then	O
averaged	O
over	O
the	O
number	O
of	O
classes	O
,	O
i.e.	O
by	O
macro	O
-	O
averaging	O
.	O

Section	O
2	O
)	O
,	O
a	O
correct	O
core	O
sentence	O
is	O
equivalent	O
to	O
a	O
correct	O
translation	O
.	O

Since	O
the	O
system	O
relies	O
on	O
human	O
pre	O
-	O
translation	O
(	O
cf	O
.	O

We	O
want	O
to	O
compare	O
the	O
different	O
approaches	O
at	O
the	O
task	O
level	O
,	O
namely	O
how	O
many	O
elliptical	O
utterances	O
will	O
result	O
in	O
a	O
correct	O
translation	O
for	O
the	O
patient	O
.	O

Evaluation	O
.	O

Since	O
the	O
focus	O
of	O
this	O
study	O
is	O
not	O
on	O
speech	O
recognition	O
performance	O
,	O
but	O
on	O
the	O
subsequent	O
processing	O
,	O
we	O
performed	O
our	O
experiments	O
with	O
the	O
transcriptions	O
as	O
input	O
rather	O
than	O
the	O
speech	O
recogniser	O
output	O
,	O
thereby	O
assuming	O
recognition	O
is	O
perfect	O
.	O

The	O
average	O
utterance	O
length	O
was	O
8.96	O
words	O
for	O
the	O
plain	O
utterances	O
and	O
3.14	O
words	O
for	O
the	O
elliptical	O
utterances	O
(	O
Rayner	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

This	O
process	O
finally	O
produced	O
838	O
recorded	O
pairs	O
,	O
with	O
the	O
corresponding	O
core	O
sentences	O
.	O

If	O
the	O
second	O
sentence	O
of	O
the	O
pair	O
was	O
not	O
elliptical	O
because	O
subjects	O
did	O
not	O
follow	O
instructions	O
,	O
they	O
were	O
removed	O
from	O
the	O
test	O
suite	O
.	O

Each	O
utterance	O
was	O
then	O
transcribed	O
and	O
matched	O
to	O
the	O
most	O
plausible	O
core	O
sentence	O
by	O
two	O
judges	O
and	O
when	O
necessary	O
disagreement	O
between	O
judges	O
resolved	O
.	O

This	O
produced	O
a	O
total	O
of	O
1'676	O
recorded	O
utterances	O
.	O

Data	O
were	O
collected	O
using	O
a	O
web	O
tool	O
which	O
prompted	O
the	O
subjects	O
and	O
recorded	O
their	O
responses	O
.	O

Five	O
native	O
francophone	O
subjects	O
were	O
then	O
asked	O
to	O
speak	O
the	O
pairs	O
(	O
context	O
and	O
elliptical	O
utterance	O
)	O
in	O
a	O
natural	O
way	O
,	O
freely	O
varying	O
the	O
wording	O
,	O
but	O
with	O
the	O
instruction	O
to	O
respect	O
the	O
distinction	O
between	O
elliptical	O
and	O
plain	O
utterances	O
.	O

This	O
was	O
created	O
by	O
extracting	O
the	O
list	O
of	O
available	O
core	O
sentences	O
for	O
the	O
abdominal	O
domain	O
and	O
transforming	O
complete	O
sentences	O
into	O
elliptical	O
sentences	O
where	O
possible	O
,	O
for	O
example	O
:	O
avez	O
-	O
vous	O
mal	O
au	O
ventre	O
avez	O
-	O
vous	O
mal	O
dans	O
le	O
bas	O
-	O
ventre	O
--	O
>	O
dans	O
le	O
bas	O
-	O
ventre	O
avez	O
-	O
vous	O
mal	O
dans	O
le	O
haut	O
du	O
ventre	O
--	O
>	O
le	O
haut	O
du	O
ventre	O
Each	O
elliptical	O
utterance	O
was	O
associated	O
with	O
a	O
corresponding	O
complete	O
utterance	O
to	O
serve	O
as	O
context	O
.	O

Consequently	O
,	O
real	O
usage	O
data	O
contains	O
very	O
few	O
elliptical	O
utterances	O
.	O

Test	O
data	O
.	O

3	O
)	O
Does	O
inclusion	O
of	O
ellipsis	O
-	O
specific	O
training	O
data	O
improve	O
performance	O
?	O

2	O
)	O
How	O
does	O
the	O
distribution	O
of	O
class	O
instances	O
affect	O
the	O
performance	O
of	O
the	O
proposed	O
models	O
?	O

The	O
research	O
questions	O
guiding	O
our	O
experiments	O
are	O
listed	O
as	O
follows	O
1	O
)	O
What	O
is	O
the	O
best	O
approach	O
to	O
handle	O
ellipsis	O
in	O
this	O
context	O
?	O

Objective	O
and	O
research	O
question	O
.	O

Methodology	O
.	O

The	O
proposed	O
ellipsis	O
processing	O
workflow	O
is	O
illustrated	O
in	O
Figure	O
1	O
and	O
will	O
be	O
discussed	O
in	O
further	O
detail	O
in	O
Section	O
4	O
.	O

To	O
resolve	O
the	O
ellipsis	O
,	O
we	O
use	O
context	O
information	O
,	O
which	O
in	O
a	O
diagnostic	O
interview	O
is	O
the	O
previous	O
translated	O
utterance	O
.	O

This	O
presents	O
the	O
advantage	O
of	O
removing	O
all	O
ambiguity	O
related	O
to	O
ellipsis	O
and	O
their	O
translation	O
.	O

The	O
latter	O
is	O
one	O
of	O
the	O
approaches	O
evaluated	O
in	O
the	O
present	O
study	O
,	O
where	O
it	O
has	O
been	O
extended	O
to	O
handle	O
elliptical	O
utterances	O
.	O

Results	O
from	O
this	O
recogniser	O
must	O
then	O
be	O
mapped	O
to	O
the	O
closest	O
core	O
sentences	O
,	O
a	O
task	O
to	O
which	O
several	O
approaches	O
have	O
been	O
applied	O
,	O
including	O
tf	O
-	O
idf	O
indexing	O
and	O
dynamic	O
programming	O
(	O
DP	O
,	O
Rayner	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
,	O
more	O
recently	O
,	O
a	O
NMT	O
approach	O
(	O
Mutal	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

This	O
grammar	O
based	O
speech	O
recognition	O
produces	O
high	O
quality	O
results	O
for	O
in	O
coverage	O
items	O
.	O

A	O
Synchronous	O
Context	O
Free	O
Grammar	O
(	O
SCFG	O
,	O
Aho	O
and	O
Ullman	O
,	O
1969	O
)	O
which	O
describes	O
source	O
language	O
variation	O
patterns	O
and	O
their	O
mapping	O
to	O
core	O
sentences	O
is	O
used	O
to	O
compile	O
a	O
language	O
model	O
used	O
by	O
Nuance	O
for	O
speech	O
recognition	O
.	O

The	O
scarcity	O
of	O
training	O
data	O
available	O
for	O
this	O
domain	O
,	O
a	O
consequence	O
of	O
data	O
confidentiality	O
issues	O
and	O
of	O
the	O
minority	O
languages	O
involved	O
(	O
e.g.	O
,	O
Tigrinya	O
,	O
Farsi	O
,	O
Albanian	O
)	O
,	O
has	O
at	O
first	O
led	O
to	O
the	O
development	O
of	O
a	O
grammar	O
-	O
based	O
approach	O
.	O

This	O
ensures	O
the	O
reliability	O
of	O
speech	O
recognition	O
and	O
of	O
translation	O
,	O
essential	O
for	O
safe	O
use	O
in	O
the	O
medical	O
domain	O
.	O

Doctors	O
can	O
freely	O
speak	O
their	O
questions	O
,	O
the	O
system	O
maps	O
the	O
recognised	O
utterance	O
(	O
hereafter	O
:	O
variation	O
)	O
to	O
the	O
closest	O
pre	O
-	O
translated	O
sentence	O
(	O
hereafter	O
:	O
core	O
sentence	O
)	O
,	O
and	O
,	O
after	O
approval	O
by	O
the	O
doctor	O
,	O
the	O
core	O
sentence	O
is	O
translated	O
for	O
the	O
patient	O
.	O

It	O
combines	O
speech	O
recognition	O
with	O
manually	O
pre	O
-	O
translated	O
sentences	O
,	O
grouped	O
by	O
diagnostic	O
domains	O
.	O

Section	O
6	O
presents	O
the	O
results	O
and	O
Section	O
7	O
concludes	O
.	O

Section	O
4	O
presents	O
the	O
approaches	O
and	O
models	O
,	O
followed	O
by	O
Section	O
5	O
which	O
describes	O
the	O
different	O
sets	O
of	O
training	O
data	O
.	O

Section	O
3	O
outlines	O
the	O
methodology	O
,	O
including	O
the	O
objective	O
and	O
research	O
questions	O
,	O
the	O
test	O
data	O
and	O
the	O
evaluation	O
metrics	O
.	O

-	O
>	O
MT	O
:	O
*	O
chuuteido	O
?	O

Source	O
:	O
moderate	O
?	O

Source	O
:	O
is	O
the	O
pain	O
severe	O
-	O
>	O
MT	O
:	O
hageshii	O
itami	O
desu	O
ka	O
?	O

-	O
>	O
MT	O
:	O
*	O
en	O
la	O
cabeza	O
?	O

Source	O
:	O
in	O
your	O
head	O
?	O

-	O
>	O
MT	O
:	O
le	O
duele	O
el	O
estómago	O
?	O

-	O
>	O
MT	O
:	O
*	O
soudain	O
Source	O
:	O
do	O
you	O
have	O
pain	O
in	O
your	O
stomach	O
?	O

->MT	O
:	O
la	O
douleur	O
est	O
-	O
elle	O
intense	O
Source	O
:	O
sudden	O
?	O

Source	O
:	O
is	O
the	O
pain	O
intense	O
?	O

The	O
following	O
examples	O
illustrate	O
elliptical	O
utterances	O
where	O
literal	O
translation	O
is	O
problematic	O
,	O
as	O
it	O
produces	O
agreement	O
errors	O
,	O
wrong	O
prepositions	O
or	O
other	O
syntactical	O
or	O
grammatical	O
issues	O
that	O
can	O
make	O
the	O
elliptical	O
utterance	O
difficult	O
to	O
understand	O
.	O

For	O
example	O
in	O
Japanese	O
,	O
adjectival	O
ellipsis	O
are	O
very	O
informal	O
and	O
should	O
be	O
translated	O
by	O
complete	O
sentences	O
(	O
Bouillon	O
et	O
al	O
.	O
,	O
2007	O
)	O
.	O

Literal	O
translation	O
of	O
these	O
elliptical	O
utterances	O
is	O
rarely	O
possible	O
without	O
affecting	O
communication	O
,	O
in	O
particular	O
with	O
structurally	O
different	O
languages	O
which	O
do	O
not	O
share	O
the	O
same	O
type	O
of	O
ellipsis	O
.	O

Moderate	O
?	O
)	O
(	O
Tanguy	O
et	O
al	O
.	O
,	O
2011	O
)	O
.	O

Is	O
the	O
pain	O
severe	O
?	O

In	O
the	O
back	O
?	O

In	O
the	O
medical	O
dialogues	O
we	O
are	O
interested	O
in	O
,	O
ellipsis	O
allows	O
doctors	O
to	O
question	O
patients	O
in	O
a	O
more	O
efficient	O
way	O
(	O
Where	O
is	O
your	O
pain	O
?	O

Elliptical	O
utterances	O
are	O
very	O
common	O
in	O
dialogues	O
,	O
since	O
they	O
ensure	O
the	O
principle	O
of	O
economy	O
and	O
provide	O
a	O
way	O
to	O
avoid	O
duplication	O
(	O
Hamza	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Very	O
recently	O
,	O
some	O
qualitative	O
studies	O
showed	O
the	O
negative	O
impact	O
of	O
ellipsis	O
on	O
generalist	O
neural	O
systems	O
(	O
DeepL	O
,	O
Google	O
Translate	O
,	O
etc	O
.	O
)	O
from	O
a	O
translation	O
point	O
of	O
view	O
in	O
the	O
English	O
-	O
French	O
pair	O
(	O
for	O
example	O
,	O
Hamza	O
,	O
2019	O
)	O
.	O

More	O
specifically	O
,	O
the	O
implicit	O
semantic	O
context	O
is	O
recovered	O
from	O
elements	O
of	O
linguistic	O
and	O
extralinguistic	O
context	O
"	O
(	O
Ginzburg	O
and	O
Miller	O
,	O
2018	O
)	O
.	O

The	O
syntax	O
thus	O
appears	O
to	O
be	O
incomplete	O
.	O

The	O
characterising	O
feature	O
of	O
ellipsis	O
is	O
that	O
"	O
elements	O
of	O
semantic	O
content	O
are	O
obtained	O
in	O
the	O
absence	O
of	O
any	O
corresponding	O
form	O
.	O

Ellipsis	O
is	O
one	O
of	O
the	O
least	O
studied	O
discursive	O
phenomena	O
in	O
automatic	O
translation	O
.	O

Introduction	O
.	O

Results	O
show	O
that	O
the	O
best	O
model	O
is	O
able	O
to	O
translate	O
88	O
%	O
of	O
elliptical	O
utterances	O
correctly	O
.	O

However	O
,	O
literal	O
translation	O
of	O
such	O
incomplete	O
utterances	O
is	O
rarely	O
possible	O
without	O
affecting	O
communication	O
.	O

