G	O
denotes	O
a	O
feed	O
-	O
forward	O
neural	O
network	O
,	O
and	O
[	O
;	O
]	O
denotes	O
concatenation	O
.	O

As	O
explained	O
in	O
section	O
3	O
,	O
the	O
Predictor	O
uses	O
Decomposable	B-MethodName
Attention	I-MethodName
for	O
prediction	O
.	O

Decomposable	B-MethodName
Attention	I-MethodName
computes	O
a	O
two	O
-	O
dimensional	O
attention	O
matrix	O
,	O
computed	O
by	O
two	O
sets	O
of	O
vectors	O
,	O
and	O
thus	O
,	O
captures	O
detailed	O
information	O
useful	O
for	O
prediction	O
.	O

We	O
verified	O
these	O
hypotheses	O
with	O
the	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
,	O
but	O
not	O
with	O
the	O
email	O
datasets	O
,	O
because	O
few	O
emails	O
included	O
annotated	O
summaries	O
,	O
and	O
those	O
emails	O
did	O
not	O
have	O
replies	O
with	O
quotes	O
.	O

We	O
evaluated	O
our	O
model	O
with	O
two	O
mail	O
datasets	O
,	O
ECS	B-DatasetName
and	O
EPS	B-DatasetName
,	O
and	O
one	O
social	O
media	O
dataset	O
TIFU	B-DatasetName
,	O
using	O
ROUGE	B-MetricName
as	O
an	O
evaluation	O
metric	O
,	O
and	O
validated	O
that	O
our	O
model	O
is	O
useful	O
for	O
summarization	B-TaskName
.	O

This	O
paper	O
proposes	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
,	O
a	O
model	O
that	O
extracts	B-TaskName
implicit	I-TaskName
quotes	I-TaskName
as	O
summaries	O
.	O

appear	O
only	O
once	O
in	O
the	O
source	O
text	O
;	O
thus	O
TextRank	B-MethodName
fails	O
to	O
capture	O
the	O
salient	O
sentences	O
.	O

The	O
sample	O
is	O
from	O
the	O
EPS	B-DatasetName
dataset	O
.	O

Figure	O
3	O
shows	O
the	O
correlation	O
between	O
the	O
maximum	O
PageRank	O
in	O
each	O
post	O
of	O
ECS	B-DatasetName
/	O
EPS	B-DatasetName
and	O
ROUGE-1	B-MetricName
-	I-MetricName
F	I-MetricName
scores	O
Table	O
8	O
shows	O
a	O
demonstrative	O
example	O
of	O
extracted	O
summaries	O
of	O
IQE	B-MethodName
and	O
TextRank	B-MethodName
.	O

Comparing	O
with	O
TextRank	B-MethodName
,	O
we	O
verify	O
that	O
our	O
method	O
can	O
capture	O
salient	O
sentences	O
that	O
the	O
centrality	O
-	O
based	O
method	O
fails	O
to	O
.	O

TextRank	B-MethodName
is	O
a	O
typical	O
example	O
;	O
TextRank	B-MethodName
is	O
a	O
centrality	O
-	O
based	O
method	O
that	O
extracts	O
sentences	O
with	O
high	O
PageRank	O
as	O
the	O
summary	O
.	O

As	O
explained	O
in	O
the	O
Introduction	O
,	O
most	O
conventional	O
unsupervised	B-TaskName
summarization	I-TaskName
methods	O
are	O
based	O
on	O
the	O
assumption	O
that	O
important	O
topics	O
appear	O
frequently	O
in	O
a	O
document	O
.	O

Without	O
pretraining	O
,	O
the	O
accuracy	B-MetricName
decreased	O
.	O

However	O
,	O
on	O
the	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
,	O
NER	B-TaskName
did	O
not	O
affect	O
the	O
accuracy	O
.	O

To	O
validate	O
the	O
effect	O
of	O
NER	B-TaskName
,	O
we	O
experiment	O
without	O
replacing	O
named	O
entities	O
.	O

Effect	O
of	O
replacing	O
named	O
entities	O
As	O
explained	O
in	O
the	O
section	O
4.3	O
,	O
our	O
models	O
shown	O
in	O
Tables	O
2	O
,	O
3	O
and	O
4	O
all	O
use	O
the	O
Stanford	B-MethodName
NER	I-MethodName
.	O

The	O
results	O
of	O
the	O
two	O
analyses	O
support	O
the	O
claim	O
that	O
our	O
model	O
is	O
more	O
likely	O
to	O
extract	O
quotes	O
and	O
that	O
the	O
ability	O
of	O
extracting	B-TaskName
quotes	I-TaskName
leads	O
to	O
better	O
summarization	B-TaskName
.	O

The	O
result	O
in	O
the	O
Table	O
6	O
shows	O
ROUGE	B-MetricName
scores	O
are	O
higher	O
when	O
the	O
extracted	O
sentence	O
coincides	O
with	O
a	O
quote	O
.	O

IQEquote	B-MethodName
indicates	O
the	O
data	O
where	O
the	O
extracted	O
sentence	O
coincides	O
with	O
a	O
quote	O
,	O
and	O
IQEnonquote	B-MethodName
vice	O
versa	O
.	O

We	O
compute	O
ROUGE	B-MetricName
scores	O
when	O
our	O
model	O
succeeds	O
or	O
fails	O
in	O
quote	B-TaskName
extraction	I-TaskName
(	O
which	O
means	O
when	O
MRR	B-MetricName
equals	O
1	B-MetricValue
or	O
otherwise	O
)	O
.	O

Next	O
,	O
we	O
validate	O
whether	O
the	O
ROUGE	B-MetricName
scores	O
become	O
better	O
when	O
our	O
model	O
succeeded	O
in	O
extracting	O
quotes	O
.	O

IQE	B-MethodName
is	O
more	O
likely	O
to	O
extract	O
quotes	O
than	O
TextRank	B-MethodName
,	O
LexRank	B-MethodName
and	O
Random	B-MethodName
.	O

For	O
each	O
data	O
,	O
we	O
compute	O
MRR	B-MetricName
and	O
use	O
the	O
mean	B-MetricName
value	I-MetricName
as	O
a	O
result	O
.	O

Thus	O
we	O
set	O
the	O
threshold	B-HyperparameterName
at	O
four	B-HyperparameterValue
;	O
if	O
R(q	O
)	O
is	O
larger	O
than	O
4	O
we	O
set	O
MRR	B-MethodName
0	O
.	O

Therefore	O
,	O
the	O
MRR	B-MethodName
in	O
our	O
study	O
indicates	O
the	O
capability	O
of	O
a	O
model	O
to	O
extract	O
quotes	O
.	O

MRR	B-MethodName
=	O
1	O
R(q	O
)	O
(	O
R(q	O
)	O
≤	O
4	O
)	O
0	O
(	O
R(q	O
)	O
>	O
4)(12	O
)	O
The	O
function	O
R	O
denotes	O
the	O
rank	O
of	O
the	O
saliency	O
scores	O
a	O
model	O
computes	O
;	O
our	O
model	O
does	O
not	O
compute	O
the	O
scores	O
but	O
sequentially	O
extracts	O
sentences	O
,	O
and	O
the	O
order	O
is	O
regarded	O
as	O
the	O
rank	O
here	O
.	O

We	O
compute	O
MRR	B-MethodName
as	O
follows	O
.	O

To	O
assess	O
the	O
ability	O
of	O
quote	O
extraction	O
,	O
we	O
regard	O
the	O
extraction	O
of	O
quotes	O
as	O
an	O
information	B-TaskName
retrieval	I-TaskName
task	O
and	O
evaluate	O
with	B-MetricName
Mean	I-MetricName
Reciprocal	I-MetricName
Rank	I-MetricName
(	O
MRR	B-MetricName
)	O
.	O

297	O
Model	O
ROUGE-1	B-MetricName
-	I-MetricName
F	I-MetricName
ROUGE-2	B-MetricName
-	I-MetricName
F	I-MetricName
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
-	I-MetricName
F	I-MetricName
#	O
of	O
For	O
the	O
experiments	O
,	O
we	O
use	O
the	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
and	O
replies	O
extracted	O
via	O
praw	O
as	O
described	O
in	O
4.2	O
.	O
From	O
the	O
dataset	O
,	O
we	O
extract	O
replies	O
that	O
contain	O
quotes	O
,	O
which	O
start	O
with	O
the	O
symbol	O
"	O
>	O
"	O
.	O

Second	O
,	O
following	O
Carenini	O
's	O
work	O
(	O
Carenini	O
et	O
al	O
.	O
,	O
2007;Oya	O
and	O
Carenini	O
,	O
2014	O
)	O
,	O
we	O
assumed	O
quotes	O
were	O
useful	O
for	O
summarization	O
but	O
it	O
is	O
not	O
clear	O
whether	O
the	O
quote	B-TaskName
extraction	I-TaskName
leads	O
to	O
better	O
results	O
of	O
summarization	B-TaskName
.	O

The	O
Performance	O
of	O
Summarization	B-TaskName
and	O
Quote	B-TaskName
Extraction	I-TaskName
.	O

IQE	B-MethodName
did	O
not	O
outperform	O
TextRank	B-MethodName
on	O
TIFU	B-DatasetName
dataset	O
.	O

Baseline	O
models	O
such	O
as	O
LexRank	B-DatasetName
and	O
TextRank	B-DatasetName
compute	O
similarity	O
of	O
sentences	O
using	O
the	O
co	O
-	O
occurrence	O
of	O
words	O
.	O

The	O
average	O
number	O
of	O
words	O
each	O
sentence	O
has	O
is	O
smaller	O
in	O
EPS	B-DatasetName
.	O

Our	O
model	O
outperforms	O
the	O
baseline	O
models	O
more	O
with	O
the	O
EPS	B-DatasetName
dataset	O
than	O
the	O
ECS	B-DatasetName
dataset	O
.	O

IQE	B-MethodName
-	I-MethodName
TextRank	I-MethodName
performed	O
worse	O
than	O
IQE	B-MethodName
with	O
the	O
mail	O
datasets	O
.	O

PacSum	B-MethodName
significantly	O
outperformed	O
TextRank	B-MethodName
on	O
the	O
news	O
article	O
dataset	O
(	O
Zheng	O
and	O
Lapata	O
,	O
2019	O
)	O
but	O
does	O
not	O
work	O
well	O
on	O
our	O
datasets	O
where	O
the	O
sentence	O
position	O
is	O
not	O
an	O
important	O
factor	O
.	O

Reranking	O
improves	O
the	O
accuracy	O
on	O
ECS	B-DatasetName
and	O
TIFU	B-DatasetName
but	O
not	O
on	O
EPS	B-DatasetName
.	O

On	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
,	O
IQE	B-MethodName
with	O
reranking	O
outperforms	O
most	O
baseline	O
models	O
except	O
TextRank	B-MethodName
.	O

Our	O
model	O
outperforms	O
baseline	O
models	O
on	O
the	O
mail	O
datasets	O
(	O
ECS	B-DatasetName
and	O
EPS	B-DatasetName
)	O
in	O
most	O
metrics	O
.	O

As	O
another	O
baseline	O
,	O
we	O
employ	O
IQETextRank	B-MethodName
;	O
the	O
TextRank	B-MethodName
model	O
that	O
leverages	O
cosine	O
similarities	O
of	O
sentence	O
vectors	O
of	O
IQE	B-MethodName
's	O
Encoder	O
as	O
similarities	O
between	O
sentences	O
.	O

PacSum	B-MethodName
and	O
LexRank	B-MethodName
leverage	O
idf	O
.	O

KLSum	B-MethodName
employs	O
the	O
Kullbuck	O
-	O
Leibler	O
divergence	O
to	O
constrain	O
extracted	O
sentences	O
and	O
the	O
source	O
text	O
to	O
have	O
the	O
similar	O
word	O
distribution	O
.	O

296	O
Model	O
ROUGE-1	B-MetricName
-	I-MetricName
F	I-MetricName
ROUGE-2	B-MetricName
-	I-MetricName
F	I-MetricName
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
-	I-MetricName
F	I-MetricName
#	O
of	O
PacSum	B-MethodName
is	O
an	O
improved	O
model	O
of	O
TextRank	B-MethodName
,	O
which	O
harnesses	O
the	O
position	O
of	O
sentences	O
as	O
a	O
feature	O
.	O

TextRank	B-MethodName
and	O
LexRank	B-MethodName
are	O
graph	O
-	O
centrality	O
based	O
methods	O
that	O
have	O
long	O
been	O
considered	O
as	O
strong	O
methods	O
for	O
unsupervised	B-TaskName
summarization	I-TaskName
.	O

As	O
baseline	O
models	O
,	O
we	O
employ	O
TextRank	B-MethodName
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
,	O
LexRank	B-MethodName
(	O
Erkan	O
and	O
Radev	O
,	O
2004	O
)	O
,	O
KLSum	B-MethodName
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
,	O
PacSum	B-MethodName
(	O
Zheng	O
and	O
Lapata	O
,	O
2019	O
)	O
,	O
Lead	B-MethodName
,	O
and	O
Random	B-MethodName
.	O

As	O
a	O
validation	O
metric	O
,	O
we	O
use	O
an	O
average	O
of	O
ROUGE-1	B-MetricName
-	I-MetricName
F	I-MetricName
,	O
ROUGE-2	B-MetricName
-	I-MetricName
F	I-MetricName
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
-	I-MetricName
F.	I-MetricName

For	O
ROUGE	B-MetricName
computation	O
,	O
we	O
use	O
ROUGE	B-MetricName
2.0	I-MetricName
(	O
Ganesan	O
,	O
2015	O
)	O
.	O

We	O
use	O
the	O
first	O
20	B-HyperparameterValue
,	O
40	B-HyperparameterValue
,	O
and	O
60	B-HyperparameterValue
words	B-HyperparameterName
of	O
the	O
extracted	O
sentences	O
.	O

Following	O
previous	O
work	O
,	O
we	O
report	O
the	O
average	B-MetricName
F1	I-MetricName
of	O
ROUGE-1	B-MetricName
,	O
ROUGE-2	B-MetricName
,	O
and	O
ROUGE	B-MetricName
-	I-MetricName
L	I-MetricName
for	O
the	O
evaluation	O
(	O
Lin	O
,	O
2004	O
)	O
.	O

We	O
pretrain	O
word	O
embeddings	O
of	O
the	O
model	O
with	O
Skipgram	B-MethodName
,	O
using	O
the	O
same	O
data	O
as	O
the	O
training	O
.	O

We	O
replace	O
the	O
named	O
entities	O
on	O
the	O
text	O
data	O
with	O
tags	O
(	O
person	O
,	O
location	O
,	O
and	O
organization	O
)	O
using	O
the	O
Stanford	B-MethodName
Named	I-MethodName
Entity	I-MethodName
Recognizer	I-MethodName
(	O
NER	B-TaskName
)	O
4	O
,	O
to	O
prevent	O
the	O
model	O
from	O
simply	O
using	O
named	O
entities	O
as	O
a	O
hint	O
for	O
the	O
prediction	O
.	O

During	O
training	O
,	O
L	B-HyperparameterName
,	O
the	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
sentences	I-HyperparameterName
the	I-HyperparameterName
Extractor	I-HyperparameterName
extracts	I-HyperparameterName
is	O
randomly	O
set	O
from	O
1	B-HyperparameterValue
to	I-HyperparameterValue
4	I-HyperparameterValue
,	O
so	O
that	O
the	O
model	O
can	O
extract	O
an	O
arbitrary	O
number	O
of	O
sentences	O
.	O

We	O
set	O
this	O
threshold	B-HyperparameterName
as	O
4	B-HyperparameterValue
.	O

The	O
epoch	B-HyperparameterName
size	O
is	O
10	B-HyperparameterValue
,	O
and	O
we	O
use	O
Adam	B-HyperparameterValue
(	O
Kingma	O
and	O
Ba	O
,	O
2015	O
)	O
as	O
an	O
optimizer	B-HyperparameterName
.	O

The	O
upper	B-HyperparameterName
limit	I-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
sentences	I-HyperparameterName
is	O
set	O
to	O
30	B-HyperparameterValue
,	O
and	O
that	O
of	O
words	B-HyperparameterName
in	I-HyperparameterName
each	I-HyperparameterName
sentence	I-HyperparameterName
is	O
set	O
to	O
200	B-HyperparameterValue
.	O

We	O
tokenize	O
each	O
email	O
or	O
post	O
into	O
sentences	O
and	O
each	O
sentence	O
into	O
words	O
using	O
the	O
nltk	B-MethodName
tokenizer	I-MethodName
3	O
.	O

The	O
size	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
vocabulary	I-HyperparameterName
is	O
set	O
to	O
30,000	B-HyperparameterValue
.	O

The	O
dimensions	B-HyperparameterName
of	I-HyperparameterName
the	I-HyperparameterName
embedding	I-HyperparameterName
layers	I-HyperparameterName
and	O
hidden	B-HyperparameterName
layers	I-HyperparameterName
of	O
the	O
LSTM	B-MethodName
are	O
100	B-HyperparameterValue
.	O

An	O
overview	O
of	O
the	O
TIFU	B-DatasetName
evaluation	O
dataset	O
is	O
also	O
summarized	O
in	O
Table	O
1	O
.	O

Because	O
the	O
TIFU	B-DatasetName
dataset	O
does	O
not	O
include	O
replies	O
,	O
we	O
collected	O
replies	O
of	O
the	O
posts	O
included	O
in	O
the	O
TIFU	B-DatasetName
dataset	O
using	O
praw	O
2	O
.	O

We	O
preprocess	O
the	O
TIFU	B-DatasetName
dataset	O
similarly	O
as	O
the	O
mail	O
datasets	O
.	O

On	O
the	O
discussion	O
forum	O
Reddit	B-DatasetName
TIFU	I-DatasetName
,	O
users	O
post	O
a	O
tldr	O
along	O
with	O
the	O
post	O
.	O

The	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
(	O
Kim	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
a	O
dataset	O
that	O
leverages	O
tldr	O
tags	O
for	O
the	O
summarization	O
task	O
,	O
which	O
is	O
the	O
abbreviation	O
of	O
"	O
too	O
long	O
did	O
n't	O
read	O
"	O
.	O

Reddit	B-DatasetName
TIFU	I-DatasetName
Dataset	O
.	O

For	O
evaluation	O
,	O
we	O
employ	O
the	O
Enron	B-DatasetName
Summarization	I-DatasetName
dataset	O
(	O
Loza	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

The	O
Avocado	B-DatasetName
collection	I-DatasetName
is	O
a	O
public	O
dataset	O
that	O
comprises	O
emails	O
obtained	O
from	O
279	O
custodians	O
of	O
a	O
defunct	O
information	O
technology	O
company	O
.	O

We	O
use	O
Avocado	B-DatasetName
collection	I-DatasetName
1	O
for	O
the	O
training	O
.	O

One	O
is	O
a	O
mail	O
dataset	O
,	O
and	O
the	O
other	O
is	O
a	O
dataset	O
from	O
the	O
social	O
media	O
platform	O
,	O
Reddit	B-DatasetName
.	O

To	O
compute	O
the	O
relation	O
between	O
the	O
post	O
and	O
the	O
reply	O
candidate	O
,	O
we	O
employ	O
Decomposable	B-MethodName
Attention	I-MethodName
(	O
Parikh	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

The	O
discretized	O
attention	O
weights	O
α	O
are	O
computed	O
as	O
follows	O
:	O
u	O
i	O
∼	O
Uniform(0	O
,	O
1	O
)	O
(	O
3	O
)	O
g	O
i	O
=	O
−	O
log	O
(	O
−	O
log	O
u	O
i	O
)	O
(	O
4	O
)	O
a	O
ti	O
=	O
c	O
T	O
tanh(h	O
ext	O
t	O
+	O
h	O
p	O
i	O
)	O
(	O
5	O
)	O
π	O
ti	O
=	O
exp	O
a	O
ti	O
N	O
k=1	O
exp	O
a	O
tk	O
(	O
6	O
)	O
α	O
ti	O
=	O
exp	O
(	O
log	O
π	O
ti	O
+	O
g	O
i	O
)	O
/τ	O
N	O
k=1	O
exp	O
(	O
log	O
π	O
tk	O
+	O
g	O
k	O
)	O
/τ	O
(	O
7	O
)	O
c	O
is	O
a	O
parameter	O
vector	O
,	O
and	O
the	O
temperature	O
τ	O
is	O
set	O
to	O
0.1	O
.	O
We	O
input	O
the	O
linear	O
sum	O
of	O
the	O
attention	O
weights	O
α	O
and	O
the	O
sentence	O
vectors	O
h	O
p	O
i	O
to	O
LSTM	B-MethodName
and	O
update	O
the	O
hidden	O
state	O
of	O
the	O
Extractor	O
.	O

We	O
employ	O
LSTM	B-MethodName
to	O
sequentially	O
compute	O
features	O
on	O
the	O
Extractor	O
.	O

IQE	B-MethodName
requires	O
replies	O
only	O
during	O
the	O
training	O
and	O
can	O
induce	O
summaries	O
without	O
replies	O
during	O
the	O
evaluation	O
.	O

We	O
compute	O
the	O
features	O
of	O
each	O
sentence	O
h	O
p	O
i	O
by	O
inputting	O
embedded	O
vectors	O
to	O
Bidirectional	B-MethodName
Long	I-MethodName
Short	I-MethodName
-	I-MethodName
Term	I-MethodName
Memory	I-MethodName
(	O
BiLSTM	B-MethodName
)	O
and	O
concatenating	O
the	O
last	O
two	O
hidden	O
layers	O
:	O
h	O
p	O
i	O
=	O
BiLSTM(X	O
p	O
i	O
)	O
(	O
1	O
)	O
Extractor	O
The	O
Extractor	O
extracts	O
a	O
few	O
sentences	O
of	O
a	O
post	O
for	O
prediction	O
.	O

We	O
propose	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
(	O
IQE	B-MethodName
)	O
,	O
an	O
unsupervised	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
model	O
.	O

A	O
few	O
studies	O
used	O
these	O
quotes	O
as	O
features	O
for	O
summarization	B-TaskName
.	O

Quotes	O
are	O
also	O
important	O
factors	O
of	O
summarization	B-TaskName
.	O

Despite	O
the	O
rise	O
of	O
neural	O
summarization	O
models	O
,	O
most	O
research	O
on	O
conversation	B-TaskName
summarization	I-TaskName
is	O
based	O
on	O
non	O
-	O
neural	O
models	O
.	O

However	O
,	O
these	O
methods	O
use	O
pretrained	O
neural	O
network	O
models	O
as	O
a	O
feature	O
extractor	O
,	O
whereas	O
we	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
model	O
.	O

A	O
few	O
neural	O
-	O
network	O
-	O
based	O
unsupervised	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
methods	O
were	O
proposed	O
(	O
Kågebäck	O
et	O
al	O
.	O
,	O
2014;Yin	O
and	O
Pei	O
,	O
2015;Ma	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Despite	O
the	O
rise	O
of	O
neural	O
networks	O
,	O
conventional	O
non	O
-	O
neural	O
methods	O
are	O
still	O
powerful	O
in	O
the	O
field	O
of	O
unsupervised	B-TaskName
extractive	I-TaskName
summarization	I-TaskName
.	O

Most	O
unsupervised	B-TaskName
summarization	I-TaskName
methods	O
proposed	O
are	O
extractive	O
methods	O
.	O

Summarization	B-TaskName
methods	O
can	O
be	O
roughly	O
grouped	O
into	O
two	O
methods	O
:	O
extractive	B-TaskName
summarization	I-TaskName
and	O
abstractive	O
summarization	O
.	O

•	O
Using	O
the	O
Reddit	B-DatasetName
dataset	O
,	O
we	O
verified	O
that	O
quote	B-TaskName
extraction	I-TaskName
leads	O
to	O
a	O
high	O
performance	O
of	O
summarization	B-TaskName
.	O

•	O
We	O
proposed	O
an	O
unsupervised	B-DatasetName
extractive	I-DatasetName
neural	I-DatasetName
summarization	I-DatasetName
model	O
,	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
(	O
IQE	B-MethodName
)	O
,	O
and	O
demonstrated	O
that	O
the	O
model	O
outperformed	O
or	O
achieved	O
results	O
competitive	O
to	O
baseline	O
models	O
on	O
two	O
mail	O
datasets	O
and	O
a	O
Reddit	B-DatasetName
dataset	O
.	O

Using	O
the	O
Reddit	B-DatasetName
dataset	O
where	O
quotes	O
are	O
abundant	O
,	O
we	O
obtain	O
results	O
that	O
supports	O
the	O
hypothesis	O
.	O

We	O
also	O
evaluated	O
our	O
model	O
with	O
Reddit	B-DatasetName
TIFU	I-DatasetName
dataset	O
(	O
Kim	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
achieved	O
results	O
competitive	O
with	O
those	O
of	O
the	O
baseline	O
models	O
.	O

We	O
evaluate	O
our	O
model	O
with	O
two	O
datasets	O
of	O
Enron	B-DatasetName
mail	I-DatasetName
(	O
Loza	O
et	O
al	O
.	O
,	O
2014	O
)	O
,	O
corporate	O
and	O
private	O
mails	O
,	O
and	O
verify	O
that	O
our	O
model	O
outperforms	O
baseline	O
models	O
.	O

Summaries	O
should	O
not	O
depend	O
on	O
replies	O
,	O
so	O
IQE	B-MethodName
does	O
not	O
use	O
reply	O
features	O
to	O
extract	O
sentences	O
.	O

To	O
predict	O
accurately	O
,	O
IQE	B-MethodName
has	O
to	O
extract	O
sentences	O
that	O
replies	O
frequently	O
refer	O
to	O
.	O

IQE	B-MethodName
extracts	O
a	O
few	O
sentences	O
of	O
the	O
post	O
as	O
a	O
feature	O
for	O
prediction	O
.	O

The	O
aim	O
of	O
our	O
model	O
is	O
to	O
extract	O
these	O
implicit	O
quotes	O
for	O
extractive	B-TaskName
summarization	I-TaskName
.	O

The	O
model	O
is	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
(	O
IQE	B-MethodName
)	O
.	O

Graph	O
-	O
centrality	O
based	O
on	O
the	O
similarity	O
of	O
sentences	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004;Erkan	O
and	O
Radev	O
,	O
2004;Zheng	O
and	O
Lapata	O
,	O
2019	O
)	O
has	O
long	O
been	O
a	O
strong	O
feature	O
for	O
unsupervised	B-TaskName
summarization	I-TaskName
,	O
and	O
is	O
also	O
used	O
to	O
summarize	O
conversations	O
(	O
Mehdad	O
et	O
al	O
.	O
,	O
2014;Shang	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Previous	O
research	O
proposed	O
diverse	O
methods	O
of	O
unsupervised	B-TaskName
summarization	I-TaskName
.	O

Neuralnetwork	O
-	O
based	O
models	O
have	O
achieved	O
great	O
performance	O
on	O
supervised	O
summarization	O
,	O
but	O
its	O
application	O
to	O
unsupervised	B-TaskName
summarization	I-TaskName
is	O
not	O
sufficiently	O
explored	O
.	O

As	O
the	O
amount	O
of	O
information	O
exchanged	O
via	O
online	O
conversations	O
is	O
growing	O
rapidly	O
,	O
automated	B-TaskName
summarization	I-TaskName
of	I-TaskName
conversations	I-TaskName
is	O
in	O
demand	O
.	O

We	O
evaluate	O
our	O
model	O
on	O
two	O
email	O
datasets	O
and	O
one	O
social	O
media	O
dataset	O
,	O
and	O
confirm	O
that	O
our	O
model	O
is	O
useful	O
for	O
extractive	B-TaskName
summarization	I-TaskName
.	O

Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
aims	O
to	O
extract	O
implicit	O
quotes	O
as	O
summaries	O
.	O

We	O
propose	O
Implicit	B-MethodName
Quote	I-MethodName
Extractor	I-MethodName
,	O
an	O
endto	O
-	O
end	O
unsupervised	B-TaskName
extractive	I-TaskName
neural	I-TaskName
summarization	I-TaskName
model	O
for	O
conversational	O
texts	O
.	O

Identifying	O
Implicit	O
Quotes	O
for	O
Unsupervised	B-TaskName
Extractive	I-TaskName
Summarization	I-TaskName
of	O
Conversations	O
.	O

Finally	O
,	O
we	O
concatenate	O
v	O
1	O
and	O
v	O
2	O
and	O
obtain	O
binary	O
-	O
classification	O
result	O
y	O
through	O
a	O
linear	O
layer	O
H	O
and	O
the	O
sigmoid	O
function	O
.	O

Next	O
,	O
we	O
separately	O
compare	O
the	O
aligned	O
phrases	O
β	O
t	O
and	O
x	O
ext	O
t	O
,	O
α	O
j	O
and	O
h	O
r	O
j	O
,	O
using	O
a	O
function	O
G.	O

β	O
i	O
is	O
a	O
linear	O
sum	O
of	O
reply	O
features	O
h	O
r	O
j	O
that	O
is	O
aligned	O
to	O
x	O
ext	O
t	O
and	O
vice	O
versa	O
for	O
α	O
j	O
.	O

The	O
weights	O
of	O
the	O
co	O
-	O
attention	O
matrix	O
are	O
normalized	O
row	O
-	O
wise	O
and	O
column	O
-	O
wise	O
in	O
the	O
equations	O
(	O
14	O
)	O
and	O
(	O
15	O
)	O
.	O

First	O
,	O
we	O
compute	O
a	O
co	O
-	O
attention	O
matrix	O
E	O
as	O
in	O
(	O
13	O
)	O
.	O

The	O
computation	O
uses	O
the	O
following	O
equations	O
:	O
The	O
computation	O
of	O
x	O
ext	O
t	O
and	O
h	O
r	O
j	O
are	O
explained	O
in	O
section	O
3	O
.	O

A.1	O
Decomposable	O
Attention	O
.	O

A	O
Appendices	O
.	O

For	O
future	O
work	O
,	O
we	O
will	O
examine	O
whether	O
our	O
hypotheses	O
are	O
valid	O
for	O
emails	O
and	O
other	O
datasets	O
.	O

We	O
hypothesized	O
that	O
our	O
model	O
is	O
more	O
likely	O
to	O
extract	O
quotes	O
and	O
that	O
ability	O
improved	O
the	O
performance	O
of	O
our	O
model	O
.	O

Conclusion	O
.	O

Our	O
model	O
,	O
by	O
contrast	O
,	O
can	O
capture	O
them	O
because	O
they	O
are	O
topics	O
that	O
replies	O
often	O
refer	O
to	O
.	O

She	O
run	O
into	O
heather	O
evans	O
which	O
she	O
had	O
n't	O
talked	O
in	O
10	O
years	O
.	O

Rachel	O
is	O
coming	O
to	O
visit	O
her	O
in	O
couple	O
of	O
weeks	O
and	O
she	O
is	O
asking	O
if	O
he	O
/	O
she	O
will	O
join	O
for	O
any	O
of	O
the	O
rodeo	O
stuff	O
.	O

She	O
is	O
scared	O
of	O
being	O
a	O
mother	O
but	O
also	O
pretty	O
exited	O
about	O
it	O
.	O

She	O
is	O
having	O
a	O
baby	O
due	O
in	O
June	O
.	O

The	O
sender	O
just	O
move	O
out	O
to	O
Katy	O
few	O
months	O
ago	O
.	O

The	O
sender	O
wants	O
to	O
congratulate	O
the	O
recipient	O
for	O
his	O
/	O
her	O
new	O
promotion	O
,	O
as	O
well	O
as	O
,	O
updating	O
him	O
/	O
her	O
about	O
her	O
life	O
.	O

Summary	O
(	O
Gold	O
)	O
.	O

Looking	O
forward	O
to	O
hearing	O
back	O
from	O
ya	O
.	O

Got	O
ta	O
get	O
back	O
to	O
work	O
.	O

Anyway	O
,	O
I	O
'll	O
let	O
you	O
go	O
.	O

Seems	O
like	O
she	O
's	O
doing	O
well	O
but	O
I	O
can	O
never	O
really	O
tell	O
with	O
her	O
.	O

I	O
had	O
n't	O
talked	O
to	O
her	O
in	O
about	O
10	O
years	O
.	O

It	O
was	O
the	O
weirdest	O
thing	O
-heather	O
evans	O
.	O

You	O
'll	O
never	O
guess	O
who	O
I	O
got	O
in	O
touch	O
with	O
about	O
a	O
month	O
ago	O
.	O

You	O
planning	O
on	O
coming	O
in	O
for	O
any	O
of	O
the	O
rodeo	O
stuff	O
?	O

Rachel	O
is	O
coming	O
to	O
visit	O
me	O
in	O
a	O
couple	O
of	O
weeks	O
.	O

I	O
'm	O
really	O
excited	O
though	O
.	O

The	O
thought	O
of	O
me	O
being	O
a	O
mother	O
is	O
downright	O
scary	O
but	O
I	O
figure	O
since	O
I	O
'm	O
almost	O
30	O
,	O
I	O
probably	O
need	O
to	O
start	O
growing	O
up	O
.	O

I	O
ca	O
n't	O
even	O
believe	O
it	O
myself	O
.	O

New	O
news	O
from	O
me	O
-I'm	O
having	O
a	O
baby	O
-due	O
in	O
June	O
.	O

I	O
love	O
it	O
there	O
-my	O
parents	O
live	O
about	O
10	O
minutes	O
away	O
.	O

My	O
hubby	O
and	O
'	O
I	O
moved	O
out	O
to	O
Katy	O
a	O
few	O
months	O
ago	O
.	O

I	O
'm	O
sure	O
it	O
's	O
going	O
to	O
be	O
alot	O
different	O
for	O
you	O
but	O
it	O
sounds	O
like	O
a	O
great	O
deal	O
.	O

Congrats	O
on	O
your	O
promotion	O
.	O

However	O
,	O
those	O
words	O
Source	O
Text	O
Just	O
got	O
your	O
email	O
address	O
from	O
Rachel	O
.	O

The	O
summary	O
includes	O
descriptions	O
regarding	O
a	O
promotion	O
and	O
that	O
the	O
sender	O
is	O
having	O
a	O
baby	O
.	O

We	O
suspected	O
that	O
important	O
topics	O
are	O
not	O
always	O
referred	O
to	O
frequently	O
,	O
and	O
suggested	O
another	O
criterion	O
:	O
the	O
frequency	O
of	O
being	O
referred	O
to	O
in	O
replies	O
.	O

A	O
sentence	O
having	O
high	O
PageRank	O
indicates	O
that	O
the	O
sentence	O
has	O
high	O
similarity	O
with	O
many	O
other	O
sentences	O
,	O
meaning	O
that	O
many	O
sentences	O
refer	O
to	O
the	O
same	O
topic	O
.	O

Difference	O
from	O
Conventional	O
Methods	O
.	O

This	O
shows	O
the	O
importance	O
of	O
the	O
separate	O
training	O
of	O
each	O
component	O
.	O

Table	O
7	O
shows	O
the	O
effect	O
of	O
pretraining	O
.	O

As	O
explained	O
in	O
the	O
section	O
4.3	O
,	O
we	O
pretrained	O
the	O
Predictor	O
in	O
the	O
first	O
few	O
epochs	O
so	O
that	O
the	O
model	O
can	O
learn	O
the	O
extraction	O
and	O
the	O
prediction	O
separately	O
.	O

Effect	O
of	O
pretraining	O
Predictor	O
.	O

Thus	O
,	O
named	O
entities	O
will	O
not	O
be	O
hints	O
to	O
predict	O
reply	O
-	O
relation	O
.	O

Reddit	O
is	O
an	O
anonymized	O
social	O
media	O
platform	O
,	O
and	O
the	O
posts	O
are	O
less	O
likely	O
to	O
refer	O
to	O
people	O
's	O
names	O
.	O

Ablation	O
Tests	O
.	O

Does	O
extracting	O
quotes	O
lead	O
to	O
good	O
summarization	O
?	O

Table	O
5	O
shows	O
the	O
results	O
.	O

As	O
explained	O
in	O
the	O
section	O
4.3	O
,	O
we	O
trained	O
our	O
model	O
to	O
extract	O
up	O
to	O
four	O
sentences	O
.	O

If	O
a	O
model	O
extracts	O
quotes	O
as	O
salient	O
sentences	O
,	O
the	O
rank	O
becomes	O
higher	O
.	O

How	O
well	O
our	O
model	O
extracts	O
quotes	O
?	O
.	O

We	O
label	O
sentences	O
of	O
the	O
posts	O
that	O
are	O
quoted	O
by	O
the	O
replies	O
and	O
verify	O
how	O
accurately	O
our	O
model	O
can	O
extract	O
the	O
quoted	O
sentences	O
.	O

In	O
total	O
,	O
1,969	O
posts	O
have	O
replies	O
that	O
include	O
quotes	O
.	O

To	O
answer	O
these	O
questions	O
,	O
we	O
conduct	O
two	O
experiments	O
.	O

First	O
,	O
because	O
we	O
did	O
not	O
use	O
quotes	O
as	O
supervision	O
,	O
it	O
is	O
not	O
clear	O
how	O
well	O
our	O
model	O
extracts	O
quotes	O
.	O

Our	O
model	O
performed	O
well	O
on	O
the	O
Mail	O
datasets	O
but	O
two	O
questions	O
remain	O
unclear	O
.	O

It	O
is	O
conceivable	O
that	O
Reddit	O
users	O
are	O
less	O
likely	O
to	O
refer	O
to	O
important	O
topics	O
on	O
the	O
post	O
,	O
given	O
that	O
anyone	O
can	O
reply	O
.	O

Thus	O
,	O
if	O
the	O
lengths	O
of	O
sentences	O
are	O
short	O
,	O
it	O
fails	O
to	O
build	O
decent	O
co	O
-	O
occurrence	O
networks	O
and	O
to	O
capture	O
the	O
saliency	O
of	O
the	O
sentences	O
.	O

The	O
overview	O
of	O
the	O
datasets	O
in	O
Table	O
1	O
explains	O
the	O
reason	O
.	O

This	O
indicates	O
that	O
the	O
performance	O
of	O
our	O
model	O
does	O
not	O
result	O
from	O
the	O
use	O
of	O
neural	O
networks	O
.	O

Experimental	O
results	O
for	O
each	O
evaluation	O
dataset	O
are	O
listed	O
in	O
Table	O
2	O
,	O
3	O
and	O
4	O
.	O

Results	O
and	O
Discussion	O
.	O

This	O
is	O
added	O
to	O
verify	O
that	O
the	O
success	O
of	O
our	O
model	O
is	O
not	O
only	O
because	O
our	O
model	O
uses	O
neural	O
networks	O
.	O

We	O
compute	O
idf	O
using	O
the	O
validation	O
data	O
.	O

Lead	O
is	O
a	O
simple	O
method	O
that	O
extracts	O
the	O
first	O
few	O
sentences	O
from	O
the	O
source	O
text	O
but	O
is	O
considered	O
as	O
a	O
strong	O
baseline	O
for	O
the	O
summarization	O
of	O
news	O
articles	O
.	O

Baseline	O
.	O

Each	O
model	O
extracts	O
3	O
sentences	O
as	O
a	O
summary	O
.	O

In	O
the	O
evaluation	O
phase	O
,	O
we	O
only	O
use	O
the	O
Encoder	O
and	O
Extractor	O
and	O
do	O
not	O
use	O
the	O
Predictor	O
.	O

Evaluation	O
.	O

We	O
conduct	O
the	O
same	O
experiment	O
five	O
times	O
and	O
use	O
the	O
average	O
of	O
the	O
results	O
to	O
mitigate	O
the	O
effect	O
of	O
randomness	O
rooting	O
in	O
initialization	O
and	O
optimization	O
.	O

Thus	O
,	O
we	O
train	O
the	O
Predictor	O
in	O
the	O
first	O
few	O
epochs	O
before	O
training	O
the	O
Extractor	O
.	O

Models	O
with	O
several	O
components	O
generally	O
achieve	O
better	O
results	O
if	O
each	O
component	O
is	O
pretrained	O
separately	O
(	O
Hashimoto	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

The	O
Extractor	O
learns	O
to	O
extract	O
proper	O
sentences	O
and	O
the	O
Predictor	O
learns	O
to	O
predict	O
the	O
relation	O
between	O
a	O
post	O
and	O
a	O
reply	O
candidate	O
.	O

This	O
is	O
to	O
train	O
the	O
Extractor	O
and	O
the	O
Predictor	O
efficiently	O
.	O

In	O
the	O
first	O
few	O
epochs	O
,	O
we	O
do	O
not	O
use	O
the	O
Extractor	O
;	O
all	O
the	O
post	O
sentences	O
are	O
used	O
for	O
the	O
prediction	O
of	O
post	O
-	O
reply	O
relations	O
.	O

Training	O
.	O

We	O
use	O
3,000	O
posts	O
and	O
tldrs	O
that	O
are	O
not	O
included	O
in	O
the	O
training	O
dataset	O
as	O
the	O
validation	O
dataset	O
,	O
and	O
the	O
same	O
number	O
of	O
posts	O
and	O
tldrs	O
as	O
the	O
evaluation	O
dataset	O
.	O

We	O
use	O
that	O
367,000	O
pairs	O
of	O
posts	O
and	O
replies	O
as	O
the	O
training	O
dataset	O
.	O

As	O
a	O
consequence	O
,	O
we	O
obtained	O
183,500	O
correct	O
pairs	O
of	O
posts	O
and	O
replies	O
and	O
the	O
same	O
number	O
of	O
wrong	O
pairs	O
.	O

tldr	O
briefly	O
explains	O
what	O
is	O
written	O
in	O
the	O
original	O
post	O
and	O
thus	O
can	O
be	O
regarded	O
as	O
a	O
summary	O
.	O

Therefore	O
,	O
we	O
have	O
112,348	O
pairs	O
in	O
total	O
.	O

The	O
number	O
of	O
positive	O
labels	O
and	O
negative	O
labels	O
are	O
equal	O
.	O

We	O
labeled	O
a	O
pair	O
with	O
an	O
actual	O
reply	O
as	O
positive	O
and	O
a	O
pair	O
with	O
a	O
wrong	O
reply	O
that	O
is	O
randomly	O
sampled	O
from	O
the	O
whole	O
dataset	O
as	O
negative	O
.	O

After	O
the	O
preprocessing	O
,	O
we	O
have	O
56,174	O
pairs	O
.	O

We	O
exclude	O
pairs	O
where	O
the	O
number	O
of	O
words	O
in	O
a	O
post	O
or	O
a	O
reply	O
is	O
smaller	O
than	O
50	O
or	O
25	O
.	O

From	O
this	O
dataset	O
,	O
we	O
use	O
post	O
-	O
and	O
-	O
reply	O
pairs	O
to	O
train	O
our	O
model	O
.	O

Mail	O
Dataset	O
.	O

We	O
train	O
and	O
evaluate	O
the	O
model	O
on	O
two	O
domains	O
of	O
datasets	O
.	O

Experiment	O
.	O

To	O
take	O
advantage	O
of	O
our	O
method	O
and	O
conventional	O
methods	O
,	O
we	O
employ	O
reranking	O
;	O
we	O
simply	O
reorder	O
summaries	O
(	O
3	O
sentences	O
)	O
extracted	O
by	O
our	O
model	O
based	O
on	O
the	O
ranking	O
of	O
TextRank	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004	O
)	O
.	O

L	O
rep	O
=	O
−t	O
rep	O
log	O
y	O
−	O
(	O
1	O
−	O
t	O
rep	O
)	O
log	O
(	O
1	O
−	O
y	O
)	O
(	O
11	O
)	O
Reranking	O
As	O
we	O
mentioned	O
in	O
the	O
Introduction	O
,	O
we	O
are	O
seeking	O
for	O
a	O
criterion	O
that	O
is	O
different	O
from	O
conventional	O
methods	O
.	O

The	O
loss	O
of	O
this	O
classification	O
L	O
rep	O
is	O
obtained	O
by	O
cross	O
entropy	O
as	O
follows	O
where	O
t	O
rep	O
is	O
1	O
when	O
a	O
reply	O
candidate	O
is	O
an	O
actual	O
reply	O
,	O
and	O
otherwise	O
0	O
.	O

The	O
detail	O
of	O
the	O
computation	O
is	O
described	O
in	O
Appendix	O
A.1	O
.	O
Decomposable	O
Attention	O
.	O

y	O
=	O
sigmoid(DA(x	O
ext	O
1	O
,	O
...	O
,	O
x	O
ext	O
L−1	O
,	O
h	O
r	O
1	O
,	O
...	O
,	O
h	O
r	O
M	O
)	O
)	O
(	O
10	O
)	O
where	O
DA	O
denotes	O
Decomposable	O
Attention	O
.	O

From	O
this	O
architecture	O
,	O
we	O
obtain	O
the	O
probability	O
of	O
binary	O
-	O
classification	O
y	O
through	O
the	O
sigmoid	O
function	O
.	O

Sentence	O
vectors	O
{	O
h	O
r	O
j	O
}	O
of	O
each	O
sentence	O
{	O
s	O
r	O
j	O
}	O
on	O
the	O
reply	O
are	O
computed	O
similarly	O
to	O
the	O
equation	O
1	O
.	O

Suppose	O
a	O
reply	O
candidate	O
R	O
=	O
{	O
s	O
r	O
1	O
,	O
s	O
r	O
2	O
,	O
...	O
,	O
s	O
r	O
M	O
}	O
has	O
M	O
sentences	O
.	O

We	O
labeled	O
actual	O
replies	O
as	O
positive	O
,	O
and	O
randomly	O
sampled	O
posts	O
as	O
negative	O
.	O

Predictor	O
Then	O
,	O
using	O
only	O
the	O
extracted	O
sentences	O
and	O
a	O
reply	O
candidate	O
,	O
the	O
Predictor	O
predicts	O
whether	O
the	O
candidate	O
is	O
an	O
actual	O
reply	O
or	O
not	O
.	O

x	O
ext	O
t	O
=	O
N	O
i=1	O
α	O
ti	O
h	O
p	O
i	O
(	O
1	O
≤	O
t	O
≤	O
L	O
)	O
(	O
8)	O
h	O
ext	O
t+1	O
=	O
LSTM(x	O
ext	O
t	O
)	O
(	O
0	O
≤	O
t	O
≤	O
L	O
−	O
1	O
)	O
(	O
9	O
)	O
The	O
initial	O
input	O
vector	O
x	O
ext	O
0	O
of	O
the	O
Extractor	O
is	O
a	O
parameter	O
,	O
and	O
L	O
is	O
defined	O
by	O
a	O
user	O
depending	O
on	O
the	O
number	O
of	O
sentences	O
required	O
for	O
a	O
summary	O
.	O

We	O
repeat	O
this	O
step	O
L	O
times	O
.	O

By	O
adding	O
Gumbel	O
noise	O
g	O
using	O
noise	O
u	O
from	O
a	O
uniform	O
distribution	O
,	O
the	O
attention	O
weights	O
a	O
become	O
a	O
one	O
-	O
hot	O
vector	O
.	O

During	O
the	O
training	O
,	O
we	O
use	O
Gumbel	O
Softmax	O
(	O
Jang	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
make	O
this	O
discrete	O
process	O
differentiable	O
.	O

The	O
sentence	O
with	O
the	O
highest	O
attention	O
weight	O
is	O
extracted	O
.	O

h	O
ext	O
0	O
=	O
1	O
N	O
N	O
i=1	O
h	O
p	O
i	O
(	O
2	O
)	O
The	O
Extractor	O
computes	O
attention	O
weights	O
using	O
the	O
hidden	O
states	O
of	O
the	O
Extractor	O
h	O
ext	O
t	O
and	O
the	O
sentence	O
features	O
h	O
p	O
i	O
computed	O
on	O
the	O
Encoder	O
.	O

We	O
set	O
the	O
mean	O
vector	O
of	O
the	O
sentence	O
features	O
of	O
the	O
Encoder	O
h	O
p	O
i	O
as	O
the	O
initial	O
hidden	O
state	O
of	O
the	O
Extractor	O
h	O
ext	O
0	O
.	O

This	O
is	O
because	O
summaries	O
should	O
not	O
depend	O
on	O
replies	O
.	O

Note	O
that	O
the	O
Extractor	O
does	O
not	O
use	O
reply	O
features	O
for	O
extraction	O
.	O

For	O
accurate	O
prediction	O
,	O
the	O
Extractor	O
learns	O
to	O
extract	O
sentences	O
that	O
replies	O
frequently	O
refer	O
to	O
.	O

..	O
,	O
x	O
p	O
iK	O
i	O
}	O
through	O
word	O
embedding	O
layers	O
.	O

Words	O
are	O
embedded	O
to	O
continuous	O
vectors	O
X	O
p	O
i	O
=	O
{	O
x	O
p	O
i1	O
,	O
x	O
p	O
i2	O
,	O
.	O

Each	O
sentence	O
s	O
p	O
i	O
comprises	O
K	O
i	O
words	O
W	O
p	O
i	O
=	O
{	O
w	O
p	O
i1	O
,	O
w	O
p	O
i2	O
,	O
...	O
,	O
w	O
p	O
iK	O
i	O
}	O
.	O

First	O
,	O
the	O
post	O
is	O
split	O
into	O
N	O
sentences	O
{	O
s	O
p	O
1	O
,	O
s	O
p	O
2	O
,	O
...	O
,	O
s	O
p	O
N	O
}	O
.	O

Encoder	O
The	O
Encoder	O
computes	O
features	O
of	O
posts	O
.	O

We	O
describe	O
each	O
component	O
below	O
.	O

The	O
Encoder	O
computes	O
features	O
of	O
posts	O
,	O
the	O
Extractor	O
extracts	O
sentences	O
of	O
a	O
post	O
to	O
use	O
for	O
prediction	O
,	O
and	O
the	O
Predictor	O
predicts	O
whether	O
a	O
reply	O
candidate	O
is	O
an	O
actual	O
reply	O
or	O
not	O
.	O

The	O
model	O
comprises	O
an	O
Encoder	O
,	O
an	O
Extractor	O
,	O
and	O
a	O
Predictor	O
.	O

The	O
training	O
task	O
of	O
the	O
model	O
is	O
to	O
predict	O
whether	O
a	O
reply	O
candidate	O
is	O
true	O
or	O
not	O
.	O

A	O
reply	O
candidate	O
can	O
be	O
either	O
a	O
true	O
or	O
a	O
false	O
reply	O
to	O
the	O
post	O
.	O

The	O
inputs	O
to	O
the	O
model	O
during	O
training	O
are	O
a	O
post	O
and	O
reply	O
candidate	O
.	O

Figure	O
2	O
shows	O
the	O
structure	O
of	O
the	O
model	O
.	O

Model	O
.	O

In	O
our	O
research	O
,	O
we	O
solely	O
focus	O
on	O
quotes	O
,	O
and	O
do	O
not	O
directly	O
use	O
quotes	O
as	O
supervision	O
;	O
rather	O
,	O
we	O
aim	O
to	O
extract	O
implicit	O
quotes	O
.	O

The	O
previous	O
research	O
used	O
quotes	O
as	O
auxiliary	O
features	O
.	O

Some	O
previous	O
work	O
(	O
Carenini	O
et	O
al	O
.	O
,	O
2007;Oya	O
and	O
Carenini	O
,	O
2014	O
)	O
assigned	O
weights	O
to	O
words	O
that	O
appeared	O
in	O
quotes	O
,	O
and	O
improved	O
the	O
conventional	O
centroidbased	O
methods	O
.	O

When	O
we	O
reply	O
to	O
a	O
post	O
or	O
an	O
email	O
and	O
when	O
we	O
want	O
to	O
emphasize	O
a	O
certain	O
part	O
of	O
it	O
,	O
we	O
quote	O
the	O
original	O
text	O
.	O

Dialogue	O
act	O
classification	O
is	O
a	O
classification	O
task	O
that	O
classifies	O
sentences	O
depending	O
on	O
what	O
their	O
functions	O
are	O
(	O
e.g.	O
:	O
questions	O
,	O
answers	O
,	O
greetings	O
)	O
,	O
and	O
has	O
also	O
been	O
applied	O
for	O
summarization	O
(	O
Bhatia	O
et	O
al	O
.	O
,	O
2014;Oya	O
and	O
Carenini	O
,	O
2014	O
)	O
.	O

A	O
few	O
used	O
path	O
scores	O
of	O
word	O
graphs	O
(	O
Mehdad	O
et	O
al	O
.	O
,	O
2014;Shang	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

!	O
"	O
!	O
!	O
#	O
!	O
!	O
$	O
!	O
!	O
"	O
%	O
&	O
'	O
!	O
#	O
%	O
&	O
'	O
!	O
(	O
%	O
&	O
'	O
…	O
!	O
"	O
)	O
!	O
#	O
)	O
!	O
*	O
)	O
…	O
…	O
…	O
"	O
"	O
!	O
Split	O
to	O
sentences	O
Attention	O
&	O
Gumbel	O
Softmax	O
#	O
"	O
%	O
&	O
'	O
≓	O
!	O
+	O
!	O
#	O
,	O
%	O
&	O
'	O
#	O
(	O
%	O
&	O
'	O
≓	O
!	O
-	O
!	O
"	O
#	O
!	O
"	O
$	O
!	O
"	O
"	O
)	O
"	O
#	O
)	O
"	O
*	O
)	O
BiLSTM	O
BiLSTM	O
BiLSTM	O
…	O
Research	O
on	O
the	O
summarization	O
of	O
online	O
conversations	O
such	O
as	O
mail	O
,	O
chat	O
,	O
social	O
media	O
,	O
and	O
online	O
discussion	O
fora	O
has	O
been	O
conducted	O
for	O
a	O
long	O
time	O
.	O

Another	O
research	O
employed	O
a	O
task	O
to	O
reconstruct	O
masked	O
sentences	O
for	O
summarization	O
(	O
Laban	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

(	O
2019	O
)	O
(	O
2019	O
)	O
generated	O
summaries	O
from	O
mean	O
vectors	O
of	O
review	O
vectors	O
,	O
and	O
Amplayo	O
and	O
Lapata	O
(	O
2020	O
)	O
employed	O
the	O
prior	O
distribution	O
of	O
Variational	O
Auto	O
-	O
Encoder	O
to	O
induce	O
summaries	O
.	O

For	O
review	O
abstractive	O
summarization	O
,	O
Isonuma	O
et	O
al	O
.	O

(	O
2019	O
)	O
employed	O
the	O
reconstruction	O
task	O
of	O
the	O
original	O
sentence	O
from	O
a	O
compressed	O
one	O
.	O

Baziotis	O
et	O
al	O
.	O

For	O
sentence	O
compression	O
,	O
Fevry	O
and	O
Phang	O
(	O
2018	O
)	O
employed	O
the	O
task	O
to	O
reorder	O
the	O
shuffled	O
word	O
order	O
of	O
sentences	O
.	O

As	O
for	O
end	O
-	O
to	O
-	O
end	O
unsupervised	O
neural	O
models	O
,	O
a	O
few	O
abstractive	O
models	O
have	O
been	O
proposed	O
.	O

That	O
is	O
,	O
our	O
model	O
can	O
extract	O
salient	O
sentences	O
that	O
conventional	O
methods	O
fail	O
to	O
.	O

These	O
methods	O
assume	O
that	O
important	O
topics	O
appear	O
frequently	O
in	O
a	O
document	O
,	O
but	O
our	O
model	O
focuses	O
on	O
a	O
different	O
aspect	O
of	O
texts	O
:	O
the	O
probability	O
of	O
being	O
quoted	O
.	O

Other	O
models	O
use	O
reconstruction	O
loss	O
(	O
He	O
et	O
al	O
.	O
,	O
2012;Liu	O
et	O
al	O
.	O
,	O
2015;Ma	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
Kullback	O
-	O
Leibler	O
divergence	O
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
or	O
path	O
score	O
calculation	O
(	O
Mehdad	O
et	O
al	O
.	O
,	O
2014;Shang	O
et	O
al	O
.	O
,	O
2018	O
)	O
based	O
on	O
multi	O
-	O
sentence	O
compression	O
algorithm	O
(	O
Filippova	O
,	O
2010	O
)	O
.	O

The	O
graph	O
-	O
centrality	O
-	O
based	O
method	O
(	O
Mihalcea	O
and	O
Tarau	O
,	O
2004;Erkan	O
and	O
Radev	O
,	O
2004;Zheng	O
and	O
Lapata	O
,	O
2019	O
)	O
and	O
centroid	O
-	O
based	O
method	O
(	O
Gholipour	O
Ghalandari	O
,	O
2017	O
)	O
have	O
been	O
major	O
methods	O
in	O
this	O
field	O
.	O

Related	O
Works	O
.	O

The	O
contributions	O
of	O
our	O
research	O
are	O
as	O
follows	O
:	O
•	O
We	O
verified	O
that	O
"	O
the	O
possibility	O
of	O
being	O
quoted	O
"	O
is	O
useful	O
for	O
summarization	O
,	O
and	O
demonstrated	O
that	O
it	O
reflects	O
an	O
important	O
aspect	O
of	O
saliency	O
that	O
conventional	O
methods	O
do	O
not	O
.	O

Furthermore	O
,	O
we	O
both	O
quantitatively	O
and	O
qualitatively	O
analyzed	O
that	O
our	O
model	O
can	O
capture	O
salient	O
sentences	O
that	O
conventional	O
frequency	O
-	O
based	O
methods	O
can	O
not	O
.	O

Our	O
model	O
is	O
based	O
on	O
a	O
hypothesis	O
that	O
the	O
ability	O
of	O
extracting	O
quotes	O
leads	O
to	O
a	O
good	O
result	O
.	O

The	O
model	O
requires	O
replies	O
only	O
during	O
the	O
training	O
and	O
not	O
during	O
the	O
evaluation	O
.	O

The	O
training	O
task	O
of	O
the	O
model	O
is	O
to	O
predict	O
if	O
a	O
reply	O
candidate	O
is	O
an	O
actual	O
reply	O
to	O
the	O
post	O
.	O

We	O
use	O
pairs	O
of	O
a	O
post	O
and	O
reply	O
candidate	O
to	O
train	O
the	O
model	O
.	O

As	O
shown	O
in	O
Figure	O
1	O
,	O
implicit	O
quotes	O
are	O
sentences	O
of	O
posts	O
that	O
are	O
not	O
explicitly	O
quoted	O
in	O
replies	O
,	O
but	O
are	O
those	O
the	O
replies	O
most	O
likely	O
refer	O
to	O
.	O

We	O
propose	O
a	O
model	O
that	O
can	O
be	O
trained	O
without	O
explicit	O
labels	O
of	O
quotes	O
.	O

However	O
,	O
most	O
replies	O
do	O
not	O
include	O
quotes	O
,	O
so	O
it	O
is	O
difficult	O
to	O
use	O
quotes	O
as	O
the	O
training	O
labels	O
of	O
neural	O
models	O
.	O

Previous	O
research	O
assigned	O
weights	O
to	O
words	O
that	O
appear	O
in	O
quotes	O
,	O
and	O
improved	O
the	O
centroidbased	O
summarization	O
(	O
Carenini	O
et	O
al	O
.	O
,	O
2007;Oya	O
and	O
Carenini	O
,	O
2014	O
)	O
.	O

Thus	O
,	O
we	O
aim	O
to	O
extract	O
quotes	O
as	O
summaries	O
.	O

If	O
we	O
can	O
predict	O
quoted	O
parts	O
,	O
we	O
can	O
extract	O
important	O
sentences	O
irrespective	O
of	O
how	O
frequently	O
the	O
same	O
topic	O
appears	O
in	O
the	O
text	O
.	O

The	O
reply	O
on	O
the	O
bottom	O
includes	O
a	O
quote	O
,	O
which	O
generally	O
starts	O
with	O
a	O
symbol	O
"	O
>	O
"	O
.	O

When	O
one	O
replies	O
to	O
an	O
email	O
or	O
a	O
post	O
,	O
a	O
quote	O
is	O
used	O
to	O
highlight	O
the	O
important	O
parts	O
of	O
the	O
text	O
;	O
an	O
example	O
is	O
shown	O
in	O
Figure	O
1	O
.	O

As	O
an	O
alternative	O
aspect	O
,	O
we	O
propose	O
"	O
the	O
probability	O
of	O
being	O
quoted	O
"	O
.	O

For	O
more	O
accurate	O
summarization	O
,	O
relying	O
solely	O
on	O
the	O
frequency	O
is	O
not	O
sufficient	O
and	O
we	O
need	O
to	O
focus	O
on	O
other	O
aspects	O
of	O
texts	O
.	O

Therefore	O
,	O
if	O
important	O
topics	O
appear	O
only	O
a	O
few	O
times	O
,	O
these	O
methods	O
fail	O
to	O
capture	O
salient	O
sentences	O
.	O

The	O
premise	O
of	O
these	O
methods	O
is	O
that	O
important	O
topics	O
appear	O
frequently	O
in	O
a	O
document	O
.	O

Apart	O
from	O
centrality	O
,	O
centroid	O
of	O
vectors	O
(	O
Gholipour	O
Ghalandari	O
,	O
2017	O
)	O
,	O
Kullback	O
-	O
Leibler	O
divergence	O
(	O
Haghighi	O
and	O
Vanderwende	O
,	O
2009	O
)	O
,	O
reconstruction	O
loss	O
(	O
He	O
et	O
al	O
.	O
,	O
2012;Liu	O
et	O
al	O
.	O
,	O
2015;Ma	O
et	O
al	O
.	O
,	O
2016	O
)	O
,	O
and	O
path	O
scores	O
of	O
word	O
graphs	O
(	O
Mehdad	O
et	O
al	O
.	O
,	O
2014;Shang	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
are	O
leveraged	O
for	O
summarization	O
.	O

Because	O
it	O
is	O
not	O
realistic	O
to	O
prepare	O
such	O
large	O
datasets	O
for	O
every	O
domain	O
,	O
there	O
is	O
a	O
growing	O
requirement	O
for	O
unsupervised	O
methods	O
.	O

Supervised	O
summarization	O
requires	O
tens	O
of	O
thousands	O
of	O
human	O
-	O
annotated	O
summaries	O
.	O

Introduction	O
.	O

We	O
further	O
discuss	O
two	O
topics	O
;	O
one	O
is	O
whether	O
quote	O
extraction	O
is	O
an	O
important	O
factor	O
for	O
summarization	O
,	O
and	O
the	O
other	O
is	O
whether	O
our	O
model	O
can	O
capture	O
salient	O
sentences	O
that	O
conventional	O
methods	O
can	O
not	O
.	O

To	O
predict	O
accurately	O
,	O
the	O
model	O
learns	O
to	O
extract	O
sentences	O
that	O
replies	O
frequently	O
refer	O
to	O
.	O

For	O
prediction	O
,	O
the	O
model	O
has	O
to	O
choose	O
a	O
few	O
sentences	O
from	O
the	O
post	O
.	O

The	O
training	O
task	O
of	O
the	O
model	O
is	O
to	O
predict	O
whether	O
a	O
reply	O
candidate	O
is	O
a	O
true	O
reply	O
to	O
a	O
post	O
.	O

However	O
,	O
even	O
if	O
it	O
is	O
not	O
explicitly	O
shown	O
,	O
replies	O
always	O
refer	O
to	O
certain	O
parts	O
of	O
texts	O
;	O
we	O
call	O
them	O
implicit	O
quotes	O
.	O

Most	O
replies	O
do	O
not	O
explicitly	O
include	O
quotes	O
,	O
so	O
it	O
is	O
difficult	O
to	O
use	O
quotes	O
as	O
supervision	O
.	O

We	O
aim	O
to	O
extract	O
quoted	O
sentences	O
as	O
summaries	O
.	O

When	O
we	O
reply	O
to	O
posts	O
,	O
quotes	O
are	O
used	O
to	O
highlight	O
important	O
part	O
of	O
texts	O
.	O

