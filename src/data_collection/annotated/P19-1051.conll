As	O
discussed	O
in	O
§	O
1	O
,	O
this	O
task	O
is	O
usually	O
divided	O
into	O
two	O
subtasks	O
.	O

Apart	O
from	O
sentence	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
(	O
Lin	O
and	O
He	O
,	O
2009;Kim	O
,	O
2014	O
)	O
,	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
which	O
requires	O
the	O
detection	O
of	O
sentiments	O
towards	O
mentioned	O
entities	O
in	O
the	O
open	O
domain	O
,	O
is	O
also	O
an	O
important	O
research	O
topic	O
.	O

Moreover	O
,	O
we	O
find	O
that	O
the	O
pipeline	B-MethodName
model	I-MethodName
consistently	O
surpasses	O
both	O
the	O
joint	B-MethodName
model	I-MethodName
and	O
the	O
collapsed	B-MethodName
model	I-MethodName
.	O

TAG	B-MethodName
SPAN	B-MethodName
suitable	O
for	O
long	O
sentences	O
.	O

Sentence	O
.	O

This	O
work	O
was	O
supported	O
by	O
the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
(	O
2016YFB1000101	O
)	O
.	O

We	O
also	O
thank	O
Li	O
Dong	O
for	O
his	O
helpful	O
comments	O
and	O
suggestions	O
.	O

We	O
thank	O
the	O
anonymous	O
reviewers	O
for	O
their	O
insightful	O
feedback	O
.	O

Acknowledgments	O
.	O

Model	O
analysis	O
reveals	O
that	O
the	O
main	O
performance	O
improvement	O
comes	O
from	O
the	O
span	O
-	O
level	O
polarity	O
classifier	O
,	O
and	O
the	O
multi	O
-	O
target	O
extractor	O
is	O
more	O
.	O

Our	O
approach	O
firmly	O
outperforms	O
the	O
sequence	O
tagging	O
baseline	O
as	O
well	O
as	O
previous	O
stateof	O
-	O
the	O
-	O
art	O
methods	O
on	O
three	O
benchmark	O
datasets	O
.	O

On	O
top	O
of	O
it	O
,	O
we	O
design	O
a	O
multi	O
-	O
target	O
extractor	O
for	O
proposing	O
multiple	O
candidate	O
targets	O
with	O
an	O
heuristic	O
multispan	O
decoding	O
algorithm	O
,	O
and	O
introduce	O
a	O
polarity	O
classifier	O
that	O
predicts	O
the	O
sentiment	O
towards	O
each	O
candidate	O
using	O
its	O
summarized	O
span	O
representation	O
.	O

The	O
framework	O
contains	O
a	O
pre	O
-	O
trained	O
Transformer	O
encoder	O
as	O
the	O
backbone	O
network	O
.	O

We	O
re	O
-	O
examine	O
the	O
drawbacks	O
of	O
sequence	O
tagging	O
methods	O
in	O
open	O
-	O
domain	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
,	O
and	O
propose	O
an	O
extract	B-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
classify	I-MethodName
framework	I-MethodName
with	O
the	O
span	B-MethodName
-	I-MethodName
based	I-MethodName
labeling	I-MethodName
scheme	I-MethodName
instead	O
.	O

Conclusion	O
.	O

Our	O
polarity	O
classifier	O
,	O
however	O
,	O
can	O
avoid	O
such	O
problem	O
by	O
using	O
the	O
target	O
span	O
representation	O
to	O
predict	O
the	O
sentiment	O
.	O

Moreover	O
,	O
we	O
find	O
that	O
the	O
tagging	O
method	O
sometimes	O
fails	O
to	O
predict	O
the	O
correct	O
sen-	O
timent	O
class	O
,	O
especially	O
when	O
the	O
target	O
consists	O
of	O
multiple	O
words	O
(	O
e.g.	O
,	O
"	O
battery	O
cycle	O
count	O
"	O
in	O
(	O
5	O
)	O
and	O
"	O
Casa	O
La	O
Femme	O
"	O
in	O
(	O
6	O
)	O
)	O
,	O
indicating	O
the	O
tagger	O
can	O
not	O
effectively	O
maintain	O
sentiment	O
consistency	O
across	O
words	O
.	O

For	O
example	O
,	O
it	O
additionally	O
predicts	O
the	O
entity	O
"	O
food	O
"	O
as	O
a	O
target	O
in	O
the	O
second	O
example	O
.	O

In	O
contrast	O
,	O
the	O
tagging	O
method	O
does	O
not	O
rely	O
on	O
a	O
threshold	O
and	O
is	O
observed	O
to	O
have	O
a	O
higher	O
recall	O
.	O

As	O
a	O
result	O
,	O
the	O
model	O
only	O
makes	O
cautious	O
but	O
confident	O
predictions	O
.	O

We	O
find	O
that	O
our	O
approach	O
may	O
sometimes	O
fail	O
to	O
propose	O
target	O
entities	O
(	O
e.g.	O
,	O
"	O
adjustments	O
"	O
in	O
(	O
3	O
)	O
and	O
"	O
feel	O
"	O
in	O
(	O
4	O
)	O
)	O
,	O
which	O
is	O
due	O
to	O
the	O
fact	O
that	O
a	O
relatively	O
large	O
has	O
been	O
set	O
.	O

But	O
when	O
it	O
comes	O
to	O
shorter	O
inputs	O
(	O
e.g.	O
,	O
the	O
third	O
and	O
the	O
fourth	O
examples	O
)	O
,	O
the	O
tagging	O
baseline	O
usually	O
performs	O
better	O
than	O
our	O
approach	O
.	O

A	O
likely	O
reason	O
of	O
its	O
failure	O
is	O
that	O
the	O
input	O
sentences	O
are	O
relatively	O
longer	O
,	O
and	O
the	O
tagging	O
method	O
is	O
less	O
effective	O
when	O
dealing	O
with	O
them	O
.	O

As	O
observed	O
in	O
the	O
first	O
two	O
examples	O
,	O
the	O
"	O
TAG	B-MethodName
"	O
model	O
incorrectly	O
predicts	O
the	O
target	O
span	O
by	O
either	O
missing	O
the	O
word	O
"	O
Mac	O
"	O
or	O
proposing	O
a	O
phrase	O
across	O
two	O
targets	O
(	O
"	O
scallps	O
and	O
prawns	O
"	O
)	O
.	O

Table	O
4	O
shows	O
some	O
qualitative	O
cases	O
sampled	O
from	O
the	O
pipeline	O
methods	O
.	O

Case	O
Study	O
.	O

Our	O
span	O
-	O
based	O
method	O
,	O
on	O
the	O
contrary	O
,	O
can	O
naturally	O
alleviate	O
such	O
problem	O
because	O
the	O
polarity	O
is	O
classified	O
by	O
taking	O
all	O
target	O
words	O
into	O
account	O
.	O

It	O
demonstrates	O
that	O
the	O
tagging	O
method	O
indeed	O
suffers	O
from	O
the	O
sentiment	O
inconsistency	O
problem	O
when	O
it	O
comes	O
to	O
multi	O
-	O
word	O
target	O
entities	O
.	O

The	O
performance	O
of	O
tagging	O
baseline	O
,	O
however	O
,	O
significantly	O
decreases	O
as	O
the	O
target	O
becomes	O
longer	O
.	O

We	O
find	O
that	O
the	O
accuracy	O
of	O
span	O
-	O
level	O
classifier	O
only	O
drops	O
a	O
little	O
as	O
the	O
number	O
of	O
words	O
increases	O
on	O
the	O
LAPTOP	B-DatasetName
and	O
REST	B-DatasetName
datasets	O
.	O

To	O
gain	O
more	O
insights	O
on	O
performance	O
improvements	O
,	O
we	O
plot	O
the	O
accuracy	O
of	O
both	O
methods	O
with	O
respect	O
to	O
different	O
target	O
lengths	O
in	O
Figure	O
7	O
.	O

The	O
large	O
improvement	O
over	O
the	O
tagging	O
baseline	O
suggests	O
that	O
detecting	O
sentiment	O
with	O
the	O
entire	O
span	O
representation	O
is	O
much	O
more	O
beneficial	O
than	O
predicting	O
polarities	O
over	O
each	O
word	O
,	O
as	O
the	O
semantics	O
of	O
the	O
given	O
target	O
has	O
been	O
fully	O
considered	O
.	O

The	O
results	O
show	O
that	O
our	O
approach	O
significantly	O
outperforms	O
the	O
tagging	O
baseline	O
by	O
achieving	O
9.97	B-MetricValue
%	I-MetricValue
,	O
8.15	B-MetricValue
%	I-MetricValue
and	O
15.4	B-MetricValue
%	I-MetricValue
absolute	O
gains	O
on	O
three	O
datasets	O
,	O
and	O
firmly	O
surpasses	O
previous	O
stateof	O
-	O
the	O
-	O
art	O
models	O
on	O
LAPTOP	B-DatasetName
.	O

To	O
assess	O
the	O
polarity	O
classification	O
subtask	O
,	O
we	O
compare	O
the	O
performance	O
of	O
our	O
span	B-MethodName
-	I-MethodName
level	I-MethodName
polarity	I-MethodName
classifier	I-MethodName
with	O
the	O
CRF	B-MethodName
-	I-MethodName
based	I-MethodName
tagger	I-MethodName
in	O
Table	O
5	O
.	O

Analysis	O
on	O
Polarity	O
Classification	O
.	O

Moreover	O
,	O
removing	O
the	O
non	O
-	O
maximum	O
suppression	O
(	O
NMS	O
)	O
leads	O
to	O
significant	O
performance	O
degradations	O
,	O
suggesting	O
that	O
it	O
is	O
crucial	O
to	O
prune	O
redundant	O
spans	O
that	O
refer	O
to	O
the	O
same	O
text	O
.	O

The	O
model	O
without	O
length	O
heuristics	O
is	O
very	O
likely	O
to	O
output	O
the	O
whole	O
phrase	O
as	O
a	O
single	O
target	O
,	O
thus	O
being	O
totally	O
wrong	O
.	O

By	O
sampling	O
incorrect	O
predictions	O
we	O
find	O
that	O
there	O
are	O
many	O
targets	O
closely	O
aligned	O
with	O
each	O
other	O
,	O
such	O
as	O
"	O
perfect	O
[	O
size	O
]	O
+	O
and	O
[	O
speed	O
]	O
+	O
"	O
,	O
"	O
[	O
portions	O
]	O
+	O
all	O
at	O
a	O
reasonable	O
[	O
price	O
]	O
+	O
"	O
,	O
and	O
so	O
on	O
.	O

As	O
can	O
be	O
seen	O
from	O
Figure	O
6	O
,	O
ablating	O
the	O
length	O
heuristics	O
results	O
in	O
consistent	O
performance	O
drops	O
across	O
two	O
datasets	O
.	O

Since	O
a	O
trade	O
-	O
off	O
between	O
precision	B-MetricName
and	O
recall	B-MetricName
can	O
be	O
adjusted	O
according	O
to	O
the	O
threshold	O
in	O
our	O
extractor	O
,	O
we	O
further	O
plot	O
the	O
precision	O
-	O
recall	O
curves	O
under	O
different	O
ablations	O
to	O
show	O
the	O
effects	O
of	O
heuristic	O
multi	O
-	O
span	O
decoding	O
algorithm	O
.	O

The	O
above	O
result	O
demonstrates	O
that	O
our	O
extractor	O
is	O
more	O
suitable	O
for	O
long	O
sentences	O
due	O
to	O
the	O
fact	O
that	O
its	O
search	O
space	O
only	O
increases	O
linearly	O
with	O
the	O
sentence	O
length	O
.	O

A	O
likely	O
reason	O
for	O
this	O
observation	O
is	O
that	O
the	O
lengths	O
of	O
input	O
sentences	O
on	O
these	O
datasets	O
are	O
usually	O
small	O
(	O
e.g.	O
,	O
98	O
%	O
of	O
sentences	O
are	O
less	O
than	O
40	O
words	O
in	O
REST	B-DatasetName
)	O
,	O
which	O
limits	O
the	O
tagger	O
's	O
search	O
space	O
(	O
the	O
power	O
set	O
of	O
all	O
sentence	O
words	O
)	O
.	O

Our	O
extractor	O
manages	O
to	O
surpass	O
the	O
tagger	O
by	O
16.1	B-MetricValue
F1	B-MetricName
and	O
1.0	B-MetricValue
F1	B-MetricName
when	O
the	O
length	O
exceeds	O
40	O
on	O
LAPTOP	B-DatasetName
and	O
REST	B-DatasetName
,	O
respectively	O
.	O

We	O
observe	O
that	O
the	O
performance	O
of	O
BIO	B-MethodName
tagger	I-MethodName
dramatically	O
decreases	O
as	O
the	O
sentence	O
length	O
increases	O
,	O
while	O
our	O
extractor	O
is	O
more	O
robust	O
for	O
long	O
sentences	O
.	O

In	O
order	O
to	O
confirm	O
the	O
above	O
hypothesis	O
,	O
we	O
plot	O
the	O
F1	B-MetricName
score	O
with	O
respect	O
to	O
different	O
sentence	O
lengths	O
in	O
Figure	O
5	O
.	O

As	O
a	O
result	O
,	O
the	O
computational	O
complexity	O
has	O
been	O
largely	O
reduced	O
,	O
which	O
is	O
beneficial	O
for	O
the	O
tagging	O
method	O
.	O

We	O
find	O
that	O
the	O
BIO	B-MethodName
tagger	I-MethodName
outperforms	O
our	O
extractor	O
on	O
LAPTOP	B-DatasetName
and	O
REST	B-DatasetName
.	O

To	O
analyze	O
the	O
performance	O
on	O
target	O
extraction	O
,	O
we	O
run	O
both	O
the	O
tagging	O
baseline	O
and	O
the	O
multitarget	O
extractor	O
on	O
three	O
datasets	O
,	O
as	O
shown	O
in	O
Table	O
3	O
.	O

Analysis	O
on	O
Target	O
Extraction	O
.	O

Model	O
.	O

The	O
conclusion	O
is	O
also	O
supported	O
by	O
the	O
result	O
of	O
SPANcollapsed	O
method	O
,	O
which	O
severely	O
drops	O
across	O
all	O
datasets	O
,	O
implying	O
that	O
merging	O
polarity	O
labels	O
into	O
target	O
spans	O
does	O
not	O
address	O
the	O
task	O
effectively	O
.	O

This	O
suggests	O
that	O
there	O
is	O
only	O
a	O
weak	O
connection	O
between	O
target	O
extraction	O
and	O
polarity	O
classification	O
.	O

(	O
2015	O
)	O
.	O

(	O
2013	O
)	O
;	O
Zhang	O
et	O
al	O
.	O

Second	O
,	O
among	O
the	O
span	O
-	O
based	O
methods	O
,	O
the	O
SPAN	B-MethodName
-	I-MethodName
pipeline	I-MethodName
achieves	O
the	O
best	O
performance	O
,	O
which	O
is	O
similar	O
to	O
the	O
results	O
of	O
Mitchell	O
et	O
al	O
.	O

The	O
best	O
span	O
-	O
based	O
method	O
achieves	O
1.55	B-MetricValue
%	I-MetricValue
,	O
0.94	B-MetricValue
%	I-MetricValue
and	O
3.43	B-MetricValue
%	I-MetricValue
absolute	O
gains	O
on	O
three	O
datasets	O
compared	O
to	O
the	O
best	O
tagging	O
method	O
,	O
indicating	O
the	O
efficacy	O
of	O
our	O
extract	B-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
classify	I-MethodName
framework	I-MethodName
.	O

First	O
,	O
despite	O
that	O
the	O
"	O
TAG	B-MethodName
"	O
baselines	O
already	O
outperform	O
previous	O
best	O
approach	O
(	O
"	O
UNIFIED	B-MethodName
"	O
)	O
,	O
they	O
are	O
all	O
beaten	O
by	O
the	O
"	O
SPAN	B-MethodName
"	O
methods	O
.	O

Two	O
main	O
observations	O
can	O
be	O
obtained	O
from	O
the	O
Table	O
.	O

We	O
denote	O
our	O
approach	O
as	O
"	O
SPAN	B-MethodName
"	O
,	O
and	O
use	O
BERT	O
LARGE	O
as	O
backbone	O
networks	O
for	O
both	O
the	O
"	O
TAG	B-MethodName
"	O
and	O
"	O
SPAN	B-MethodName
"	O
models	O
to	O
make	O
the	O
comparison	O
fair	O
.	O

We	O
compare	O
models	O
under	O
either	O
the	O
sequence	O
tagging	O
scheme	O
or	O
the	O
span	O
-	O
based	O
labeling	O
scheme	O
,	O
and	O
show	O
the	O
results	O
in	O
Table	O
2	O
.	O

Main	O
Results	O
.	O

TNet	B-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2018	O
)	O
is	O
the	O
current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
on	O
polarity	O
classification	O
,	O
which	O
consists	O
of	O
a	O
multi	O
-	O
layer	O
context	O
-	O
preserving	O
network	O
architecture	O
and	O
uses	O
CNNs	O
as	O
feature	O
extractor	O
5	O
.	O

Finally	O
,	O
the	O
polarity	O
classifier	O
is	O
compared	O
with	O
the	O
following	O
methods	O
:	O
MGAN	B-MethodName
(	O
Fan	O
et	O
al	O
.	O
,	O
2018	O
)	O
uses	O
a	O
multi	O
-	O
grained	O
attention	O
mechanism	O
to	O
capture	O
interactions	O
between	O
targets	O
and	O
sentences	O
for	O
polarity	O
classification	O
.	O

We	O
also	O
compare	O
our	O
multi	O
-	O
target	O
extractor	O
with	O
the	O
following	O
method	O
:	O
DE	B-MethodName
-	I-MethodName
CNN	I-MethodName
(	O
Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
is	O
the	O
current	O
stateof	O
-	O
the	O
-	O
art	O
model	O
on	O
target	O
extraction	O
,	O
which	O
combines	O
a	O
double	O
embeddings	O
mechanism	O
with	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
4	O
.	O

It	O
contains	O
two	O
stacked	O
recurrent	O
neural	O
networks	O
enhanced	O
with	O
multi	O
-	O
task	O
learning	O
and	O
adopts	O
the	O
collapsed	O
tagging	O
scheme	O
.	O

UNIFIED	B-MethodName
(	O
Li	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
the	O
current	O
stateof	O
-	O
the	O
-	O
art	O
model	O
on	O
targeted	O
sentiment	O
analysis	O
3	O
.	O

"	O
pipeline	B-MethodName
"	O
and	O
"	O
joint	B-MethodName
"	O
denote	O
the	O
pipeline	O
and	O
joint	O
approaches	O
that	O
utilize	O
the	O
BIO	O
and	O
+	O
/-/0	O
tagging	O
schemes	O
,	O
while	O
"	O
collapsed	B-MethodName
"	O
is	O
the	O
model	O
following	O
the	O
collapsed	O
tagging	O
scheme	O
(	O
Figure	O
2(a	O
)	O
)	O
.	O

We	O
compare	O
the	O
proposed	O
span	O
-	O
based	O
approach	O
with	O
the	O
following	O
methods	O
:	O
TAG-{pipeline	B-MethodName
,	I-MethodName
joint	I-MethodName
,	I-MethodName
collapsed	I-MethodName
}	I-MethodName
are	O
the	O
sequence	O
tagging	O
baselines	O
that	O
involve	O
a	O
BERT	O
encoder	O
and	O
a	O
CRF	O
decoder	O
.	O

Baseline	O
Methods	O
.	O

Model	O
settings	O
.	O

All	O
experiments	O
are	O
conducted	O
on	O
a	O
single	O
NVIDIA	O
P100	O
GPU	O
card	O
.	O

The	O
threshold	O
is	O
manually	O
tuned	O
on	O
each	O
dataset	O
.	O

The	O
number	B-HyperparameterName
of	I-HyperparameterName
candidate	I-HyperparameterName
M	B-HyperparameterName
is	O
set	O
as	O
20	B-HyperparameterName
while	O
the	O
maximum	B-HyperparameterName
number	I-HyperparameterName
of	I-HyperparameterName
proposed	I-HyperparameterName
targets	I-HyperparameterName
K	B-HyperparameterName
is	O
10	B-HyperparameterValue
(	O
Algorithm	O
1	O
)	O
.	O

The	O
batch	B-HyperparameterName
size	I-HyperparameterName
is	O
32	B-HyperparameterValue
and	O
a	O
dropout	B-HyperparameterName
probability	I-HyperparameterName
of	O
0.1	B-HyperparameterValue
is	O
used	O
.	O

We	O
use	O
Adam	B-HyperparameterValue
optimizer	B-HyperparameterName
with	O
a	O
learning	B-HyperparameterName
rate	I-HyperparameterName
of	O
2e-5	B-HyperparameterValue
and	O
warmup	B-HyperparameterValue
over	O
the	O
first	B-HyperparameterValue
10	I-HyperparameterValue
%	I-HyperparameterValue
steps	I-HyperparameterValue
to	O
train	O
for	O
3	B-HyperparameterValue
epochs	B-HyperparameterValue
.	O

(	O
2018	O
)	O
for	O
details	O
on	O
model	O
sizes	O
.	O

and	O
refer	O
readers	O
to	O
Devlin	O
et	O
al	O
.	O

To	O
separately	O
analyze	O
the	O
performance	O
of	O
two	O
subtasks	O
,	O
precision	B-MetricName
,	O
recall	B-MetricName
,	O
and	O
F1	B-MetricName
are	O
also	O
used	O
for	O
the	O
target	O
extraction	O
subtask	O
,	O
while	O
the	O
accuracy	B-MetricName
(	O
ACC	B-MetricName
)	O
metric	O
is	O
applied	O
to	O
polarity	O
classification	O
.	O

A	O
predicted	O
target	O
is	O
correct	O
only	O
if	O
it	O
exactly	O
matches	O
the	O
gold	O
target	O
entity	O
and	O
the	O
corresponding	O
polarity	O
.	O

Metrices	O
We	O
adopt	O
the	O
precision	B-MetricName
(	O
P	B-MetricName
)	O
,	O
recall	B-MetricName
(	O
R	B-MetricName
)	O
,	O
and	O
F1	B-MetricName
score	O
as	O
evaluation	O
metrics	O
.	O

For	O
each	O
dataset	O
,	O
the	O
gold	O
target	O
span	O
boundaries	O
are	O
available	O
,	O
and	O
the	O
targets	O
are	O
labeled	O
with	O
three	O
sentiment	O
polarities	O
,	O
namely	O
positive	O
(	O
+	O
)	O
,	O
negative	O
(	O
-	O
)	O
,	O
and	O
neutral	O
(	O
0	O
)	O
.	O

(	O
2019	O
)	O
,	O
we	O
report	O
the	O
ten	O
-	O
fold	O
cross	O
validation	O
results	O
for	O
TWITTER	B-DatasetName
,	O
as	O
there	O
is	O
no	O
train	O
-	O
test	O
split	O
.	O

(	O
2015	O
)	O
;	O
Li	O
et	O
al	O
.	O

Following	O
Zhang	O
et	O
al	O
.	O

(	O
2013	O
)	O
,	O
consisting	O
of	O
twitter	O
posts	O
.	O

TWITTER	B-DatasetName
is	O
built	O
by	O
Mitchell	O
et	O
al	O
.	O

REST	B-DatasetName
is	O
the	O
union	O
set	O
of	O
the	O
restaurant	O
domain	O
from	O
SemEval	O
2014	O
,	O
2015	O
and	O
2016	O
(	O
Pontiki	O
et	O
al	O
.	O
,	O
2015(Pontiki	O
et	O
al	O
.	O
,	O
,	O
2016	O
)	O
)	O
.	O

LAPTOP	B-DatasetName
contains	O
product	O
reviews	O
from	O
the	O
laptop	O
domain	O
in	O
SemEval	B-DatasetName
2014	I-DatasetName
ABSA	I-DatasetName
challenges	O
(	O
Pontiki	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

Datasets	O
We	O
conduct	O
experiments	O
on	O
three	O
benchmark	O
sentiment	O
analysis	O
datasets	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O

Setup	O
.	O

Experiments	O
.	O

During	O
inference	O
,	O
the	O
heuristic	O
multi	O
-	O
span	O
decoding	O
algorithm	O
is	O
performed	O
on	O
each	O
set	O
of	O
scores	O
(	O
e.g.	O
,	O
g	O
s+	O
and	O
g	O
e+	O
)	O
,	O
and	O
the	O
output	O
sets	O
O	O
+	O
,	O
O	O
,	O
and	O
O	O
0	O
are	O
aggregated	O
as	O
the	O
final	O
prediction	O
.	O

Then	O
,	O
we	O
define	O
three	O
objectives	O
to	O
optimize	O
towards	O
each	O
polarity	O
.	O

We	O
then	O
modify	O
the	O
multi	O
-	O
target	O
extractor	O
by	O
producing	O
three	O
sets	O
of	O
probabilities	O
of	O
the	O
start	O
and	O
end	O
positions	O
,	O
where	O
each	O
set	O
corresponds	O
to	O
one	O
sentiment	O
class	O
(	O
e.g.	O
,	O
p	O
s+	O
and	O
p	O
e+	O
for	O
positive	O
targets	O
)	O
.	O

For	O
example	O
,	O
the	O
sentence	O
in	O
Figure	O
2(b	O
(	O
11-	O
,	O
11-	O
)	O
.	O

We	O
combine	O
target	O
span	O
boundaries	O
and	O
sentiment	O
polarities	O
into	O
one	O
label	O
space	O
.	O

Collapsed	B-MethodName
model	I-MethodName
.	O

The	O
inference	O
procedure	O
is	O
the	O
same	O
as	O
the	O
pipeline	B-MethodName
model	I-MethodName
.	O

A	O
joint	O
training	O
loss	O
L	O
+	O
J	O
is	O
used	O
to	O
optimize	O
the	O
whole	O
model	O
.	O

Joint	B-MethodName
model	I-MethodName
In	O
this	O
model	O
,	O
each	O
sentence	O
is	O
fed	O
into	O
a	O
shared	O
BERT	O
backbone	O
network	O
that	O
finally	O
branches	O
into	O
two	O
sibling	O
output	O
layers	O
:	O
one	O
for	O
proposing	O
multiple	O
candidate	O
targets	O
and	O
another	O
for	O
predicting	O
the	O
sentiment	O
polarity	O
over	O
each	O
extracted	O
target	O
.	O

Two	O
models	O
are	O
separately	O
trained	O
and	O
combined	O
as	O
a	O
pipeline	O
during	O
inference	O
.	O

Then	O
,	O
a	O
second	O
backbone	O
network	O
is	O
used	O
to	O
provide	O
contextual	O
sentence	O
vectors	O
for	O
the	O
polarity	O
classifier	O
.	O

(	O
2015	O
)	O
,	O
we	O
investigate	O
three	O
kinds	O
of	O
models	O
under	O
the	O
extract	B-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
classify	I-MethodName
framework	I-MethodName
:	O
Pipeline	B-MethodName
model	I-MethodName
We	O
first	O
build	O
a	O
multi	O
-	O
target	O
extractor	O
where	O
a	O
BERT	O
encoder	O
is	O
exclusively	O
used	O
.	O

(	O
2013	O
)	O
;	O
Zhang	O
et	O
al	O
.	O

Following	O
Mitchell	O
et	O
al	O
.	O

Model	O
Variants	O
.	O

During	O
inference	O
,	O
the	O
polarity	O
probability	O
is	O
calculated	O
for	O
each	O
candidate	O
target	O
span	O
in	O
the	O
set	O
O	O
,	O
and	O
the	O
sentiment	O
class	O
that	O
possesses	O
the	O
maximum	O
value	O
in	O
p	O
p	O
is	O
chosen	O
.	O

We	O
minimize	O
the	O
negative	O
log	O
probabilities	O
of	O
the	O
true	O
polarity	O
on	O
the	O
predicted	O
probability	O
as	O
:	O
J	O
=	O
X	O
k	O
i=1	O
y	O
p	O
i	O
log(p	O
p	O
i	O
)	O
where	O
y	O
p	O
is	O
an	O
one	O
-	O
hot	O
label	O
indicating	O
the	O
true	O
polarity	O
,	O
and	O
k	O
is	O
the	O
number	O
of	O
sentiment	O
classes	O
.	O

The	O
polarity	O
score	O
is	O
obtained	O
by	O
applying	O
two	O
linear	O
transformations	O
with	O
a	O
Tanh	O
activation	O
in	O
between	O
,	O
and	O
is	O
normalized	O
with	O
the	O
softmax	O
function	O
to	O
output	O
the	O
polarity	O
probability	O
as	O
:	O
g	O
p	O
=	O
W	O
p	O
tanh(W	O
v	O
v	O
)	O
,	O
p	O
p	O
=	O
softmax(g	O
p	O
)	O
where	O
W	O
v	O
2	O
R	O
h	O
⇥	O
h	O
and	O
W	O
p	O
2	O
R	O
k	O
⇥	O
h	O
are	O
two	O
trainable	O
parameter	O
matrices	O
.	O

(	O
2018	O
):	O
↵	O
=	O
softmax(w	O
↵	O
h	O
L	O
s	O
i	O
:	O
e	O
j	O
)	O
v	O
=	O
X	O
e	O
j	O
t	O
=	O
s	O
i	O
↵	O
t	O
s	O
i	O
+1	O
h	O
L	O
t	O
where	O
w	O
↵	O
2	O
R	O
h	O
is	O
a	O
trainable	O
weight	O
vector	O
.	O

(	O
2017	O
)	O
and	O
He	O
et	O
al	O
.	O

Specifically	O
,	O
given	O
a	O
target	O
span	O
r	O
,	O
we	O
calculate	O
a	O
summarized	O
vector	O
v	O
using	O
the	O
attention	O
mechanism	O
(	O
Bahdanau	O
et	O
al	O
.	O
,	O
2014	O
)	O
over	O
tokens	O
in	O
its	O
corrsponding	O
bound	O
(	O
s	O
i	O
,	O
e	O
j	O
)	O
,	O
similar	O
to	O
Lee	O
et	O
al	O
.	O

Instead	O
,	O
we	O
propose	O
to	O
summarize	O
the	O
target	O
representation	O
from	O
contextual	O
sentence	O
vectors	O
according	O
to	O
its	O
span	O
boundary	O
,	O
and	O
use	O
feed	O
-	O
forward	O
neural	O
networks	O
to	O
predict	O
the	O
sentiment	O
polarity	O
,	O
as	O
shown	O
in	O
Figure	O
3(b	O
)	O
.	O

Typically	O
,	O
polarity	O
classification	O
is	O
solved	O
using	O
either	O
sequence	O
tagging	O
methods	O
or	O
sophisticated	O
neural	O
networks	O
that	O
separately	O
encode	O
the	O
target	O
and	O
the	O
sentence	O
.	O

Polarity	O
Classifier	O
.	O

Input	O
:	O
g	O
s	O
,	O
g	O
e	O
,	O
,	O
K	O
g	O
s	O
denotes	O
the	O
score	O
of	O
start	O
positions	O
g	O
e	O
denotes	O
the	O
score	O
of	O
end	O
positions	O
is	O
a	O
minimum	O
score	O
threshold	O
K	O
is	O
the	O
maximum	O
number	O
of	O
proposed	O
targets	O
1	O
:	O
Initialize	O
R	O
,	O
U	O
,	O
O	O
=	O
{	O
}	O
,	O
{	O
}	O
,	O
{	O
}	O
2	O
:	O
Get	O
top	O
-	O
M	O
indices	O
S	O
,	O
E	O
from	O
g	O
s	O
,	O
g	O
e	O
3	O
:	O
for	O
si	O
in	O
S	O
do	O
4	O
:	O
for	O
ej	O
in	O
E	O
do	O
5	O
:	O
if	O
si	O
	O
ej	O
and	O
g	O
s	O
s	O
i	O
+	O
g	O
e	O
e	O
j	O
then	O
6	O
:	O
u	O
l	O
=	O
g	O
s	O
s	O
i	O
+	O
g	O
e	O
e	O
j	O
(	O
ej	O
si	O
+	O
1	O
)	O
7	O
:	O
r	O
l	O
=	O
(	O
si	O
,	O
ej	O
)	O
8	O
:	O
R	O
=	O
R	O
[	O
{	O
r	O
l	O
}	O
,	O
U	O
=	O
U	O
[	O
{	O
u	O
l	O
}	O
9	O
:	O
while	O
R	O
6	O
=	O
{	O
}	O
and	O
size(O	O
)	O
<	O
K	O
do	O
10	O
:	O
l	O
=	O
arg	O
max	O
U	O
11	O
:	O
O	O
=	O
O	O
[	O
{	O
r	O
l	O
}	O
;	O
R	O
=	O
R	O
{	O
r	O
l	O
}	O
;	O
U	O
=	O
U	O
{	O
u	O
l	O
}	O
12	O
:	O
for	O
r	O
k	O
in	O
R	O
do	O
13	O
:	O
if	O
f1(r	O
l	O
,	O
r	O
k	O
)	O
6	O
=	O
0	O
then	O
14	O
:	O
R	O
=	O
R	O
{	O
r	O
k	O
}	O
;	O
U	O
=	O
U	O
{	O
u	O
k	O
}	O
15	O
:	O
return	O
O.	O

Algorithm	O
1	O
Heuristic	O
multi	O
-	O
span	O
decoding	O
.	O

This	O
process	O
is	O
repeated	O
for	O
remaining	O
spans	O
in	O
R	O
,	O
until	O
R	O
is	O
empty	O
or	O
top	O
-	O
K	O
target	O
spans	O
have	O
been	O
proposed	O
(	O
line	O
9	O
)	O
.	O

We	O
also	O
delete	O
any	O
span	O
r	O
k	O
that	O
is	O
overlapped	O
with	O
r	O
l	O
,	O
which	O
is	O
measured	O
with	O
the	O
word	O
-	O
level	O
F1	O
function	O
(	O
line	O
12	O
-	O
14	O
)	O
.	O

Specifically	O
,	O
we	O
remove	O
the	O
span	O
r	O
l	O
that	O
possesses	O
the	O
maximum	O
score	O
u	O
l	O
from	O
the	O
set	O
R	O
and	O
add	O
it	O
to	O
the	O
set	O
O	O
(	O
line	O
10	O
-	O
11	O
)	O
.	O

Next	O
,	O
we	O
prune	O
redundant	O
spans	O
in	O
R	O
using	O
the	O
non	O
-	O
maximum	O
suppression	O
algorithm	O
(	O
Rosenfeld	O
and	O
Thurston	O
,	O
1971	O
)	O
.	O

Note	O
that	O
we	O
heuristically	O
calculate	O
u	O
l	O
as	O
the	O
sum	O
of	O
two	O
scores	O
minus	O
the	O
span	O
length	O
(	O
line	O
6	O
)	O
,	O
which	O
turns	O
out	O
to	O
be	O
critical	O
to	O
the	O
performance	O
as	O
targets	O
are	O
usually	O
short	O
entities	O
.	O

For	O
each	O
example	O
,	O
top	O
-	O
M	O
indices	O
are	O
first	O
chosen	O
from	O
the	O
two	O
predicted	O
scores	O
g	O
s	O
and	O
g	O
e	O
(	O
line	O
2	O
)	O
,	O
and	O
the	O
candidate	O
span	O
(	O
s	O
i	O
,	O
e	O
j	O
)	O
(	O
denoted	O
as	O
r	O
l	O
)	O
along	O
with	O
its	O
heuristicregularized	O
score	O
u	O
l	O
are	O
then	O
added	O
to	O
the	O
lists	O
R	O
and	O
U	O
respectively	O
,	O
under	O
the	O
constraints	O
that	O
the	O
end	O
position	O
is	O
no	O
less	O
than	O
the	O
start	O
position	O
as	O
well	O
as	O
the	O
addition	O
of	O
two	O
scores	O
exceeds	O
a	O
threshold	O
(	O
line	O
3	O
-	O
8)	O
.	O

To	O
adapt	O
to	O
multi	O
-	O
target	O
scenarios	O
,	O
we	O
propose	O
an	O
heuristic	O
multi	O
-	O
span	O
decoding	O
algorithm	O
as	O
shown	O
in	O
Algorithm	O
1	O
.	O

Sentence	O
:	O
Great	O
food	O
but	O
the	O
service	O
was	O
dreadful	O
!	O
Targets	O
:	O
food	O
,	O
service	O
Predictions	O
:	O
food	O
but	O
the	O
service	O
,	O
food	O
,	O
Great	O
food	O
,	O
service	O
,	O
service	O
was	O
dreadful	O
,	O
...	O

Figure	O
4	O
gives	O
a	O
qualitative	O
example	O
to	O
illustrate	O
this	O
phenomenon	O
.	O

Moreover	O
,	O
simply	O
taking	O
top	O
-	O
K	O
spans	O
according	O
to	O
the	O
addition	O
of	O
two	O
scores	O
is	O
also	O
not	O
optimal	O
,	O
as	O
multiple	O
candidates	O
may	O
refer	O
to	O
the	O
same	O
text	O
.	O

However	O
,	O
such	O
decoding	O
method	O
is	O
not	O
suitable	O
for	O
the	O
multi	O
-	O
target	O
extraction	O
task	O
.	O

Then	O
,	O
we	O
define	O
the	O
training	O
objective	O
as	O
the	O
sum	O
of	O
the	O
negative	O
log	O
probabilities	O
of	O
the	O
true	O
start	O
and	O
end	O
positions	O
on	O
two	O
predicted	O
probabilities	O
as	O
:	O
L	O
=	O
X	O
n+2	O
i=1	O
y	O
s	O
i	O
log(p	O
s	O
i	O
)	O
X	O
n+2	O
j=1	O
y	O
e	O
j	O
log(p	O
e	O
j	O
)	O
At	O
inference	O
time	O
,	O
previous	O
works	O
choose	O
the	O
span	O
(	O
k	O
,	O
l	O
)	O
(	O
k	O
	O
l	O
)	O
with	O
the	O
maximum	O
value	O
of	O
g	O
s	O
k	O
+	O
g	O
e	O
l	O
as	O
the	O
final	O
prediction	O
.	O

As	O
a	O
result	O
,	O
we	O
can	O
obtain	O
a	O
vector	O
y	O
s	O
2	O
R	O
(	O
n+2	O
)	O
,	O
where	O
each	O
element	O
y	O
s	O
i	O
indicates	O
whether	O
the	O
i	O
-	O
th	O
token	O
starts	O
a	O
target	O
,	O
and	O
also	O
get	O
another	O
vector	O
y	O
e	O
2	O
R	O
(	O
n+2	O
)	O
for	O
labeling	O
the	O
end	O
positions	O
.	O

Similarly	O
,	O
we	O
can	O
get	O
the	O
probability	O
of	O
the	O
end	O
position	O
along	O
with	O
its	O
confidence	O
score	O
by	O
:	O
g	O
e	O
=	O
w	O
e	O
h	O
L	O
,	O
p	O
e	O
=	O
softmax(g	O
e	O
)	O
During	O
training	O
,	O
since	O
each	O
sentence	O
may	O
contain	O
multiple	O
targets	O
,	O
we	O
label	O
the	O
span	O
boundaries	O
for	O
all	O
target	O
entities	O
in	O
the	O
list	O
T.	O

We	O
obtain	O
the	O
unnormalized	O
score	O
as	O
well	O
as	O
the	O
probability	O
distribution	O
of	O
the	O
start	O
position	O
as	O
:	O
g	O
s	O
=	O
w	O
s	O
h	O
L	O
,	O
p	O
s	O
=	O
softmax(g	O
s	O
)	O
where	O
w	O
s	O
2	O
R	O
h	O
is	O
a	O
trainable	O
weight	O
vector	O
.	O

Rather	O
than	O
finding	O
targets	O
via	O
sequence	O
tagging	O
methods	O
,	O
we	O
detect	O
candidate	O
targets	O
by	O
predicting	O
the	O
start	O
and	O
end	O
positions	O
of	O
the	O
target	O
in	O
the	O
sentence	O
,	O
as	O
suggested	O
in	O
extractive	O
question	O
answering	O
(	O
Wang	O
and	O
Jiang	O
,	O
2017;Seo	O
et	O
al	O
.	O
,	O
2017;Hu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

Multi	O
-	O
target	O
extractor	O
aims	O
to	O
propose	O
multiple	O
candidate	O
opinion	O
targets	O
(	O
Figure	O
3(a	O
)	O
)	O
.	O

Multi	O
-	O
Target	O
Extractor	O
.	O

(	O
2017	O
)	O
for	O
more	O
details	O
.	O

Next	O
,	O
we	O
use	O
a	O
series	O
of	O
L	O
stacked	O
Transformer	O
blocks	O
to	O
project	O
the	O
input	O
embeddings	O
into	O
a	O
sequence	O
of	O
contextual	O
vectors	O
h	O
i	O
2	O
R	O
(	O
n+2)	O
⇥	O
h	O
as	O
:	O
h	O
i	O
=	O
TransformerBlock(h	O
i	O
1	O
)	O
,	O
8i	O
2	O
[	O
1	O
,	O
L	O
]	O
Here	O
,	O
we	O
omit	O
an	O
exhaustive	O
description	O
of	O
the	O
block	O
architecture	O
and	O
refer	O
readers	O
to	O
Vaswani	O
et	O
al	O
.	O

Then	O
for	O
each	O
token	O
xi	O
in	O
x	O
,	O
we	O
convert	O
it	O
into	O
vector	O
space	O
by	O
summing	O
the	O
token	O
,	O
segment	O
,	O
and	O
position	O
embeddings	O
,	O
thus	O
yielding	O
the	O
input	O
embeddings	O
h	O
0	O
2	O
R	O
(	O
n+2)	O
⇥	O
h	O
,	O
where	O
h	O
is	O
the	O
hidden	O
size	O
.	O

We	O
first	O
tokenize	O
the	O
sentence	O
x	O
using	O
a	O
30,522	O
wordpiece	O
vocabulary	O
,	O
and	O
then	O
generate	O
the	O
input	O
sequence	O
x	O
by	O
concatenating	O
a	O
[	O
CLS	O
]	O
token	O
,	O
the	O
tokenized	O
sentence	O
,	O
and	O
a	O
[	O
SEP	O
]	O
token	O
.	O

We	O
use	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
(	O
BERT	O
)	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
a	O
pre	O
-	O
trained	O
bidirectional	O
Transformer	O
encoder	O
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
across	O
a	O
variety	O
of	O
NLP	O
tasks	O
,	O
as	O
our	O
backbone	O
network	O
.	O

We	O
further	O
investigate	O
three	O
different	O
approaches	O
under	O
this	O
framework	O
,	O
namely	O
the	O
pipeline	B-MethodName
,	O
joint	B-MethodName
,	O
and	O
collapsed	B-MethodName
models	O
in	O
§	O
3.4	O
.	O
BERT	O
as	O
Backbone	O
Network	O
.	O

Then	O
,	O
a	O
polarity	O
classifier	O
is	O
designed	O
to	O
predict	O
the	O
sentiment	O
towards	O
each	O
extracted	O
candidate	O
using	O
its	O
summarized	O
span	O
representation	O
(	O
§	O
3.3	O
)	O
.	O

A	O
multitarget	O
extractor	O
is	O
first	O
used	O
to	O
propose	O
multiple	O
candidate	O
targets	O
from	O
the	O
sentence	O
(	O
§	O
3.2	O
)	O
.	O

The	O
basis	O
of	O
our	O
frame	O
-	O
work	O
is	O
the	O
BERT	O
encoder	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
):	O
we	O
map	O
word	O
embeddings	O
into	O
contextualized	O
token	O
representations	O
using	O
pre	O
-	O
trained	O
Transformer	O
blocks	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
(	O
§	O
3.1	O
)	O
.	O

The	O
overall	O
illustration	O
of	O
the	O
proposed	O
framework	O
is	O
shown	O
in	O
Figure	O
3	O
.	O

The	O
goal	O
is	O
to	O
find	O
all	O
targets	O
from	O
the	O
sentence	O
as	O
well	O
as	O
predict	O
their	O
polarities	O
.	O

Instead	O
of	O
formulating	O
the	O
open	O
-	O
domain	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
task	O
as	O
a	O
sequence	O
tagging	O
problem	O
,	O
we	O
propose	O
to	O
use	O
a	O
span	B-MethodName
-	I-MethodName
based	I-MethodName
labeling	I-MethodName
scheme	I-MethodName
as	O
follows	O
:	O
given	O
an	O
input	O
sentence	O
x	O
=	O
(	O
x	O
1	O
,	O
...	O
,	O
x	O
n	O
)	O
with	O
length	O
n	O
,	O
and	O
a	O
target	O
list	O
T	O
=	O
{	O
t	O
1	O
,	O
...	O
,	O
t	O
m	O
}	O
,	O
where	O
the	O
number	O
of	O
targets	O
is	O
m	O
and	O
each	O
target	O
t	O
i	O
is	O
annotated	O
with	O
its	O
start	O
position	O
,	O
its	O
end	O
position	O
,	O
and	O
its	O
sentiment	O
polarity	O
.	O

Extract	B-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
Classify	I-MethodName
Framework	I-MethodName
.	O

However	O
,	O
unlike	O
these	O
works	O
that	O
extract	O
one	O
span	O
as	O
the	O
final	O
answer	O
,	O
our	O
approach	O
is	O
designed	O
to	O
dynamically	O
output	O
one	O
or	O
multiple	O
opinion	O
targets	O
.	O

Our	O
approach	O
is	O
related	O
to	O
this	O
line	O
of	O
work	O
.	O

Wang	O
and	O
Jiang	O
(	O
2017	O
)	O
explore	O
two	O
answer	O
prediction	O
methods	O
,	O
namely	O
the	O
sequence	O
method	O
and	O
the	O
boundary	O
method	O
,	O
finding	O
that	O
the	O
later	O
performs	O
better	O
.	O

(	O
2016	O
)	O
investigate	O
several	O
predicting	O
strategies	O
,	O
such	O
as	O
BIO	O
prediction	O
,	O
boundary	O
prediction	O
,	O
and	O
the	O
results	O
show	O
that	O
predicting	O
the	O
two	O
endpoints	O
of	O
the	O
answer	O
is	O
more	O
beneficial	O
than	O
the	O
tagging	O
method	O
.	O

To	O
solve	O
this	O
task	O
,	O
Lee	O
et	O
al	O
.	O

The	O
proposed	O
span	O
-	O
based	O
labeling	O
scheme	O
is	O
inspired	O
by	O
recent	O
advances	O
in	O
machine	O
comprehension	O
and	O
question	O
answering	O
(	O
Seo	O
et	O
al	O
.	O
,	O
2017;Hu	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
where	O
the	O
task	O
is	O
to	O
extract	O
a	O
continuous	O
span	O
of	O
text	O
from	O
the	O
document	O
as	O
the	O
answer	O
to	O
the	O
question	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Our	O
work	O
differs	O
from	O
these	O
approaches	O
in	O
that	O
we	O
formulate	O
this	O
task	O
as	O
a	O
spanlevel	B-DatasetName
extract	I-DatasetName
-	I-DatasetName
then	I-DatasetName
-	I-DatasetName
classify	I-DatasetName
process	I-DatasetName
instead	O
.	O

word	O
detection	O
.	O

The	O
last	O
block	O
's	O
hidden	O
states	O
are	O
used	O
to	O
(	O
a	O
)	O
propose	O
one	O
or	O
multiple	O
candidate	O
targets	O
based	O
on	O
the	O
probabilities	O
of	O
the	O
start	O
and	O
end	O
positions	O
,	O
(	O
b	O
)	O
predict	O
the	O
sentiment	O
polarity	O
using	O
the	O
span	O
representation	O
of	O
the	O
given	O
target	O
.	O

(	O
2019	O
)	O
have	O
proposed	O
a	O
unified	B-MethodName
model	O
that	O
contains	O
two	O
stacked	O
LSTMs	O
along	O
with	O
carefully	O
-	O
designed	O
components	O
for	O
maintaining	O
sentiment	O
consistency	O
and	O
improving	O
target	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
that	O
contains	O
L	O
pre	O
-	O
trained	O
Transformer	O
blocks	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

Recently	O
,	O
Li	O
et	O
al	O
.	O

(	O
2015	O
)	O
further	O
leverage	O
these	O
linguistic	O
features	O
to	O
enhance	O
a	O
neural	O
CRF	O
model	O
.	O

Zhang	O
et	O
al	O
.	O

(	O
2013	O
)	O
formulate	O
the	O
whole	O
task	O
as	O
a	O
sequence	B-TaskName
tagging	I-TaskName
problem	O
and	O
propose	O
to	O
use	O
CRF	B-MethodName
with	O
hand	O
-	O
crafted	O
linguistic	O
features	O
.	O

Specifically	O
,	O
Mitchell	O
et	O
al	O
.	O

Rather	O
than	O
solving	O
these	O
two	O
subtasks	O
with	O
separate	O
models	O
,	O
a	O
more	O
practical	O
approach	O
is	O
to	O
directly	O
predict	O
the	O
sentiment	O
towards	O
an	O
entity	O
along	O
with	O
discovering	O
the	O
entity	O
itself	O
.	O

Recent	O
works	O
mainly	O
focus	O
on	O
capturing	O
the	O
interaction	O
between	O
the	O
target	O
and	O
the	O
sentence	O
,	O
by	O
utilizing	O
various	O
neural	O
architectures	O
such	O
as	O
LSTMs	B-MethodName
(	O
Hochreiter	O
and	O
Schmidhuber	O
,	O
1997;Tang	O
et	O
al	O
.	O
,	O
2016a	O
)	O
with	O
attention	O
mechanism	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2016b;Li	O
et	O
al	O
.	O
,	O
2018;Fan	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
CNNs	B-MethodName
(	O
Xue	O
and	O
Li	O
,	O
2018;Huang	O
and	O
Carley	O
,	O
2018	O
)	O
,	O
and	O
Memory	B-MethodName
Networks	I-MethodName
(	O
Tang	O
et	O
al	O
.	O
,	O
2016b;Chen	O
et	O
al	O
.	O
,	O
2017;Li	O
and	O
Lam	O
,	O
2017	O
)	O
.	O

Next	O
,	O
polarity	B-TaskName
classification	I-TaskName
aims	O
to	O
predict	O
the	O
sentiment	O
polarities	O
over	O
the	O
extracted	O
target	O
entities	O
(	O
Jiang	O
et	O
al	O
.	O
,	O
2011;Dong	O
et	O
al	O
.	O
,	O
2014;Tang	O
et	O
al	O
.	O
,	O
2016a;Wang	O
et	O
al	O
.	O
,	O
2016b;Chen	O
et	O
al	O
.	O
,	O
2017;Xue	O
and	O
Li	O
,	O
2018;Li	O
et	O
al	O
.	O
,	O
2018;Fan	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

This	O
subtask	O
,	O
which	O
is	O
usually	O
denoted	O
as	O
target	B-TaskName
extraction	I-TaskName
,	O
can	O
be	O
solved	O
by	O
sequence	O
tagging	O
methods	O
(	O
Jakob	O
and	O
Gurevych	O
,	O
2010;Liu	O
et	O
al	O
.	O
,	O
2015;Wang	O
et	O
al	O
.	O
,	O
2016a;Poria	O
et	O
al	O
.	O
,	O
2016;Shu	O
et	O
al	O
.	O
,	O
2017;He	O
et	O
al	O
.	O
,	O
2017;Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

We	O
further	O
investigate	O
three	O
approaches	O
under	O
this	O
framework	O
,	O
namely	O
the	O
pipeline	O
,	O
joint	O
,	O
and	O
collapsed	O
models	O
.	O

The	O
second	O
is	O
polarity	O
classification	O
,	O
assuming	O
that	O
the	O
target	O
entities	O
are	O
given	O
.	O

Recently	O
,	O
many	O
works	O
concentrate	O
on	O
leveraging	O
deep	O
neural	O
networks	O
to	O
tackle	O
this	O
task	O
,	O
e.g.	O
,	O
using	O
CNNs	B-MethodName
(	O
Poria	O
et	O
al	O
.	O
,	O
2016;Xu	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
RNNs	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2015;He	O
et	O
al	O
.	O
,	O
2017	O
)	O
,	O
and	O
so	O
on	O
.	O

Traditionally	O
,	O
Conditional	O
Random	O
Fields	O
(	O
CRF	O
)	O
(	O
Lafferty	O
et	O
al	O
.	O
,	O
2001	O
)	O
have	O
been	O
widely	O
explored	O
(	O
Jakob	O
and	O
Gurevych	O
,	O
2010;Wang	O
et	O
al	O
.	O
,	O
2016a;Shu	O
et	O
al	O
.	O
,	O
2017	O
)	O
.	O

The	O
first	O
is	O
target	O
extraction	O
for	O
identifying	O
entities	O
from	O
the	O
input	O
sentence	O
.	O

Related	O
Work	O
.	O

1	O
https://github.com/huminghao16/SpanABSA	O
.	O

Source	O
code	O
is	O
released	O
to	O
facilitate	O
future	O
research	O
in	O
this	O
field	O
1	O
.	O

In	O
addition	O
,	O
the	O
pipeline	O
model	O
firmly	O
improves	O
over	O
both	O
the	O
joint	O
and	O
collapsed	O
models	O
.	O

Extensive	O
experiments	O
on	O
three	O
benchmark	O
datasets	O
show	O
that	O
our	O
models	O
consistently	O
outperform	O
sequence	O
tagging	O
baselines	O
.	O

Second	O
,	O
following	O
previous	O
works	O
(	O
Mitchell	O
et	O
al	O
.	O
,	O
2013;Zhang	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
we	O
compare	O
the	O
pipeline	O
,	O
joint	O
,	O
and	O
collapsed	O
models	O
under	O
the	O
span	O
-	O
based	O
labeling	O
scheme	O
.	O

First	O
,	O
we	O
make	O
an	O
elaborate	O
comparison	O
between	O
tagging	O
-	O
based	O
models	O
and	O
span	O
-	O
based	O
models	O
.	O

We	O
take	O
BERT	O
(	O
Devlin	O
et	O
al	O
.	O
,	O
2018	O
)	O
as	O
the	O
default	O
backbone	O
network	O
,	O
and	O
explore	O
two	O
research	O
questions	O
.	O

Moreover	O
,	O
since	O
the	O
polarity	O
is	O
decided	O
using	O
the	O
targeted	O
span	O
representation	O
,	O
the	O
model	O
is	O
able	O
to	O
take	O
all	O
target	O
words	O
into	O
account	O
before	O
making	O
predictions	O
,	O
thus	O
naturally	O
avoiding	O
sentiment	O
inconsistency	O
.	O

The	O
advantage	O
of	O
this	O
approach	O
is	O
that	O
the	O
extractive	O
search	O
space	O
can	O
be	O
reduced	O
linearly	O
with	O
the	O
sentence	O
length	O
,	O
which	O
is	O
far	O
less	O
than	O
the	O
tagging	O
method	O
.	O

Under	O
such	O
annotation	O
,	O
we	O
introduce	O
an	O
extract	O
-	O
then	O
-	O
classify	O
framework	O
that	O
first	O
extracts	O
multiple	O
opinion	O
targets	O
using	O
an	O
heuristic	O
multispan	O
decoding	O
algorithm	O
,	O
and	O
then	O
classifies	O
their	O
polarities	O
with	O
corresponding	O
summarized	O
span	O
representations	O
.	O

The	O
key	O
insight	O
is	O
to	O
annotate	O
each	O
opinion	O
target	O
with	O
its	O
span	O
boundary	O
followed	O
by	O
its	O
sentiment	O
polarity	O
.	O

To	O
address	O
the	O
problems	O
,	O
we	O
propose	O
a	O
spanbased	O
labeling	O
scheme	O
for	O
open	O
-	O
domain	O
targeted	O
sentiment	O
analysis	O
,	O
as	O
shown	O
in	O
Figure	O
2(b	O
)	O
.	O

For	O
example	O
,	O
there	O
is	O
a	O
chance	O
that	O
the	O
words	O
"	O
Windows	O
"	O
and	O
"	O
7	O
"	O
in	O
Figure	O
2(a	O
)	O
are	O
predicted	O
to	O
have	O
different	O
polarities	O
due	O
to	O
word	O
-	O
level	O
tagging	O
decisions	O
.	O

(	O
2019	O
)	O
.	O

Second	O
,	O
since	O
predicted	O
polarities	O
over	O
target	O
words	O
may	O
be	O
different	O
,	O
the	O
sentiment	O
consistency	O
of	O
multi	O
-	O
word	O
entity	O
can	O
not	O
be	O
guaranteed	O
,	O
as	O
mentioned	O
by	O
Li	O
et	O
al	O
.	O

First	O
,	O
tagging	O
polarity	O
over	O
each	O
word	O
ignores	O
the	O
semantics	O
of	O
the	O
entire	O
opinion	O
target	O
.	O

As	O
for	O
polarity	O
classification	O
,	O
the	O
sequence	O
tagging	O
scheme	O
turns	O
out	O
to	O
be	O
problematic	O
for	O
two	O
reasons	O
.	O

(	O
2016	O
)	O
show	O
that	O
,	O
when	O
using	O
BIO	O
tags	O
for	O
extractive	O
question	O
answering	O
tasks	O
,	O
the	O
model	O
must	O
consider	O
a	O
huge	O
search	O
space	O
due	O
to	O
the	O
compositionality	O
of	O
labels	O
(	O
the	O
power	O
set	O
of	O
all	O
sentence	O
words	O
)	O
,	O
thus	O
being	O
less	O
effective	O
.	O

Lee	O
et	O
al	O
.	O

However	O
,	O
the	O
above	O
annotation	O
scheme	O
has	O
several	O
disadvantages	O
in	O
target	O
extraction	O
and	O
polarity	O
classification	O
.	O

As	O
a	O
result	O
,	O
the	O
entire	O
task	O
is	O
formulated	O
as	O
a	O
sequence	O
tagging	O
problem	O
,	O
and	O
solved	O
using	O
either	O
a	O
pipeline	O
model	O
,	O
a	O
joint	O
model	O
,	O
or	O
a	O
collapsed	O
model	O
under	O
the	O
same	O
network	O
architecture	O
.	O

The	O
key	O
insight	O
is	O
to	O
label	O
each	O
word	O
with	O
a	O
set	O
of	O
target	O
tags	O
(	O
e.g.	O
,	O
B	O
,	O
I	O
,	O
O	O
)	O
as	O
well	O
as	O
a	O
set	O
of	O
polarity	O
tags	O
(	O
e.g.	O
,	O
+	O
,	O
-	O
,	O
0	O
)	O
,	O
or	O
use	O
a	O
more	O
collapsed	O
set	O
of	O
tags	O
(	O
e.g.	O
,	O
B+	O
,	O
I-	O
)	O
to	O
directly	O
indicate	O
the	O
boundary	O
of	O
targeted	O
sentiment	O
,	O
as	O
shown	O
in	O
Figure	O
2(a	O
)	O
.	O

Rather	O
than	O
using	O
separate	O
models	O
for	O
each	O
subtask	O
,	O
some	O
works	O
attempt	O
to	O
solve	O
the	O
task	O
in	O
a	O
more	O
integrated	O
way	O
,	O
by	O
jointly	O
extracting	O
targets	O
and	O
predicting	O
their	O
sentiments	O
(	O
Mitchell	O
et	O
al	O
.	O
,	O
2013;Zhang	O
et	O
al	O
.	O
,	O
2015;Li	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

Although	O
lots	O
of	O
efforts	O
have	O
been	O
made	O
to	O
design	O
sophisticated	O
classifiers	O
for	O
this	O
subtask	O
,	O
they	O
all	O
assume	O
that	O
the	O
targets	O
are	O
already	O
given	O
.	O

Since	O
opinion	O
targets	O
are	O
not	O
given	O
,	O
we	O
need	O
to	O
first	O
detect	O
the	O
targets	O
from	O
the	O
input	O
text	O
.	O

Typically	O
,	O
the	O
whole	O
task	O
can	O
be	O
decoupled	O
into	O
two	O
subtasks	O
.	O

Taking	O
Figure	O
1	O
as	O
an	O
example	O
,	O
the	O
goal	O
is	O
to	O
first	O
identify	O
"	O
Windows	O
7	O
"	O
and	O
"	O
Vista	O
"	O
as	O
opinion	O
targets	O
and	O
then	O
predict	O
their	O
corresponding	O
sentiment	O
classes	O
.	O

Open	O
-	O
domain	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
is	O
a	O
fundamental	O
task	O
in	O
opinion	O
mining	O
and	O
sentiment	O
analysis	O
(	O
Pang	O
et	O
al	O
.	O
,	O
2008;Liu	O
,	O
2012	O
)	O
.	O

Introduction	O
.	O

Moreover	O
,	O
we	O
find	O
that	O
the	O
pipeline	O
model	O
achieves	O
the	O
best	O
performance	O
compared	O
with	O
the	O
other	O
two	O
models	O
.	O

Experiments	O
on	O
three	O
benchmark	O
datasets	O
show	O
that	O
our	O
approach	O
consistently	O
outperforms	O
the	O
sequence	O
tagging	O
baseline	O
.	O

To	O
address	O
these	O
problems	O
,	O
we	O
propose	O
a	O
span	B-MethodName
-	I-MethodName
based	I-MethodName
extract	I-MethodName
-	I-MethodName
then	I-MethodName
-	I-MethodName
classify	I-MethodName
framework	I-MethodName
,	O
where	O
multiple	O
opinion	O
targets	O
are	O
directly	O
extracted	O
from	O
the	O
sentence	O
under	O
the	O
supervision	O
of	O
target	O
span	O
boundaries	O
,	O
and	O
corresponding	O
polarities	O
are	O
then	O
classified	O
using	O
their	O
span	O
representations	O
.	O

Compared	O
to	O
traditional	O
sentence	O
-	O
level	O
sentiment	B-TaskName
analysis	I-TaskName
tasks	O
(	O
Lin	O
and	O
He	O
,	O
2009;Kim	O
,	O
2014	O
)	O
,	O
the	O
task	O
requires	O
detecting	O
target	O
entities	O
mentioned	O
in	O
the	O
sentence	O
along	O
with	O
their	O
sentiment	O
polarities	O
,	O
thus	O
being	O
more	O
challenging	O
.	O

However	O
,	O
such	O
formulation	O
suffers	O
from	O
problems	O
such	O
as	O
huge	O
search	O
space	O
and	O
sentiment	O
inconsistency	O
.	O

Prior	O
work	O
typically	O
formulates	O
this	O
task	O
as	O
a	O
sequence	O
tagging	O
problem	O
.	O

Open	O
-	O
domain	O
targeted	O
sentiment	B-TaskName
analysis	I-TaskName
aims	O
to	O
detect	O
opinion	O
targets	O
along	O
with	O
their	O
sentiment	O
polarities	O
from	O
a	O
sentence	O
.	O

Open	O
-	O
Domain	O
Targeted	O
Sentiment	B-TaskName
Analysis	I-TaskName
via	O
Span	O
-	O
Based	O
Extraction	O
and	O
Classification	O
.	O

